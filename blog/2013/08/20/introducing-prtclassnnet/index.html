
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Introducing prtClassNNET - PRT Blog</title>
  <meta name="author" content="Kenneth Morton and Peter Torrione">

  
  <meta name="description" content="Introducing prtClassNNET Aug 20th, 2013 People are often asking us why we don&#8217;t have a neural-network (NNET) implemented in the PRT. Formerly &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://newfolder.github.io/blog/2013/08/20/introducing-prtclassnnet">
  <link href="/favicon.ico" rel="icon">
  
  <link href="/assets/bootstrap/css/spacelab.min.css" rel="stylesheet" type="text/css">
  <link href="/assets/bootstrap/css/bootstrap-responsive.min.css" rel="stylesheet" type="text/css">
  <link href="/assets/bootstrap/css/custom.css" rel="stylesheet" type="text/css">
  <link href="/assets/font-awesome/css/font-awesome.css" rel="stylesheet" type="text/css">
  
  <link href="/atom.xml" rel="alternate" title="PRT Blog" type="application/atom+xml">
  <style type="text/css">
pre, tt, code { font-size:12px; }
pre { margin:0px 0px 20px; }
pre.error { color:red; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }
</style>
  

</head>

<body    data-spy="scroll">

  <div class="container">
    <header class="jumbotron subhead" id="overview">
      
<div class="subscribe">
  <table>
    <tr>
      <td><span>Get Updates: &nbsp;</span></td>
      
      
      <td><a href="/atom.xml" class="btn"><i class="icon-cog"></i> By RSS</a></td>
      
      
    </tr>
  </table>
</div>

<h1 class="title">PRT Blog</h1>

  <p class="lead">MATLAB Pattern Recognition Open Free and Easy</p>


      <div class="navbar">
  <div class="navbar-inner">
    <div class="container" style="width: auto;">
      <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </a>
      <div class="nav-collapse">
                <ul class="nav">
          <li><a href="/">Home</a></li>
          <li><a href="/blog/archives">Blog</a></li>
          <li><a href="https://github.com/newfolder/PRT">Code</a></li>
          <li><a href="/prtdoc/">Documentation</a></li>
		  <li><a href="https://github.com/newfolder/PRT/issues">Get Help</a><li>
          <li><a href="/about">About</a></li>
        </ul>

        
          <form action="http://google.com/search" method="get" class="navbar-search pull-left">
            <fieldset role="search">
              <input type="hidden" name="q" value="site:newfolder.github.io" />
              <input type="text" name="q" results="0" placeholder="Search" class="search-query span2" />
            </fieldset>
          </form>
        
        
      </div><!-- /.nav-collapse -->
    </div>
  </div><!-- /navbar-inner -->
</div>

    </header>
    <div id="main">
      <div id="content">
        <div class="row">
  
  <div class="span8">
    <br>

  <header>
    
      <h1 class="entry-title">Introducing prtClassNNET</h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-08-20T12:18:00-04:00" pubdate data-updated="true">Aug 20<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>People are often asking us why we don&#8217;t have a neural-network (NNET) implemented in the PRT.  Formerly, we never focused on making a home-grown NNET object, since the MathWorks already has a (<a href="http://www.mathworks.com/help/nnet/">neural network toolbox</a>), and often we&#8217;ve found that NNET results aren&#8217;t significantly better than other classifiers, and they can be difficult to train. That said, neural network classifiers can provide good results, some recent advances in deep-learning have brought NNET classifiers back into vogue, and they&#8217;re often fun to play with.  As a result, we&#8217;ve finally rolled our own NNET classifier in the PRT that doesn&#8217;t require any additional toolboxes. One thing to note: If you have the MATLAB NNET toolbox, you can incorporate it in the PRT using prtClassMatlabNnet.</p>




<!--/introduction-->


<h2>Contents</h2>


<div><ul><li><a href="#1">Current Restrictions:</a></li><li><a href="#2">Using prtClassNnet - Basic parameters</a></li><li><a href="#3">Using prtClassNnet - Advanced Parameters</a></li><li><a href="#4">Visualizing</a></li><li><a href="#5">Example Processing</a></li><li><a href="#6">Concluding</a></li></ul></div>




<h2>Current Restrictions:<a name="1"></a></h2>


<p>Our classifier only currently allows standard batch-propagation learning. It should be relatively easy to include new training approaches, but we haven&#8217;t done so yet. The current prtClassNnet only allows for three-layer (one hidden-layer) networks.  Depending on who you ask, this is either very important, or not important at all. In either case, we hope to expand the capabilities here eventually. The current formulation only works for binary classification problems. Extensions to enable multi-class classification are also in progress.</p>




<h2>Using prtClassNnet - Basic parameters<a name="2"></a></h2>


<p>prtClassNnet acts pretty much the same as any other classifier.  As you might expect, we can set the number of neurons in the hidden layer, and set the min ans max number of training epochs, and the tolerance to check for convergence:</p>




<pre class="codeinput">nnet = prtClassNnet;
nnet.nHiddenUnits = 10;
nnet.minIters = 10000;
nnet.relativeErrorChangeThreshold = 1.0000e-04; <span class="comment">% check for convergence if nIters &gt; minIters</span>
nnet.maxIters = 100000;                         <span class="comment">% kick out after this many, no matter what</span>
</pre>




<h2>Using prtClassNnet - Advanced Parameters<a name="3"></a></h2>


<p>The activation functions are an important part of neural network design. The prtClassNnet object allows you to manually specify the activation function, but you need to set both the &#8220;forward function&#8221; and the first derivative of the forward function.  These can be specified using function handles in the fields fwdFn and fwdFnDeriv. The &#8220;classic&#8221; formulation of a neural network uses a sigmoid activation function, so the parameters can be set like so:</p>




<pre class="codeinput">sigmoidFn = @(x) 1./(1 + exp(-x));
nnet.fwdFn = sigmoidFn;
nnet.fwdFnDeriv = @(x) sigmoidFn(x).*(1-sigmoidFn(x));
</pre>




<h2>Visualizing<a name="4"></a></h2>


<p>prtClassNnet enables automatic visualization of the algorithm progress as learning proceeds.  You can set how often (or whether) this visualization occurs by setting nnet.plotOnIter to a scalar; the scalar represents how often to update the plots.  Use 0 to use no visualization.</p>


<h2>Example Processing<a name="5"></a></h2>


<p>So, what does the resulting process look like?  Let&#8217;s give it a whirl with a stadnard X-OR data set:</p>


<pre class="codeinput">dsTrain = prtDataGenXor;
dsTest = prtDataGenXor;
nnet = prtClassNnet(<span class="string">'nHiddenUnits'</span>,10,<span class="string">'plotOnIter'</span>,1000,<span class="string">'relativeErrorChangeThreshold'</span>,1e-4);
nnet = nnet.train(dsTrain);
yOut = nnet.run(dsTest);
</pre>


<p><img vspace="5" hspace="5" src="/images/torrione_blog_2013_08_20_01.png" alt=""> <h2>Concluding<a name="6"></a></h2><p>We hope that using prtClassNnet enables you to do some new, neat things. If you like it, please help us re-write the code to overcome our current restrictions!</p><p>Happy coding.</p></p>
</div>


<br>
<hr>
<br>
    <footer>
      <p class="meta">
        
  

<span class="byline author vcard">Posted by <span class="fn">Pete</span></span>

        








  


<time datetime="2013-08-20T12:18:00-04:00" pubdate data-updated="true">Aug 20<span>th</span>, 2013</time>
        


      </p>
      
        <div class="sharing">
  <br/>
  
  
  
</div>

      
      <p class="meta">
        
          <a class="basic-alignment pull-left" href="/blog/2013/07/29/supervised-learning/" title="Previous Post: Supervised Learning: An Introduction for Scientists and Engineers">&laquo; Supervised Learning: An Introduction for Scientists and Engineers</a>
        
        
          <a class="basic-alignment pull-right" href="/blog/2013/09/04/verbosestorage-and-a-little-prtalgorithm-plotting/" title="Next Post: verboseStorage and a little prtAlgorithm plotting">verboseStorage and a little prtAlgorithm plotting &raquo;</a>
        
      </p>
    </footer>
    
    
      <section>
        <h1>Comments</h1>
        <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
      </section>
    
  </div>

  
    
  <div class="span3 sidebar">
    <div class="well">
      
        <section>
  <h2>Recent Posts</h2>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2013/10/09/dude-wheres-my-help/">Dude Where's My Help?</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/09/04/verbosestorage-and-a-little-prtalgorithm-plotting/">verboseStorage and a little prtAlgorithm plotting</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/08/20/introducing-prtclassnnet/">Introducing prtClassNNET</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/07/29/supervised-learning/">Supervised Learning: An Introduction for Scientists and Engineers</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/07/24/using-svms/">Using SVMs for Scientists and Engineers</a>
      </li>
    
  </ul>
</section>


      
    </div>
  </div>


  
</div>


      </div>
    </div>
    <footer class="footer"><p>
  Copyright &copy; 2013 - Kenneth Morton and Peter Torrione -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span> -
  
    <span class="credit">Theme by <a href="http://brianarmstrong.org">Brian Armstrong</a></span>
  
</p>
</footer>
    

<script type="text/javascript">
      var disqus_shortname = 'newfolderconsulting';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://newfolder.github.io/blog/2013/08/20/introducing-prtclassnnet/';
        var disqus_url = 'http://newfolder.github.io/blog/2013/08/20/introducing-prtclassnnet/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
<script src="/assets/bootstrap/js/bootstrap.min.js"></script>


  </div>
</body>
</html>
