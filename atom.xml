<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[PRT Blog]]></title>
  <link href="http://newfolder.github.io/atom.xml" rel="self"/>
  <link href="http://newfolder.github.io/"/>
  <updated>2013-07-08T21:49:07-04:00</updated>
  <id>http://newfolder.github.io/</id>
  <author>
    <name><![CDATA[Kenneth Morton and Peter Torrione]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[twoClassParadigm]]></title>
    <link href="http://newfolder.github.io/blog/2013/07/08/twoclassparadigm/"/>
    <updated>2013-07-08T20:57:00-04:00</updated>
    <id>http://newfolder.github.io/blog/2013/07/08/twoclassparadigm</id>
    <content type="html"><![CDATA[<p>When using an M-ary classifier on a binary classification problem, sometimes you want multiple dimensional output and sometimes you don&#8217;t. In the PRT the default is to supply only a single output but this can be specified by setting the twoClassParadigm property of classifiers.</p>


<!--/introduction-->


<h2>Contents</h2>


<div><ul><li><a href="#1">A Quick Example</a></li><li><a href="#5">twoClassParadigm</a></li><li><a href="#7">Why did you do that?</a></li><li><a href="#10">Conclusion</a></li></ul></div>


<h2>A Quick Example<a name="1"></a></h2>


<p>Consider a binary classification problem with an m-ary classifer (prtClassMap).</p>




<pre class="codeinput">dsTrain = prtDataGenUnimodal;
dsTest = prtDataGenUnimodal;

classifier = prtClassMap;
trainedClassifier = train(classifier, dsTrain);

plot(trainedClassifier);
title(<span class="string">'Binary Classification with MAP'</span>,<span class="string">'FontSize'</span>,16);

output = run(trainedClassifier, dsTest);

output.nFeatures
</pre>


<pre class="codeoutput">
ans =
 1
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/morton_blog_20130708_twoClassParadigm_01.png" alt=""> <p>As you can see the output only has one feature. But consider the same M-ary classifier with a 3-class problem.</p></p>

<pre class="codeinput">dsTrainMary = prtDataGenMary;
dsTestMary = prtDataGenMary;

trainedClassifierMary = train(classifier, dsTrainMary);

plot(trainedClassifierMary);
title(<span class="string">'M-ary Classification with MAP'</span>,<span class="string">'FontSize'</span>,16);

outputMary = run(trainedClassifierMary, dsTestMary);

outputMary.nFeatures
</pre>


<pre class="codeoutput">
ans =
 3
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/morton_blog_20130708_twoClassParadigm_02.png" alt=""> <p>Now the output has 3 feautres. The trend continues with more classes, because prtClassMap declares itself as an Mary classififer</p></p>

<pre class="codeinput">classifier.isNativeMary
</pre>


<pre class="codeoutput">
ans =
 1
</pre>


<p>the output is (as it should) have a column corresponding to the confidence of each class. Binary classification is an exception.</p>


<h2>twoClassParadigm<a name="5"></a></h2>


<p>By default the PRT checks when M-ary classifiers are run on binary data to see if it should output a single confidence or binary confidences. The mode of operation is stored in twoClassParadigm which by default is set to the string M-ary.</p>


<pre class="codeinput">classifier.twoClassParadigm
</pre>


<pre class="codeoutput">
ans =
binary
</pre>


<p>If we return to the binary classification problem and set twoClassParadigm to mary we can see that we get the two outputs that we expect.</p>




<pre class="codeinput">classifier.twoClassParadigm = <span class="string">'mary'</span>;
trainedClassifierBinaryActingAsMary = train(classifier, dsTrain);

outputBinaryActingAsMary = run(trainedClassifierBinaryActingAsMary, dsTest);

outputBinaryActingAsMary.nFeatures
</pre>


<pre class="codeoutput">
ans =
 2
</pre>




<h2>Why did you do that?<a name="7"></a></h2>


<p>We debated, but ultimately we decided that it was less confusing this way. It is more rare to want both outputs for a binary classification problem than it is to expect a single. Most users expected this to run out of the box.</p>




<pre class="codeinput">classifier = prtClassMap;
trainedClassifier = train(classifier, dsTrain);
output = run(trainedClassifier, dsTest);

prtScoreRoc(output);
title(<span class="string">'Binary Classification with an M-ary Classifier'</span>,<span class="string">'FontSize'</span>,16);
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/morton_blog_20130708_twoClassParadigm_03.png" alt=""> <p>If you change the classifier to non-m-ary classification (prtClassGlrt for example) you expect the same code to work only by changing the classifier declaration.</p></p>

<pre class="codeinput">classifier = prtClassGlrt;
trainedClassifier = train(classifier, dsTrain);
output = run(trainedClassifier, dsTest);

prtScoreRoc(output);
title(<span class="string">'Binary Classification with a Binary Classifier'</span>,<span class="string">'FontSize'</span>,16);
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/morton_blog_20130708_twoClassParadigm_04.png" alt=""> <p>Without the twoClassParadigm system it wouldn&rsquo;t be as easy to switch classifiers for binary problems as an extra step would be required to manually select which column of the output should be used to calculate the ROC.</p><h2>Conclusion<a name="10"></a></h2><p>Well that&rsquo;s twoClassParadigm. It&rsquo;s a convenient feature of the PRT that not many people know about because they don&rsquo;t have to. In the rare cases when you want the confidence assigned to each class, you now know how to get them.</p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spectral Clustering]]></title>
    <link href="http://newfolder.github.io/blog/2013/06/24/spectral-clustering/"/>
    <updated>2013-06-24T13:33:00-04:00</updated>
    <id>http://newfolder.github.io/blog/2013/06/24/spectral-clustering</id>
    <content type="html"><![CDATA[<p>Hi everyone,</p>


<p>A few weeks ago we talked about clustering with K-Means, and using K-Means distances as a pre-processing step.  K-Means is great when euclidean distance in your input feature-space is meaningful, but what if your data instead lies on a high-dimensional manifold?</p>


<p>We recently introduced some new clustering and distance-metric approaches suitable for these cases - spectral clustering.  The theory behind spectral clustering is beyond the scope of this entry, but as usual, the wikipedia page has a good summary - <a href="http://en.wikipedia.org/wiki/Spectral_clustering">http://en.wikipedia.org/wiki/Spectral_clustering</a>.</p>


<!--/introduction-->




<p>Although I&#8217;m writing the blog entry, all of the code in this demo was written by one of our graduate students @ Duke University - Dmitry Kalika, who&#8217;s a new convert to the PRT!  Welcome Dima!</p>




<h2>Contents</h2>


<div><ul><li><a href="#1">References</a></li><li><a href="#2">prtPreProcSpectralEmbed</a></li><li><a href="#3">prtClusterSpectralKmeans</a></li><li><a href="#4">Wrapping Up</a></li></ul></div>


<h2>References<a name="1"></a></h2>


<p>Throughout the following and the code for spectral clustering in the PRT, we make use of the excellent Bengio, 2003 paper - Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering <a href="http://www.iro.umontreal.ca/~lisa/pointeurs/tr1238.pdf">http://www.iro.umontreal.ca/~lisa/pointeurs/tr1238.pdf</a></p>


<p>In particular, we use that extention for performing cluster approximation for out-of-sample embedding estimation.</p>


<h2>prtPreProcSpectralEmbed<a name="2"></a></h2>


<p>Spectral clustering typically relies upon what&#8217;s referred to as a spectral embedding; this is a low-dimensional representation of a high-dimensional proximity graph.</p>


<p>We can use features derived from spectral embeddings like so:</p>




<pre class="codeinput">ds = prtDataGenBimodal;
dsTest = prtDataGenBimodal(10);
algo = prtPreProcSpectralEmbed;
algo = algo.train(ds);
yOut = algo.run(ds);
plot(yOut);
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_2013_06_17_01.png" alt=""> <h2>prtClusterSpectralKmeans<a name="3"></a></h2><p>While spectral embedding provides a feature space for additional processing, we can also use prtClusterSpectralKmeans to perform direct clustering in the spectral space.</p><p>For example, the Moon data set (see prtDataGenMoon) creates two crescent moon-shapes that are not well-separated by euclidean distance metrics, but can be easily separated in spectral-cluster space.</p><pre class="codeinput">ds = prtDataGenMoon;
preProc = prtPreProcZmuv;
preProc = preProc.train(ds);
dsNorm = preProc.run(ds);
kmeans = prtClusterKmeans(<span class="string">&lsquo;nClusters&rsquo;</span>,2);
kmeansSpect = prtClusterSpectralKmeans(<span class="string">&lsquo;nClusters&rsquo;</span>,2);
kmeans = kmeans.train(dsNorm);
kmeansSpect = kmeansSpect.train(dsNorm);
subplot(1,2,1);
plot(kmeans);
title(<span class="string">&lsquo;K-Means Clusters&rsquo;</span>);
subplot(1,2,2);
plot(kmeansSpect)
title(<span class="string">&lsquo;Spect-K-Means Clusters&rsquo;</span>);
</pre>
<img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_2013_06_17_02.png" alt=""> <h2>Wrapping Up<a name="4"></a></h2><p>Spectral clustering provides a very useful technique for non-linear and non-euclidean clustering.  Right now our spectral clustering approaches are constrained  to using RBF kernels, though there&rsquo;s nothing that prevents you from using alternate kernels in future versions.</p><p>As always, let us know if you have questions or comments.</p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[prtClusterMeanShift]]></title>
    <link href="http://newfolder.github.io/blog/2013/06/13/prtclustermeanshift/"/>
    <updated>2013-06-13T14:13:00-04:00</updated>
    <id>http://newfolder.github.io/blog/2013/06/13/prtclustermeanshift</id>
    <content type="html"><![CDATA[<p>Hi everyone, Today we&#8217;ll talk about a new clustering algorithm in the PRT - Mean-Shift clustering.  Mean shift clustering is widely used in image processing, and has a few nice properties - for example, it&#8217;s not necessary to specify ahead of time how many clusters you need.  Instead you specify a clustering bandwidth.  We&#8217;ll show some examples below.  If you want a good introduction to mean-shift clustering, see <a href="http://en.wikipedia.org/wiki/Mean-shift"> the wiki page</a>.</p>


<p>Note, unlike most of our other objects, prtClusterMeanShift requires the bio-informatics toolbox,</p>


<!--/introduction-->




<h2>Contents</h2>


<div><ul><li><a href="#1">prtClusterMeanShift</a></li><li><a href="#3">Bandwidth</a></li><li><a href="#5">Application to Images</a></li><li><a href="#6">Determining Stopping</a></li><li><a href="#7">Conclusion</a></li></ul></div>


<h2>prtClusterMeanShift<a name="1"></a></h2>


<p>As you might expect, we start by generating some data, and a prtClusterMeanShift object:</p>


<pre class="codeinput">ds = prtDataGenUnimodal;
ms = prtClusterMeanShift;
</pre>


<p>We can train, run, and plot the mean-shift algorithm just like anything else</p>


<pre class="codeinput">ms = ms.train(ds);
plot(ms);
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_meanShift_2013_06_13_01.png" alt=""> <h2>Bandwidth<a name="3"></a></h2><p>In the above figure, the mean-shift algorithm correctly identified two clusters.  We can mess with the Gaussian bandwidth parameter (sigma) to see how this affects how many clusters mean-shift finds:</p><pre class="codeinput">sigmaVec = [.1 .3 .6 1 2 5];
<span class="keyword">for</span> ind = 1:length(sigmaVec)</p>

<pre><code>ms = prtClusterMeanShift;
ms.sigma = sigmaVec(ind);
ms = ms.train(ds);

subplot(2,3,ind);
plot(ms);
prtPlotUtilFreezeColors
title(sprintf(&lt;span class="string"&gt;'sigma = %.2d'&lt;/span&gt;,sigmaVec(ind)));
</code></pre>

<p><span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_meanShift_2013_06_13_02.png" alt=""> <p>Note how changing the sigma value can drastically alter the number of clusters that mean-shift finds.  Careful tuning of that parameter may be necessary for your particular application.</p><h2>Application to Images<a name="5"></a></h2><p>We mentioned before that you can use mean shift in image processing &ndash; here&rsquo;s a quick and dirty example applying mean shift to the famous &ldquo;cameraman&rdquo; photo:</p><pre class="codeinput">I = imread(<span class="string">&lsquo;cameraman.tif&rsquo;</span>);
I = imresize(I,0.25);
I = double(I);
[II,JJ] = meshgrid(1:size(I,2),1:size(I,1));</p>

<p>ij = bsxfun(@minus,cat(2,II(:),JJ(:)),size(I));</p>

<p>ds = prtDataSetClass(cat(2,I(:)-128,ij));
ms = train(prtClusterMeanShift(<span class="string">&lsquo;sigma&rsquo;</span>,200),ds);
out = run(ms, ds);
[~,out] = max(out.X,[],2);</p>

<p>figure(<span class="string">&lsquo;position&rsquo;</span>,[479 447 1033 366]);
subplot(1,2,1)
imagesc(I)
colormap(gray(256))
prtPlotUtilFreezeColors;
title(<span class="string">&lsquo;Cameraman.tif&rsquo;</span>,<span class="string">&lsquo;FontSize&rsquo;</span>,16);</p>

<p>subplot(1,2,2);
imagesc(reshape(out,size(I)));
colormap(prtPlotUtilClassColors(ms.nClusters))
prtPlotUtilFreezeColors;
title(<span class="string">&lsquo;Cameraman.tif &ndash; Mean Shift&rsquo;</span>,<span class="string">&lsquo;FontSize&rsquo;</span>,16);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_meanShift_2013_06_13_03.png" alt=""> <h2>Determining Stopping<a name="6"></a></h2><p>Determining convergence in a mean shift scenario can actually be pretty subtle, the code we provide is based on</p><p><a href="http://dl.acm.org/citation.cfm?id=1143864"><a href="http://dl.acm.org/citation.cfm?id=1143864">http://dl.acm.org/citation.cfm?id=1143864</a></a>   Fast Nonparametric Clustering with Gaussian Blurring Mean-Shift       Miguel A. Carreira-Perpinan ICML 2006</p><h2>Conclusion<a name="7"></a></h2><p>That&rsquo;s all for now.  If you have the bio-informatics toolbox, have fun with prtClusterMeanShift.  If you don&rsquo;t, we need to find or write a replacement for graphconncomp to de-couple MeanShift from bioinformatics. One day, hopefully.</p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using K-Means As a Feature Extractor]]></title>
    <link href="http://newfolder.github.io/blog/2013/06/03/using-k-means-as-a-feature-extractor/"/>
    <updated>2013-06-03T11:48:00-04:00</updated>
    <id>http://newfolder.github.io/blog/2013/06/03/using-k-means-as-a-feature-extractor</id>
    <content type="html"><![CDATA[<p>Hi everyone,</p>


<p>Today we&#8217;d like to talk about using K-Means as a non-linear feature extraction algorithm.  This is becoming a pretty popular way to deal with a number of classification tasks, since K-means followed by linear classification is relatively easy to paralellize and works well on very large data sets.</p>


<p>We&#8217;ll leave the large data set processing to another time, and for now, just look at a new prtPreProc object - prtPreProcKmeans</p>




<!--/introduction-->


<h2>Contents</h2>


<div><ul><li><a href="#1">prtPreProcKmeans</a></li><li><a href="#2">Combining with Linear Classification</a></li><li><a href="#3">Visualizing</a></li><li><a href="#4">Wrapping Up</a></li></ul></div>


<h2>prtPreProcKmeans<a name="1"></a></h2>


<p>You may be used to using prtClusterKmeans previously, and wonder why we need prtPreProcKmeans - the answer is a little subtle.  prtCluster* objects are expected to output the max a-posteriori cluster assignments. But for feature extraction, we actually want to output the distances from each observation to each cluster center (vs. the class outputs).  You can see the difference in the following:</p>


<pre class="codeinput">ds = prtDataGenBimodal;
cluster = prtClusterKmeans(<span class="string">'nClusters'</span>,4);
preProc = prtPreProcKmeans(<span class="string">'nClusters'</span>,4);

cluster = cluster.train(ds);
preProc = preProc.train(ds);
dsCluster = cluster.run(ds);
dsPreProc = preProc.run(ds);

subplot(1,2,1);
imagesc(dsCluster);
title(<span class="string">'Cluster Assignments'</span>);
subplot(1,2,2);
imagesc(dsPreProc);
title(<span class="string">'Cluster Distances'</span>);
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_2013_06_03_01.png" alt=""> <h2>Combining with Linear Classification<a name="2"></a></h2><p>We can combine prtPreProcKmeans with any classifier &ndash; let&rsquo;s try with a logistic discriminant, and see how well we can do:</p><pre class="codeinput">algoSimple = prtClassLogisticDiscriminant;
algoKmeans = prtPreProcKmeans(<span class="string">&lsquo;nClusters&rsquo;</span>,4) + prtClassLogisticDiscriminant;</p>

<p>yOutSimple = kfolds(algoSimple,ds,5);
yOutKmeans = kfolds(algoKmeans,ds,5);</p>

<p>yOutAll = catFeatures(yOutSimple,yOutKmeans);
[pf,pd] = prtScoreRoc(yOutAll);
subplot(1,1,1);
h = prtUtilCellPlot(pf,pd);
set(h,<span class="string">&lsquo;linewidth&rsquo;</span>,3);
legend(h,{<span class="string">&lsquo;Log Disc&rsquo;</span>,<span class="string">&lsquo;K-Means + Log-Disc&rsquo;</span>});
xlabel(<span class="string">&lsquo;Pfa&rsquo;</span>);
ylabel(<span class="string">&lsquo;Pd&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_2013_06_03_02.png" alt=""> <h2>Visualizing<a name="3"></a></h2><p>We can visualize the resulting decision boundary using a hidden (and undocumented method) of prtAlgorithm, that lets us plot algorithms as though they were classifiers as long as certain conditions are met.</p><p>Here&rsquo;s an example:</p><pre class="codeinput">algoKmeans = algoKmeans.train(ds);
algoKmeans.plotAsClassifier;
title(<span class="string">&lsquo;K-Means + Logistic Discriminant&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_2013_06_03_03.png" alt=""> <h2>Wrapping Up<a name="4"></a></h2><p>K-Means pre-processing is a potentially powerful way to combine simple clustering and simple classification algorithms to form powerful non-linear classifiers.</p><p>We&rsquo;re working on some big additions to the PRT in the next few weeks&hellip; especially dealing with very large data sets.  Stay tuned.</p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[New Visualization with IMAGESC]]></title>
    <link href="http://newfolder.github.io/blog/2013/05/21/new-visualization-with-imagesc/"/>
    <updated>2013-05-21T08:42:00-04:00</updated>
    <id>http://newfolder.github.io/blog/2013/05/21/new-visualization-with-imagesc</id>
    <content type="html"><![CDATA[<p>In the last entry, we introduced a data set - the Cylinder-Bell-Funnel data set, prtDataGenCylinderBellFunnel.  To visualize it easily, we used the MATLAB function imagesc, which makes an image out of the data, with automatically determined colormap settings.  Today we&#8217;ll expand on that, and make the process a lot easier.
    
</p>


<!--/introduction-->


<h2>Contents</h2>


<div><ul><li><a href="#1">Example</a></li><li><a href="#2">Other Data Sets</a></li><li><a href="#3">Wrapping Up</a></li></ul></div>


<h2>Example<a name="1"></a></h2>


<p>For a lot of high-dimensional data sets, it turns out creating an observations x features image of the data is a great way to visualize and understand your data.  This week we made that process a little easier and cleaner by introducing a method of prtDataSetClass - imagesc.</p>


<p>The method takes care of a number of things that were a little tricky to do previously - first, it makes sure the observations are sorted by class index, next it creates an image of all the data with black bars denoting the class boundaries, and finally, it makes the y-tick-marks contain the relevant class names.</p>


<p>It&#8217;s now easy to generate clean visualizations like so:</p>


<pre class="codeinput">ds = prtDataGenCylinderBellFunnel;
ds.imagesc;
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_imagesc_2013_05_21_01.png" alt=""> <h2>Other Data Sets<a name="2"></a></h2><p>Of course, you can do the same thing with other data sets, too.  Look at how easy it is to see which features are important in prtDataGenFeatureSelection:</p><pre class="codeinput">ds = prtDataGenFeatureSelection;
ds.imagesc;
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_imagesc_2013_05_21_02.png" alt=""> <h2>Wrapping Up<a name="3"></a></h2><p>That&rsquo;s it for this week.  We use imagesc-based visualization all the time, and hopefully you&rsquo;ll find it interesting and useful, too.</p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[prtDataGenSandP500 and prtDataGenCylinderBellFunnel]]></title>
    <link href="http://newfolder.github.io/blog/2013/05/15/prtdatagensandp500-and-prtdatagencylinderbellfunnel/"/>
    <updated>2013-05-15T07:50:00-04:00</updated>
    <id>http://newfolder.github.io/blog/2013/05/15/prtdatagensandp500-and-prtdatagencylinderbellfunnel</id>
    <content type="html"><![CDATA[<p>Hi everyone, a quick update this time &ndash; we added two new prtDataGen*
functions to the PRT that people might find useful &ndash; prtDataGenSandP500,
and prtDataGenCylinderBellFunnel.</p>

<!--/introduction-->


<h2>Contents</h2>


<div><ul><li><a href="#1">prtDataGenSandP500</a></li><li><a href="#3">Cylinder-Bell-Funnel</a></li><li><a href="#4">Conclusion</a></li></ul></div>


<h2>prtDataGenSandP500<a name="1"></a></h2>


<p>prtDataGenSandP500 generates data containing stock-price information from the S&amp;P 500.  The information dates back to January 3, 1950, and includes the index&#8217;s open, close, volume, and other features.</p>


<p>Check it out:</p>


<pre class="codeinput">ds = prtDataGenSandP500;
ds.featureNames
spClose = ds.retainFeatures(5);
plot(spClose.X,<span class="string">'linewidth'</span>,2);
title(<span class="string">'S&amp;P 500 Closing Value vs. Days since 1/3/1950'</span>);
</pre>


<pre class="codeoutput">
ans = 

    'Date'    'Open'    'High'    'Low'    'Close'    'Volume'    'AdjClose'

</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_dataGen_2013_05_14_01.png" alt=""> <p>If you can do decent prediction on that data&hellip; you might be able to make some money :)</p><h2>Cylinder-Bell-Funnel<a name="3"></a></h2><p>prtDataGenCylinderBellFunnel is a tool for generating a synthetic data set which contains a number of time-series, each of which has either a flat plateau (cylinder), a rising (bell) or a falling (funnel) slope.</p><p>You can find the specification we used to generate the data here: <a href="http://www.cse.unsw.edu.au/~waleed/phd/html/node119.html"><a href="http://www.cse.unsw.edu.au/~waleed/phd/html/node119.html">http://www.cse.unsw.edu.au/~waleed/phd/html/node119.html</a></a></p><p>And the data was used in an important paper in the data-mining community &ndash; Keogh and Lin, Clustering of Time Series Subsequences is Meaningless: Implications for Previous and Future Research. <a href="http://www.cs.ucr.edu/~eamonn/meaningless.pdf"><a href="http://www.cs.ucr.edu/~eamonn/meaningless.pdf">http://www.cs.ucr.edu/~eamonn/meaningless.pdf</a></a></p><pre class="codeinput">ds = prtDataGenCylinderBellFunnel;
imagesc(ds.X);
title(<span class="string">&lsquo;Cylinders (1:266), Bells (267:532), and Funnels (533:798)&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_dataGen_2013_05_14_02.png" alt=""> <h2>Conclusion<a name="4"></a></h2><p>That&rsquo;s all for now.  Hope you enjoy these new data sets, we&rsquo;re always adding new data to the PRT; let us know what you&rsquo;d like to see!</p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[prtKernel]]></title>
    <link href="http://newfolder.github.io/blog/2013/04/22/prtkernel/"/>
    <updated>2013-04-22T15:37:00-04:00</updated>
    <id>http://newfolder.github.io/blog/2013/04/22/prtkernel</id>
    <content type="html"><![CDATA[<p>One of the commonly overlooked &#8220;actions&#8221; in the PRT is prtKernel. They are used internally in several places (RVM, SVM) but they can also be used by themselves. Let&#8217;s talk about em.</p>




<h2>Contents</h2>


<div><ul><li><a href="#1">Just Another Action</a></li><li><a href="#3">Let&#8217;s see what some other kernel transformations look like.</a></li><li><a href="#5">Kernel Sets</a></li><li><a href="#7">Inside the RVM</a></li><li><a href="#9">Outside of the RVM</a></li><li><a href="#10">Conclusions</a></li></ul></div>


<h2>Just Another Action<a name="1"></a></h2>


<p>Kernels have very precise meanings in certain contexts (Mercer kernels for example) so it is important that we really define what we mean by prtKernel. A prtKernel is a standard prtAction. That means that it supports the train operation, that takes a dataset and outputs a prtKernel with modified parameters, and the run operation, that takes a dataset and outputs modified dataset. What makes prtKernel different than other prtActions is that they typically transform a dataset into a different dimensionality. The new features are usually the distance to a collection of training examples and most kernels differ in their selection of the distance function. The most widely used kernel is the <a href="http://en.wikipedia.org/wiki/Radial_basis_function">radial basis function</a>.</p>


<p>Let&#8217;s look at using prtKernelRbf.</p>


<pre class="codeinput">ds = prtDataGenBimodal;
kernel = prtKernelRbf(<span class="string">'sigma'</span>,2); <span class="comment">% Set the kernel Parameter</span>
trainedKernel = kernel.train(ds); <span class="comment">% Train the kernel using the input data</span>
kernelTransformedData = trainedKernel.run(ds);

subplot(2,1,1)
plot(ds);
subplot(2,1,2)
imagesc(kernelTransformedData.X);
colormap(hot)
title(<span class="string">'Kernel Transformation'</span>);
ylabel(<span class="string">'observation'</span>);
xlabel(<span class="string">'feature'</span>)
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/morton_blog_20130421_01.png" alt=""> <p>You can see in the image space that there is a checkerboard pattern highlighting the multi-modal nature of the data.</p><h2>Let&rsquo;s see what some other kernel transformations look like.<a name="3"></a></h2><pre class="codeinput">kernelRbf = prtKernelRbf(<span class="string">&lsquo;sigma&rsquo;</span>,2);
trainedKernelRbf = kernelRbf.train(ds);
kernelTransformedDataRbf = trainedKernelRbf.run(ds);</p>

<p>subplot(2,2,1)
imagesc(kernelTransformedDataRbf.X);
title(<span class="string">&lsquo;RBF Kernel Transformation&rsquo;</span>);
ylabel(<span class="string">&lsquo;observation&rsquo;</span>);
xlabel(<span class="string">&lsquo;feature&rsquo;</span>)</p>

<p>kernelHyp = prtKernelHyperbolicTangent;
trainedKernelHyp = kernelHyp.train(ds);
kernelTransformedDataHyp = trainedKernelHyp.run(ds);</p>

<p>subplot(2,2,2)
imagesc(kernelTransformedDataHyp.X);
title(<span class="string">&lsquo;Hyperbolic Tangent Kernel Transformation&rsquo;</span>);
ylabel(<span class="string">&lsquo;observation&rsquo;</span>);
xlabel(<span class="string">&lsquo;feature&rsquo;</span>)</p>

<p>kernelPoly = prtKernelPolynomial;
trainedKernelPoly = kernelPoly.train(ds);
kernelTransformedDataPoly = trainedKernelPoly.run(ds);</p>

<p>subplot(2,2,3)
imagesc(kernelTransformedDataPoly.X);
title(<span class="string">&lsquo;Polynomial Kernel Transformation&rsquo;</span>);
ylabel(<span class="string">&lsquo;observation&rsquo;</span>);
xlabel(<span class="string">&lsquo;feature&rsquo;</span>)</p>

<p>kernelDirect  = prtKernelDirect;
trainedKernelDirect = kernelDirect.train(ds);
kernelTransformedDataDirect = trainedKernelDirect.run(ds);</p>

<p>subplot(2,2,4)
imagesc(kernelTransformedDataDirect.X);
title(<span class="string">&lsquo;Direct Kernel Transformation&rsquo;</span>);
ylabel(<span class="string">&lsquo;observation&rsquo;</span>);
xlabel(<span class="string">&lsquo;feature&rsquo;</span>)
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/morton_blog_20130421_02.png" alt=""> <p>You can see how the choice of the kernel (and kernel parameters) can really effect the outcoming feature space. It is also interesting to notice that the direct kernel is not actually a kernel at all. It just uses the data as the output featurespace (essentially doing nothing). This is useful for combining the original feature space with kernel transformed data using kernel sets.</p><h2>Kernel Sets<a name="5"></a></h2><p>Kernels can be combined using the &amp; operator to great prtKernelSets. These perform collumn wise concatonation of several kernels. This allows one to create a single kernel transformation out of several prtKernels. In theory one could use / to make a parallel prtAlgorithm to accomplish the same task but there are several reasons to use &amp; that allow them to work within the prtClassRvm and prtClassSvm to remain efficient at run-time.</p><pre class="codeinput">clf; <span class="comment">% Clear those subplots from earlier</span>
kernel = prtKernelDc &amp; prtKernelRbf(<span class="string">&lsquo;sigma&rsquo;</span>,1) &amp; prtKernelHyperbolicTangent;
trainedKernel = kernel.train(ds); <span class="comment">% Train the kernel using the input data</span>
kernelTransformedData = trainedKernel.run(ds);
imagesc(kernelTransformedData.X);
title(<span class="string">&lsquo;A Kernel Set Transformation&rsquo;</span>);
ylabel(<span class="string">&lsquo;observation&rsquo;</span>);
xlabel(<span class="string">&lsquo;feature&rsquo;</span>)
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/morton_blog_20130421_03.png" alt=""> <p>You can see that the different transformed feature spaces are concatenated together.</p><h2>Inside the RVM<a name="7"></a></h2><p>In prtClassRvm the &ldquo;kernels&rdquo; property can be set to the prtKernel of our choosing. The RVM is essentially a sparse (it tries to have most coefficients be zero) linear classifier that opperates on kernel transformed data. Let&rsquo;s look at some classification results of prtDataGenBimodal using several different choices for the kernel.</p><pre class="codeinput">subplot(2,2,1)
plot(train(prtClassRvm(<span class="string">&lsquo;kernels&rsquo;</span>,prtKernelRbf(<span class="string">&lsquo;sigma&rsquo;</span>,2)),ds))
title(<span class="string">&lsquo;RBF Kernel RVM&rsquo;</span>);</p>

<p>subplot(2,2,2)
plot(train(prtClassRvm(<span class="string">&lsquo;kernels&rsquo;</span>,prtKernelHyperbolicTangent),ds))
title(<span class="string">&lsquo;Hyperbolic Tangent Kernel RVM&rsquo;</span>);</p>

<p>subplot(2,2,3)
plot(train(prtClassRvm(<span class="string">&lsquo;kernels&rsquo;</span>,prtKernelPolynomial),ds))
title(<span class="string">&lsquo;Polynomial Kernel RVM&rsquo;</span>);</p>

<p>subplot(2,2,4)
plot(train(prtClassRvm(<span class="string">&lsquo;kernels&rsquo;</span>,prtKernelDirect),ds))
title(<span class="string">&lsquo;Direct Kernel RVM&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/morton_blog_20130421_04.png" alt=""> <p>As you can see the correct choice for the kernel is very important for robust classifcation. The RBF kenerl is a common choice but even it has the sigma parameter which can grealy impact performance. One interesting variant of the RBF kernel is call prtKernelRbfNeighborhoodScaled. This kernel sets the sigma parameter differently for each data point depending on the local neighborhood of the training point.</p><pre class="codeinput">clf; <span class="comment">% Clear those subplots from earlier</span>
plot(train(prtClassRvm(<span class="string">&lsquo;kernels&rsquo;</span>,prtKernelRbfNeighborhoodScaled),ds))
title(<span class="string">&lsquo;Locally Scaled RBF Kernel RVM&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/morton_blog_20130421_05.png" alt=""> <h2>Outside of the RVM<a name="9"></a></h2><p>In the forum the other day someone asked if we could do non-linear regression with multi-dimensional output. Sadly, the answer is &ldquo;not directly&rdquo; but using kernels you can. By transforming the data to kernel space and then using a linear regression technique you can perform non-linear regression. I wont copy the content over here but check out the answer from the forum. <a href="http://www.newfolderconsulting.com/node/412"><a href="http://www.newfolderconsulting.com/node/412">http://www.newfolderconsulting.com/node/412</a></a></p><h2>Conclusions<a name="10"></a></h2><p>This was a pretty quick overview of things you can do with kernels in the PRT. We don&rsquo;t have every kernel but we have quite a few. If there is something you think we should add let us know.</p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Principal Component Analysis - prtPreProcPca]]></title>
    <link href="http://newfolder.github.io/blog/2013/04/16/principal-component-analysis/"/>
    <updated>2013-04-16T19:56:00-04:00</updated>
    <id>http://newfolder.github.io/blog/2013/04/16/principal-component-analysis</id>
    <content type="html"><![CDATA[<p>Today I&#8217;d like to give a quick tour of how to use PCA in the PRT to easily reduce the dimensionality of your data set in a meaningful, principled way.

</p>


<!--/introduction-->


<h2>Contents</h2>


<div><ul><li><a href="#1">Introduction &amp; Theory</a></li><li><a href="#2">In the PRT</a></li><li><a href="#7">How Many Components?</a></li><li><a href="#8">Normalization</a></li><li><a href="#10">Conclusion</a></li></ul></div>


<h2>Introduction &amp; Theory<a name="1"></a></h2>


<p>Principal component analysis (PCA) is a widely used technique in the statistics and signal processing literature.  Even if you haven&#8217;t heard of PCA, if you know some linear algebra, you may have heard of the singular value decomposition (SVD), or, if you come from the signal processing literature, you&#8217;ve probably heard of the  Karhunen&#8211;Loeve transformation (KLT).  Both of these are identical in form to PCA.  Turns out a lot of different groups have re-created the same algorithm in a lot of different fields!</p>




<p>We won&#8217;t have time to delve into the nitty gritty about PCA here.  For our purposes it&#8217;s enough to say that given a (zero-mean) data set X of nObservations x nFeatures, we often want to find a linear transformation of X, S = X*Z, for a matrix Z of size nPca x nFeatures where:</p>


<pre>1) nPca &lt; nFeatures
2) The resulting data, S, contains "most of the information from" X.</pre>


<p>As you can imagine, the phrase &#8220;most of the information&#8221; is vague, and subject to interpretation&#8230; Mathematically, PCA considers &#8220;most of the information in X&#8221; to be equivalent to &#8220;explains most of the variance in X.  It turns out that this statement of the problem has some very nice mathematical solutions - e.g., the columns of S can be viewed as the dominant eigenvectors in the covariance of X!</p>


<p>You can find our more about PCA on the fabulous wikipedia article: https://en.wikipedia.org/wiki/Principal_component_analysis.</p>


<h2>In the PRT<a name="2"></a></h2>


<p>PCA is implemented in the PRT using prtPreProcPca.  Older versions of prtPreProcPca used to make use of different algorithms for different sized data sets (there are a lot of ways to do PCA quickly depending on matrix dimensions).  Since 2012, we found that the MATLAB function SVDS was beating all of our approaches in terms of speed and accuracy, so have switched over to using SVDS to solve for the principal component vectors.</p>


<p>Let&#8217;s take a quick look at some PCA projections.  First, we&#8217;ll need some data:</p>


<pre class="codeinput">ds = prtDataGenUnimodal;
</pre>


<p>We also need to make a prtPreProcPca object, and we&#8217;ll use 2 components in the PCA projection:</p>


<pre class="codeinput">pca = prtPreProcPca(<span class="string">'nComponents'</span>,2);
</pre>


<p>prtPreProc* objects can be trained and run just like any other objects:</p>


<pre class="codeinput">pca = pca.train(ds);
</pre>


<p>Let&#8217;s visualize the results, first we&#8217;ll look at the original data, and the vectors from the PCA analysis:</p>


<pre class="codeinput">plot(ds);
hold <span class="string">on</span>;
h1 = plot([0 pca.pcaVectors(1,1)],[0,pca.pcaVectors(2,1)],<span class="string">'k'</span>);
h2 = plot([0 pca.pcaVectors(1,2)],[0,pca.pcaVectors(2,2)],<span class="string">'k--'</span>);
set([h1,h2],<span class="string">'linewidth'</span>,3);
hold <span class="string">off</span>;
axis <span class="string">equal</span>;
title(<span class="string">'Original Data &amp; Two PCA Vectors'</span>);
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_2013_04_16_01.png" alt=""> <p>From this plot, we can see that the PCA vectors are oriented first along the dimension of largest variance in the data (diagonal wiht a positive slope), and the second PCA is oriented orthogonal to the first PCA.</p><p>We can project our data onto this space using the RUN method:</p><pre class="codeinput">dsPca = pca.run(ds);
plot(dsPca);
title(<span class="string">&lsquo;PCA-Projected Data&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_2013_04_16_02.png" alt=""> <h2>How Many Components?<a name="7"></a></h2><p>In general, it might be somewhat complicated to determine how many PCA components are necessary to explain most of the variance in a particular data set.  Above we used 2, but for higher dimensional data sets, how many should we use in general?</p><p>We can measure how much variance each PC explains during training by exploring the vector pca.totalPercentVarianceCumulative which is set during training.  This vector contains the percent of the total variance of the data set explained by 1:N PCA components.  For example, totalPercentVarianceCumulative(3) contains the percent variance explained by components 1 through 3.  When this metric plateaus, that&rsquo;s a pretty good sign that we have enough components.</p><p>For example:</p><pre class="codeinput">ds = prtDataGenProstate;
pca = prtPreProcPca(<span class="string">&lsquo;nComponents&rsquo;</span>,ds.nFeatures);
pca = pca.train(ds);</p>

<p>stem(pca.totalPercentVarianceCumulative,<span class="string">&lsquo;linewidth&rsquo;</span>,3);
xlabel(<span class="string">&lsquo;#Components&rsquo;</span>);
ylabel(<span class="string">&lsquo;Percent Variance Explained&rsquo;</span>);
title(<span class="string">&lsquo;Prostate Data &ndash; PCA Percent Variance Explained&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_2013_04_16_03.png" alt=""> <h2>Normalization<a name="8"></a></h2><p>For PCA to be meaningful, the data used has to have zero-mean columns, and prtPreProcPca takes care of that for you (so you don&rsquo;t have to zero mean the columns yourself).  However, different authors disagree about whether or not the columns provided to PCA should all have the same variance before PCA analysis.  Depending on normalization, you can get very different PCA projections.  To leave the option open, the PRT does <b>not</b> automatically normalize the columns of the input data to have uniform variance.  You can manually enforce this before your PCA processing with prtPreProcZmuv.</p><p>Here&rsquo;s a simplified example, where we do the two processes separately to show the differences.</p><pre class="codeinput">ds = prtDataGenProstate;
dsNorm = rt(prtPreProcZmuv,ds);
pca = prtPreProcPca(<span class="string">&lsquo;nComponents&rsquo;</span>,ds.nFeatures);
pca = pca.train(ds);
pcaNorm = pca.train(dsNorm);</p>

<p>subplot(2,1,1);
stem(pca.totalPercentVarianceCumulative,<span class="string">&lsquo;linewidth&rsquo;</span>,3);
xlabel(<span class="string">&lsquo;#Components&rsquo;</span>);
ylabel(<span class="string">&lsquo;Percent Variance Explained&rsquo;</span>);
title(<span class="string">&lsquo;Prostate Data &ndash; PCA Percent Variance Explained&rsquo;</span>);</p>

<p>subplot(2,1,2);
stem(pcaNorm.totalPercentVarianceCumulative,<span class="string">&lsquo;linewidth&rsquo;</span>,3);
xlabel(<span class="string">&lsquo;#Components&rsquo;</span>);
ylabel(<span class="string">&lsquo;Percent Variance Explained&rsquo;</span>);
title(<span class="string">&lsquo;Prostate Data &ndash; PCA Percent Variance Explained (Normalized Data)&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_2013_04_16_04.png" alt=""> <p>As you can see, processing normalized and un-normalized data results in quite different assessments of how many PCA components are required to summarize the data.</p><p>Our recommendation is that if your data comes from different sources, with different sensor ranges or variances (as in the prostate data), it&rsquo;s imperative that you perform standard-deviation normalization prior to PCA processing.</p><p>Otherwise, it&rsquo;s worthwhile to try both with and without ZMUV pre-processing and see what gives better performance.</p><h2>Conclusion<a name="10"></a></h2><p>That&rsquo;s about it for PCA processing.  Of course, you can use PCA as a pre-processor for any algorithm you&rsquo;re developing, to reduce the dimensionality of your data, for example:</p><pre>algo = prtPreProcPca + prtClassLibSvm;</pre><p>Let us know if you have questions or comments about using prtPreProcPca.</p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[rt()]]></title>
    <link href="http://newfolder.github.io/blog/2013/04/08/rt/"/>
    <updated>2013-04-08T13:04:00-04:00</updated>
    <id>http://newfolder.github.io/blog/2013/04/08/rt</id>
    <content type="html"><![CDATA[<p>In the PRT we have a a few hidden features that we don&#8217;t really advertise but at times they make life easier. This quick post is to reveal one of those quick features, <tt>prtAction.rt()</tt></p>


<p><tt>rt()</tt> stands run, train. You can use it to perform a &#8220;run on train&#8221; operation and you only the output dataset. In other words, it&#8217;s a quick and dirty method for when you would otherwise need to call <tt>run(train(action, dataset), dataset)</tt>.  To see how one might use <tt>rt()</tt>, let&#8217;s calculated the first two principal components of Fisher&#8217;s Iris dataset.</p>


<pre class="codeinput">ds = prtDataGenIris;
pca = train(prtPreProcPca(<span class="string">'nComponents'</span>,2), ds);
dsPca = run(pca, ds);
</pre>


<p>If we didn&#8217;t really care about keeping the (trained) PCA object around we could have done this all in one line.</p>


<pre class="codeinput">dsPca = run(train(prtPreProcPca(<span class="string">'nComponents'</span>,2), ds),ds);
</pre>


<p>That string at the end <tt>ds),ds)</tt> is odd looking and this is when <tt>rt()</tt> comes to the rescue.</p>


<pre class="codeinput">dsPca = rt(prtPreProcPca(<span class="string">'nComponents'</span>,2), ds);
</pre>


<p>That&#8217;s how you can use <tt>rt()</tt> in a nutshell. It&#8217;s a nice method to keep in your back pocket when you are just beginning to explore a dataset and cross-validation isn&#8217;t yet on your mind. Remember <tt>rt()</tt> is a (hidden) method of <tt>prtAction</tt> and therefore can be used with all classifiers, pre-processors etc. Let us know if you find any use for it. We do.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Determining Gender from Handwriting - A Kaggle Competition!]]></title>
    <link href="http://newfolder.github.io/blog/2013/04/02/determining-gender-from-handwriting/"/>
    <updated>2013-04-02T18:21:00-04:00</updated>
    <id>http://newfolder.github.io/blog/2013/04/02/determining-gender-from-handwriting</id>
    <content type="html"><![CDATA[<p>Hi everyone, today I wanted to introduce a new data set and some preliminary processing that helps us perform better than a random forest (gasp!).</p>


<p>The data we&#8217;re going to use is from a Kaggle competition that&#8217;s going on from now (March 28, 2013) until April 15, 2013.  Kaggle is a company that specializes in connecting data analysts with interesting data - it&#8217;s pretty great for hobbyists and individuals to get started with some data, and potentially win some money!  And they just generally have a lot of cool data from a lot of interesting problems.</p>


<p>The data we&#8217;re going to use is based on identifying an author&#8217;s gender from samples of their handwriting.  Here&#8217;s the URL for the competition home page, which gives some details on the data:</p>


<pre class="language-matlab"><a href="http://www.kaggle.com/c/icdar2013-gender-prediction-from-handwriting">http://www.kaggle.com/c/icdar2013-gender-prediction-from-handwriting</a>
    
</pre>


<p>The competition includes several sets of images,as well as some pre-extracted features.  The image files can be gigantic, so we&#8217;re only going to use the pre-extracted features for today.  Go ahead and download train.csv, train_answers.csv, and test.csv, from the link above, and put them in</p>


<pre class="codeinput">fullfile(prtRoot,<span class="string">'dataGen'</span>,<span class="string">'dataStorage'</span>,<span class="string">'kaggleTextGender_2013'</span>);
</pre>


<p>Once the files are in the correct location, you should be able to use:</p>


<pre class="codeinput">[dsTrain,dsTest] = prtDataGenTextGender;</pre>


<p>to load in the data.</p>


<!--/introduction-->


<h2>Contents</h2>


<div><ul><li><a href="#1">M-Files You Need</a></li><li><a href="#4">Naive Random Forest</a></li><li><a href="#6">Remove Meaningless features</a></li><li><a href="#8">Slight improvement</a></li><li><a href="#10">Aggregating Over Writers</a></li><li><a href="#13">PLSDA</a></li><li><a href="#14">Plotting Results</a></li><li><a href="#16">Submit it!</a></li><li><a href="#19">Results</a></li><li><a href="#21">Feature Selection</a></li><li><a href="#23">Adding in some post-processing</a></li><li><a href="#25">Our New Submission</a></li><li><a href="#29">Final Results</a></li></ul></div>


<h2>M-Files You Need<a name="1"></a></h2>


<p>Obviously, prtDataGenTextGender.m is new, as are a number of other files we&#8217;re going to use throughout this example.  These include prtEvalLogLoss.m, prtScoreLogLoss.m, prtUtilAccumArrayLike.m, and prtUtilAccumDataSetLike.m.  You&#8217;ll need to update your PRT to the newest version (as of March, 2013, anyway) to get access to these files.  You can always get the PRT here: <a href="http://github.com/newfolder/PRT">http://github.com/newfolder/PRT</a></p>


<p>Once you&#8217;ve done all that, go ahead and try the following:</p>


<pre class="codeinput">
[dsTrain,dsTest] = prtDataGenTextGender;
</pre>




<p>That should load in the data.  As always, we can visualize the data using someting simple, like PCA:</p>


<pre class="codeinput">pca = prtPreProcPca;
pca = pca.train(dsTrain);
dsPca = pca.run(dsTrain);
plot(dsPca);
title(<span class="string">'Kaggle Handwriting/Gender ICDAR 2013 Data'</span>);
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Kaggle_TextGender_01.png" alt=""> <h2>Naive Random Forest<a name="4"></a></h2><p>Kaggle competitions will often provide a baseline performance metric for some standard classification algorithms.  In this example they told us that the baseline random forest performamce they&rsquo;ve observed obtains a log-loss of about 0.65.  We can confirm this using our random forest, 3-fold cross-validation, and our new function prtScoreLogLoss:</p><pre class="codeinput">yOut = kfolds(prtClassTreeBaggingCap,dsTrain,3);
logLossInitialRf = prtScoreLogLoss(yOut);
fprintf(<span class="string">&lsquo;Random Forest LogLoss: %.2f\n&rsquo;</span>,logLossInitialRf);
</pre><pre class="codeoutput">Random Forest LogLoss: 0.64
</pre><p>About 0.65, so we&rsquo;re right in the ball-park.  Can we do better?</p><h2>Remove Meaningless features<a name="6"></a></h2><p>That performance wasn&rsquo;t that great.  And the leaderboard shows us that some clever people have already done significantly better than the basic random forest.</p><p>Let&rsquo;s investigate the data a little and see what&rsquo;s going on.  First, what is the standard deviation of the features?</p><pre class="codeinput">stem(log(std(dsTrain.X)));
xlabel(<span class="string">&lsquo;Feature Number&rsquo;</span>);
ylabel(<span class="string">&lsquo;Log-\sigma&rsquo;</span>);
title(<span class="string">&lsquo;Log(\sigma) vs. Feature Number&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Kaggle_TextGender_02.png" alt=""> <p>Wow, there are a lot of features with a standard deviation of zero!  That means that we can&rsquo;t learn anything from these features, since they always take the exact same value in the training set.  Let&rsquo;s go ahead and remove these features.</p><pre class="codeinput">fprintf(<span class="string">&lsquo;There are %d features that only take one value&hellip; \n&rsquo;</span>,length(find(std(dsTrain.X)==0)));
removeFeats = std(dsTrain.X) == 0;
dsTrainRemove = dsTrain.removeFeatures(removeFeats);
dsTestRemove = dsTest.removeFeatures(removeFeats);
</pre><pre class="codeoutput">There are 2414 features that only take one value&hellip;
</pre><h2>Slight improvement<a name="8"></a></h2><p>What happens if we re-run the random forest on this data with the new features removed?  The random forest is pretty robust to meaningless features, but not totally impervious&hellip; let&rsquo;s try it:</p><pre class="codeinput">yOutRf = kfolds(prtClassTreeBaggingCap,dsTrainRemove,3);
logLossRfFeatsRemoved = prtScoreLogLoss(yOutRf);
fprintf(<span class="string">&lsquo;Random Forest LogLoss with meaningless features removed: %.2f\n&rsquo;</span>,logLossRfFeatsRemoved);
</pre><pre class="codeoutput">Random Forest LogLoss with meaningless features removed: 0.61
</pre><p>Hey!  That did marginally better &ndash; our log-loss went from about 0.65 to 0.61 or so.  Nothing to write home about, but a slight improvement.  What else can we do?</p><h2>Aggregating Over Writers<a name="10"></a></h2><p>If you pay attention to the data set, you&rsquo;ll notice something interesting &ndash; we have a lot of writing samples (4) from each writer.  And our real goal is to identify the gender of each writer &ndash; so we should be able to average our classifications over each writer and get better performance.</p><p>This blog entry introduces a new function called &ldquo;prtUtilAccumDataSetLike&rdquo;, which acts a lot like &ldquo;accumarray&rdquo; in base MATLAB.  Basically, prtUtilAccumDataSetLike takes a set of keys of size dataSet.nObservations x 1, and for each observation corresponding to each unique key, aggregates the data in X and Y and outputs a new data set.</p><p>It&rsquo;s a little complicated to explain &ndash; take a look at the help entry for accumarray, and then take a look at this example:</p><pre class="codeinput">writerIds = [dsTrainRemove.observationInfo.writerId]&lsquo;;
yOutAccum = prtUtilAccumDataSetLike(writerIds,yOutRf,@(x)mean(x));
</pre><p>The code above outputs a new data set generated by averaging the confidences in yOutRf across sets of writerIds.</p><p>Does this help performance?</p><pre class="codeinput">logLossAccum = prtScoreLogLoss(yOutAccum);
fprintf(<span class="string">'Writer ID Accumulated Random Forest LogLoss: %.2f\n&rsquo;</span>,logLossAccum);
</pre><pre class="codeoutput">Writer ID Accumulated Random Forest LogLoss: 0.59
</pre><p>That&rsquo;s marginally better still!  What else can we try&hellip;</p><h2>PLSDA<a name="13"></a></h2><p>When a random forest seems to be doing somewhat poorly, often it&rsquo;s a good idea to take a step back and run a linear classifier in lieu of a nice fancy random forest.  I&rsquo;m partial to PLSDA as a classifier (see the help entry for prtClassPlsda for more information).</p><p>PLSDA has one parameter &ndash; the number of components to use, that we should optimize over.  Since each kfolds-run is random, we&rsquo;ll run 10 experiments of 3-Fold Cross-validation for each of 1 &ndash; 30 components in PLSDA&hellip; This might take a little while depending on your computer&hellip;</p><p>We&rsquo;re also going to do something a little tricky here &ndash; PLSDA is a linear classifier, and won&rsquo;t output values between zero and one by default.  But the outputs from PLSDA should be linearly correlated with confidence that the author of a particular text was a male.  We can translate from PLSDA outputs to values with probabilistic interpretations by attaching a logistic-discriminant function to the end of our PLSDA classifier. That&rsquo;s easy to do in the PRT like so:</p><pre>classifier = prtClassPlsda(&lsquo;nComponents&rsquo;,nComp) + prtClassLogisticDiscriminant;</pre><pre class="codeinput">nIter = 10;
maxComp = 30;
logLossPlsda = nan(maxComp,nIter);
logLossPlsdaAccum = nan(maxComp,nIter);
<span class="keyword">for</span> nComp = 1:maxComp;</p>

<pre><code>classifier = prtClassPlsda(&lt;span class="string"&gt;'nComponents'&lt;/span&gt;,nComp) + prtClassLogisticDiscriminant;
classifier.showProgressBar = false;
&lt;span class="keyword"&gt;for&lt;/span&gt; iter = 1:nIter
    yOutPlsda = kfolds(classifier,dsTrainRemove,3);
    logLossPlsda(nComp,iter) = prtScoreLogLoss(yOutPlsda);

    yOutAccum = prtUtilAccumDataSetLike(writerIds,yOutPlsda,@(x)mean(x));
    logLossPlsdaAccum(nComp,iter) = prtScoreLogLoss(yOutAccum);
&lt;span class="keyword"&gt;end&lt;/span&gt;
fprintf(&lt;span class="string"&gt;'%d '&lt;/span&gt;,nComp);
</code></pre>

<p><span class="keyword">end</span>
fprintf(<span class="string">&lsquo;\n&rsquo;</span>);
</pre><pre class="codeoutput">1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
</pre><h2>Plotting Results<a name="14"></a></h2><p>Let&rsquo;s take a look at the PLSDA classifier performance as a function of the number of components we used.  The following code generates box-plots (recall, we ran 3-fold cross-validation 10 times for each # of components between 1 and 30&hellip;</p><pre class="codeinput">boxplot(logLossPlsdaAccum')
hold <span class="string">on</span>;
h2 = plot(1:maxComp,repmat(logLossInitialRf,1,maxComp),<span class="string">&lsquo;k:&rsquo;</span>,1:maxComp,repmat(logLossRfFeatsRemoved,1,maxComp),<span class="string">&lsquo;b:&rsquo;</span>,1:maxComp,repmat(logLossAccum,1,maxComp),<span class="string">&lsquo;g:&rsquo;</span>);
hold <span class="string">off</span>;
legend(h2,{<span class="string">&lsquo;Random Forest Log-Loss&rsquo;</span>,<span class="string">&lsquo;Random Forest &ndash; Removed Features&rsquo;</span>,<span class="string">&lsquo;Random Forest &ndash; Removed Features &ndash; Accum&rsquo;</span>});
h = findobj(gca,<span class="string">&lsquo;type&rsquo;</span>,<span class="string">&lsquo;line&rsquo;</span>);
set(h,<span class="string">&lsquo;linewidth&rsquo;</span>,2);
xlabel(<span class="string">&lsquo;#PLSDA Components&rsquo;</span>);
ylabel(<span class="string">&lsquo;Log-Loss&rsquo;</span>);
title(<span class="string">&lsquo;Log-Loss For PLSDA With Accumumation (vs. # Components) and Random Forest&rsquo;</span>)
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Kaggle_TextGender_03.png" alt=""> <p>Wow!  The dotted lines here represent the random forest performance we&rsquo;ve seen, and the boxes represent the performance we get with PLSDA &ndash; PLSDA is significantly outperforming our RF classifier on this data!</p><p>PLSDA performance seems to plateau around 17 components, so we&rsquo;ll use 17 from now on.</p><h2>Submit it!<a name="16"></a></h2><p>I think we might have something here &ndash; our code gets Log-Losses around 0.46 many times.  Let&rsquo;s actually submit an experiment to Kaggle.</p><p>First we&rsquo;ll train our classifier and test it:</p><pre class="codeinput">classifier = prtClassPlsda(<span class="string">&lsquo;nComponents&rsquo;</span>,17) + prtClassLogisticDiscriminant;
classifier = classifier.train(dsTrainRemove);
yOutTest = classifier.run(dsTestRemove);</p>

<p>writerIdsTest = [dsTestRemove.observationInfo.writerId]&lsquo;;
</pre><p>Don&rsquo;t forget to accumulate:</p><pre class="codeinput">[yOutPlsdaTestAccum,uKeys] = prtUtilAccumDataSetLike(writerIdsTest,yOutTest,@(x)mean(x));
matrixOut = cat(2,uKeys,yOutPlsdaTestAccum.X);
</pre><p>And write the output the way Kaggle wants us to.</p><pre class="codeinput">csvwrite(<span class="string">'outputPlsda.csv&rsquo;</span>,matrixOut);
</pre><h2>Results<a name="19"></a></h2><p>I made a screen-cap of the results from the output above &ndash; here it is:</p><pre class="codeinput">imshow(<span class="string">&lsquo;leaderBoard_2013_03_20.PNG&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Kaggle_TextGender_04.PNG" alt=""> <p>That&rsquo;s me in the middle there &ndash; #38 out of about 100.  And way better than the naive random forest implementation &ndash; not too bad!</p><p>Can we do better?</p><h2>Feature Selection<a name="21"></a></h2><p>One way we can reduce variation and improve performance is to not include all 4652 features left in our data set.  We can use feature selection to pick the ones we want!</p><p>I&rsquo;m going to go ahead and warn you &ndash; don&rsquo;t run this code unless you want to leave it running overnight.  It takes forever&hellip;  but it gets the job done:</p></p>

<pre class="codeinput">
warning <span class="string">off</span>;
c = prtClassPlsda(<span class="string">'nComponents' </span>,17,<span class="string"> 'showProgressBar'</span>,false);
sfs = prtFeatSelSfs(<span class="string">'nFeatures'</span>,100,<span class="string"> 'evaluationMetric'</span>,@(ds)-1*prtEvalLogLoss(c,ds,2));
sfs = sfs.train(dsTrainRemove);
</pre>


<p>Instead, we already ran that code, and saved the results in sfs.mat, which you can down-load at the end of this post.</p>


<p>For now, let&#8217;s look at how performance is affected by the number of features retained:</p>


<pre class="codeinput">load <span class="string">sfs.mat</span> <span class="string">sfs</span>
set(gcf,<span class="string">'position'</span>,[403   246   560   420]); <span class="comment">%fix from IMSHOW</span>
plot(-sfs.performance)
xlabel(<span class="string">'# Features'</span>);
ylabel(<span class="string">'Log-Loss'</span>);
title(<span class="string">'Log-Loss vs. # Features Retained'</span>);
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Kaggle_TextGender_05.png" alt=""> <p>It looks like performance is bottoming out around 60 or so features, and anything past that isn&rsquo;t adding performance (though maybe if we selected 1000 or 2000 we could do better!)</p><p>We can confirm this with the following code, which also takes quite a while to run a bunch of experiments on all the sub-sets SFS found for us:</p><pre class="codeinput">logLossClassifierFeatSel = nan(100,10);
<span class="keyword">for</span> nFeats = 1:100;</p>

<pre><code>&lt;span class="keyword"&gt;for&lt;/span&gt; iter = 1:10
    dsTrainRemoveFeatSel = dsTrainRemove.retainFeatures(sfs.selectedFeatures(1:nFeats));
    yOutPlsdaFeatSel = classifier.kfolds(dsTrainRemoveFeatSel,3);

    xOutAccum = prtUtilAccumArrayLike(writerIds,yOutPlsdaFeatSel.X,[],@(x)mean(x));
    yOutAccum = prtUtilAccumArrayLike(writerIds,yOutPlsdaFeatSel.Y,[],@(x)unique(x));
    yOutAccumFeatSel = prtDataSetClass(xOutAccum,yOutAccum);
    logLossClassifierFeatSel(nFeats,iter) = prtScoreLogLoss(yOutAccumFeatSel);
&lt;span class="keyword"&gt;end&lt;/span&gt;
</code></pre>

<p><span class="keyword">end</span></p>

<p>boxplot(logLossClassifierFeatSel&#8217;)
drawnow;</p>

<p>ylabel(<span class="string">&lsquo;Log-Loss&rsquo;</span>);
xlabel(<span class="string">&lsquo;# Features&rsquo;</span>)
title(<span class="string">&lsquo;Log-Loss vs. # Features&rsquo;</span>)
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Kaggle_TextGender_06.png" alt=""> <h2>Adding in some post-processing<a name="23"></a></h2><p>In a minute we&rsquo;ll down-select the number of features we want to use &ndash; but first let&rsquo;s do one more thing.  Recall that we added a logistic discriminant at the end of our PLSDA classifier.  That was clever, but after that, we accumulted a bunch of data together.  We might be able to run <b>another</b> logistic discriminant after the accumultion to do even better!</p><p>Let&rsquo;s see what that code looks like:</p><pre class="codeinput">classifier = prtClassPlsda(<span class="string">&lsquo;nComponents&rsquo;</span>,17) + prtClassLogisticDiscriminant;
classifier = classifier.train(dsTrainRemove);
yOutPlsdaKfolds = classifier.kfolds(dsTrainRemove,3);</p>

<p>yOutAccum = prtUtilAccumDataSetLike(writerIds,yOutPlsdaKfolds,@(x)mean(x));
yOutAccumLogDisc = kfolds(prtClassLogisticDiscriminant,yOutAccum,3);</p>

<p>logLossPlsdaAccum = prtScoreLogLoss(yOutAccum);
logLossPlsdaAccumLogDisc = prtScoreLogLoss(yOutAccumLogDisc);
fprintf(<span class="string">&lsquo;Without post-Log-Disc: %.3f; With: %.3f\n&rsquo;</span>,logLossPlsdaAccum,logLossPlsdaAccumLogDisc);
</pre><pre class="codeoutput">Without post-Log-Disc: 0.479; With: 0.461
</pre><p>That&rsquo;s a slight improvement, too!</p><h2>Our New Submission<a name="25"></a></h2><p>Let&rsquo;s put everything together, and see what happens:</p><p>First, pick the right # of features based on our big experiment above:</p><pre class="codeinput">[minVal,nFeatures] = min(mean(logLossClassifierFeatSel'));
dsTrainTemp = dsTrainRemove.retainFeatures(sfs.selectedFeatures(1:nFeatures));
dsTestTemp = dsTestRemove.retainFeatures(sfs.selectedFeatures(1:nFeatures));
</pre><p>Now, train a classifier, and a logistic discriminant:</p><pre class="codeinput">classifier = prtClassPlsda(<span class="string">&lsquo;nComponents&rsquo;</span>,17) + prtClassLogisticDiscriminant;
classifier = classifier.train(dsTrainRemove);
yOutPlsdaKfolds = classifier.kfolds(dsTrainRemove,3);</p>

<p>yOutAccum = prtUtilAccumDataSetLike(writerIds,yOutPlsdaKfolds,@(x)mean(x));
yOutPostLogDisc = kfolds(prtClassLogisticDiscriminant,yOutAccum,3);
postLogDisc = train(prtClassLogisticDiscriminant,yOutAccum);
logLossPlsdaEstimate = prtScoreLogLoss(yOutPostLogDisc);
</pre><p>And run the same classifier, followed by the post-processing logistic discriminant:</p><pre class="codeinput">yOut = classifier.run(dsTestRemove);
[xOutAccumSplit,uLike] = prtUtilAccumDataSetLike(writerIdsTest,yOut,@(x)mean(x));
dsTestPost = prtDataSetClass(xOutAccumSplit);
yOutPost = postLogDisc.run(dsTestPost);</p>

<p>matrixOut = cat(2,uLike,yOutPost.X);
csvwrite(<span class="string">&lsquo;outputPlsda_FeatSelPostProc.csv&rsquo;</span>,matrixOut);
</pre><h2>Final Results<a name="29"></a></h2><p>We submitted this version to Kaggle also.  The results are shown below:</p><pre class="codeinput">imshow(<span class="string">&lsquo;leaderBoard_2013_03_21.PNG&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Kaggle_TextGender_07.PNG" alt=""> <p>That bumped us up by just a little bit in terms of overall log-loss, but quite a bit in the leader-list!</p><p>A lot of people are still doing way better than this blog entry (and have gotten better since a week ago, when we did this analysis!), but that&rsquo;s not bad performance for what turns out to be about 20 lines of code, don&rsquo;t you think?</p><p>If you have any success with the text/gender analysis data using the PRT, let us know &ndash; either post on the Kaggle boards, or here, or drop us an e-mail.</p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[RVs Part 2 - Mixture Models]]></title>
    <link href="http://newfolder.github.io/blog/2013/03/24/rvs-part-2/"/>
    <updated>2013-03-24T11:03:00-04:00</updated>
    <id>http://newfolder.github.io/blog/2013/03/24/rvs-part-2</id>
    <content type="html"><![CDATA[<p>In <a href="http://newfolder.github.io/blog/2013/01/31/random-variables-part-1/">part 1</a> we talked about how use RVs to change the statistical model used by prtClassMap so that we can flexibly model the data. If you recall we can set the &#8220;rvs&#8221; property to correspond to prtRvMvn to model the data using multi-variate normal distributions or we can set it some other value to change the resulting model and thus the classifier. Now, we are going to talk about how we can make probabilistic mixtures in much the same way.</p>


<!--/introduction-->


<h2>Contents</h2>


<div><ul><li><a href="#1">Mixture Models</a></li><li><a href="#2">prtRvMixture</a></li><li><a href="#6">Using prtRvMixtures for Classification</a></li><li><a href="#8">prtRvGmm</a></li><li><a href="#10">Using prtRvGmm for classification</a></li><li><a href="#13">Conclusions</a></li></ul></div>


<h2>Mixture Models<a name="1"></a></h2>


<p>In general, the term &#8220;mixture model&#8221; implies that each observation of data has an associated hidden variable and that observation is drawn from a distribution that is dependent on that hidden variable. In general statistics, a mixture model can have either continuous or discrete hidden variables. In the PRT, our prtRvMixture only considers discrete mixtures. That is, there are fixed number of &#8220;components&#8221; each with a mixing proportion and each component is itself a parameterized distribution, like a Gaussian.</p>


<p>The most common mixture is the Gaussian mixture model (GMM). A guassian mixture model with K components has a K dimensional discrite distrubtion for the mixing variable and has K individual Gaussian components. Today&#8217;s post will focus on using working with Gaussian mixture models.</p>


<h2>prtRvMixture<a name="2"></a></h2>


<p>prtRvMixture has two main properties that are of interest &#8220;components&#8221; and &#8220;mixingProportions&#8221;.</p>


<p>components should be an array of prtRvs that also inherit from prtRvMembershipModel. Without getting too indepth a prtRvMembershipModel is a special attribute of some prtRvs that specifies that this RV knows how to work inside of a mixture model. As we mentioned before, we are focusing on mixture of prtRvMvn objects to make a Gaussian mixture model. Luckily prtRvMvn inherits from prtRvMembershipModel and therefore it knows how to work in a mixture.</p>


<p>mixingProportions is the discrite mixing density for the mixture model. It should be a vector that sums to one with the same length as the &#8220;components&#8221; array.</p>


<p>To get started let&#8217;s make an array of 2D MVN RV objects with different means and a non-diagonal covariance matrix.</p>


<pre class="codeinput">gaussianSet1 = repmat(prtRvMvn(<span class="string">'sigma'</span>,[1 -0.5; -0.5 2;]),2,1);
gaussianSet1(1).mu = [-2 -2];
gaussianSet1(2).mu = [2 2];
</pre>


<p>Then we will can make a mixture by specifying some mixingProportions</p>


<pre class="codeinput">mixture1 = prtRvMixture(<span class="string">'components'</span>,gaussianSet1,<span class="string">'mixingProportions'</span>,[0.5 0.5]);
</pre>


<p>Because prtRvMixtures are prtRvs we get all of the nice plotting that comes along with things. Let&#8217;s take  a look at the density of our prtRv</p>


<pre class="codeinput">plotPdf(mixture1);
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/morton_blog_20130227_01.png" alt=""> <p>To show how we can do classification with these mixtures, let&rsquo;s make another mixture with different parameters. Then we will draw some data from both mixtures and plot our classification dataset.</p><pre class="codeinput">gaussianSet2 = repmat(prtRvMvn(<span class="string">&lsquo;sigma&rsquo;</span>,[1 0.5; 0.5 3;]),2,1);
gaussianSet2(1).mu = [2 -2];
gaussianSet2(2).mu = [-2 2];</p>

<p>mixture2 = prtRvMixture(<span class="string">&lsquo;components&rsquo;</span>,gaussianSet2,<span class="string">&lsquo;mixingProportions&rsquo;</span>,[0.5 0.5]);</p>

<p>ds = prtDataSetClass( cat(1,mixture1.draw(500),mixture2.draw(500)), prtUtilY(500,500)); <span class="comment">% Draw 500 samples from each mixture</span>
plot(ds)
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/morton_blog_20130227_02.png" alt=""> <h2>Using prtRvMixtures for Classification<a name="6"></a></h2><p>Like we showed in part 1 of this series we can set the &ldquo;rvs&rdquo; property of prtClassMap to any prtRv object and use that rv for classification. Let&rsquo;s for prtClassMap to use a mixture of prtRvMvn objects.</p><pre class="codeinput">emptyMixture = prtRvMixture(<span class="string">&lsquo;components&rsquo;</span>,repmat(prtRvMvn,2,1)); <span class="comment">% 2 component mixture</span></p>

<p>classifier = prtClassMap(<span class="string">&lsquo;rvs&rsquo;</span>,emptyMixture);</p>

<p>trainedClassifier = train(classifier, ds);</p>

<p>plot(trainedClassifier);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/morton_blog_20130227_03.png" alt=""> <p>As you can see it looks like this classifier would perform quite well. We can see that the learned means of the class 0 data (blue) closely match the means that we specified above for guassianSet1. So things appear to be working well.</p><pre class="codeinput">cat(1,trainedClassifier.rvs(1).components.mu)
</pre><pre class="codeoutput">ans =
   -1.9712   -2.0488</p>

<pre><code>2.0139    1.9548
</code></pre>

<p></pre><h2>prtRvGmm<a name="8"></a></h2><p>Since Guassian mixture models are the most common type of mixture a number of techniques have been established that help them perform better when working with limited and/or high dimensional data. To help facilitate some of those tweak there is prtRvGmm. It works much the same way as prtRvMixture only the components must be prtRvMvns.</p><pre class="codeinput">mixture1Gmm = prtRvGmm(<span class="string">&lsquo;components&rsquo;</span>,gaussianSet1,<span class="string">&lsquo;mixingProportions&rsquo;</span>,[0.5 0.5]);</p>

<p>plotPdf(mixture1Gmm);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/morton_blog_20130227_04.png" alt=""> <p>One of the available tweaks is that the covarianceStructure of all components is controled by a single parameter, &ldquo;covarianceStructure&#8217;. See the documentation prtRvMvn to know how this works. Let&rsquo;s see how changing the covarianceStructure of all of our components changes the appears of our density.</p><pre class="codeinput">mixture1GmmMod = mixture1Gmm;
mixture1GmmMod.covarianceStructure = <span class="string">&lsquo;spherical&rsquo;</span>; <span class="comment">% Force independence with a shared variance.</span></p>

<p>plotPdf(mixture1GmmMod);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/morton_blog_20130227_05.png" alt=""> <h2>Using prtRvGmm for classification<a name="10"></a></h2><p>Using prtRvGmm for classification is a little easier than prtRvMixture because we only need to specify the number of components. We don&rsquo;t have to built the array of components ourselves.</p><p>Let&rsquo;s redo the same problem as before.</p><pre class="codeinput">classifier = prtClassMap(<span class="string">&lsquo;rvs&rsquo;</span>,prtRvGmm(<span class="string">&lsquo;nComponents&rsquo;</span>,2));
trainedClassifier = train(classifier,ds);
plot(trainedClassifier);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/morton_blog_20130227_06.png" alt=""> <p>As you can see, things look nearly identical (as they should). Now, let&rsquo;s make use of a few of the extra tweak offered by prtRvGmm and see how they change our decision contours.</p><pre class="codeinput">classifier = prtClassMap(<span class="string">&lsquo;rvs&rsquo;</span>,prtRvGmm(<span class="string">&lsquo;nComponents&rsquo;</span>,2,<span class="string">&lsquo;covarianceStructure&rsquo;</span>,<span class="string">&lsquo;spherical&rsquo;</span>,<span class="string">&lsquo;covariancePool&rsquo;</span>,true));
trainedClassifier = train(classifier,ds);
plot(trainedClassifier);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/morton_blog_20130227_07.png" alt=""> <p>You can see that the decision contours are more regular now. This may help classificaiton performance in the precense of limited and/or high dimensional data.</p><h2>Conclusions<a name="13"></a></h2><p>We hope this post showed how you can use mixture models just like any other prtRv object when doing classificaiton using prtClassMap.</p><p>Chances are that you want to use prtRvGmm for your mixture modeling needs but you might be able to guess that prtRvMixture is much more general allowing you to make mixture models out of general prtRvs. However, at this time the only prtRv that is able to be used is prtRvMvn. We are interested in making more prtRvs compatible but we want to know what people want to use. Let us know if you need something specific.</p><p>In the next part of this series we will look at prtRvHmm and how it can be used for time-series classification.</p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Feature Selection - prtFeatSelSfs]]></title>
    <link href="http://newfolder.github.io/blog/2013/03/19/feature-selection/"/>
    <updated>2013-03-19T21:58:00-04:00</updated>
    <id>http://newfolder.github.io/blog/2013/03/19/feature-selection</id>
    <content type="html"><![CDATA[<p>Today I&#8217;d like to take a look at using a particular approach to feature selection in the PRT, and how that can be used to perform dimension reduction.  The approach we&#8217;ll use is called &#8220;sequential forward search&#8221;, and is implemented in prtFeatSelSfs.</p>


<!--/introduction-->


<h2>Contents</h2>


<div><ul><li><a href="#1">Some Data</a></li><li><a href="#3">Feature Selection</a></li><li><a href="#4">Defining &#8220;Informative&#8221;</a></li><li><a href="#7">Conclusions</a></li></ul></div>


<h2>Some Data<a name="1"></a></h2>


<p>The PRT comes with a utility function to generate data that has about 10 features, and for which only 5 of the features are actually informative. The function is prtDataGenFeatureSelection, I&#8217;ll let the help entry explain how it works:</p>


<pre class="codeinput">help <span class="string">prtDataGenFeatureSelection</span>
</pre>


<pre class="codeoutput"> prtDataGenFeatureSelection   Generates some unimodal example data for the prt.
   DataSet = prtDataGenFeatureSelection
   The data is distributed:
        H0: N([0 0 0 0 0 0 0 0 0 0],eye(10))
        H1: N([0 2 0 1 0 2 0 1 0 2],eye(10))
 
  Syntax: [X, Y] = prtDataGenFeatureSelection(N)
 
  Inputs: 
        N ~ number of samples per class (200)
 
  Outputs:
    X - 2Nx2 Unimodal data
    Y - 2Nx1 Class labels
 
  Example:
    DataSet = prtDataGenFeatureSelection;
    explore(DataSet)
 
  Other m-files required: none
  Subfunctions: none
  MAT-files required: none
 
  See also: prtDataGenUnimodal

</pre>


<p>As you can see from the help, only dimensions 2, 4, 6, 8, and 10 are actually informative in this data set.  And we can use feature selection to help us pick out what features are actually useful.</p>


<h2>Feature Selection<a name="3"></a></h2>


<p>Feature selection objects are prefaced in the prt with prtFeatSel*, and they act just like any other prtAction objects.  During training a feature selection action will typically perform some iterative search over the feature space, and determine which features are most informative.  At run-time, the same object will return a prtDataSet containing onlt the features the algorithm considered &#8220;informative&#8221;.</p>


<p>For example:</p>


<pre class="codeinput">ds = prtDataGenFeatureSelection;
featSel = prtFeatSelSfs;
featSel = featSel.train(ds);
dsFeatSel = featSel.run(ds);

plot(dsFeatSel);
title(<span class="string">'Three Most Informative Features'</span>);
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_ExampleFeatureSelection_01.png" alt=""> <h2>Defining &ldquo;Informative&rdquo;<a name="4"></a></h2><p>How does the feature selection algorithm determine what features are informative?  For many (but not necessarily all) feature selection objects, the interesting field is &ldquo;evaluationMetric&rdquo;.</p><p>Let&rsquo;s take a look:</p><pre class="codeinput">featSel.evaluationMetric
</pre><pre class="codeoutput">
ans =</p>

<pre><code>@(DS)prtEvalAuc(prtClassFld,DS)
</code></pre>

<p></pre><p>Obviously, evaluationMetric is a function handle &ndash; in particular it represents a call to a prtEval<em> method.  prtEval</em> methods typically take 2 or 3 input arguments &ndash; a classifier to train and run, a data set to train and run on, and (optionally) a number of folds (or fold specification) to use for cross-validation.</p><p>Feature selection objects iteratively search through the features available &ndash; in this case, all 10 of them, and apply the prtEval* method to the sub-sets of data formed by retaining a sub-set of the available features.  The exact order in which the features are retained and removed depends on the feature selection approach &ndash; in SFS, the algorithm first iteratively searches through the features &ndash; 1,2,3&hellip;,10.  Then it remembers which single feature provided the best performance &ndash; say it was feature 2.  Next, the SFS algorithm iteratively searches through all 9 combinations of other features with feature 2:    { {1,2},{3,2},{4,2},&hellip;,{10,2}} And remembers which of <b>those</b> performed best.  This process is iterated, and features continually added to the set being evaluated until nFeatures are selected.</p><p>The resulting performance is then stored in &ldquo;performance&rdquo;, and the features selected are stored in &ldquo;selectedFeatures&rdquo;.  Let&rsquo;s force the SFS approach to look for 10 features (so it will eventually select all of them).</p><pre class="codeinput">ds = prtDataGenFeatureSelection;
featSel = prtFeatSelSfs;
featSel.nFeatures = ds.nFeatures;
featSel.evaluationMetric = @(DS)prtEvalAuc(prtClassFld,DS,3);
featSel = featSel.train(ds);</p>

<p>h = plot(1:ds.nFeatures,featSel.performance);
set(h,<span class="string">&lsquo;linewidth&rsquo;</span>,3);
set(gca,<span class="string">&lsquo;xtick&rsquo;</span>,1:ds.nFeatures);
set(gca,<span class="string">&lsquo;xticklabel&rsquo;</span>,featSel.selectedFeatures);
xlabel(<span class="string">&lsquo;Features Selected&rsquo;</span>);
title(<span class="string">&lsquo;AUC vs. Features Selected&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_ExampleFeatureSelection_02.png" alt=""> <p>The features that get selected tend to favor features 2,6, and 10, then features 4 and 8, which makes sense as these are the 3 most informative followed by the two moderately-informative features!</p><h2>Conclusions<a name="7"></a></h2><p>There are a number of prtFeatSel<em> actions available, but not as many as we&rsquo;d like.  We&rsquo;re constantly on the look-out for new ones, and we&rsquo;d like to one day include &ldquo;K-forward, L-Backward&rdquo; searches, but just haven&rsquo;t had the time recently.</p><p>Also, this example only used prtEvalAuc as the performance metric, but there are a number of prtEval</em> functions you can use, or, of course &ndash; feel free to write your own!</p><p>Take a look at prtEvalAuc to see how they work and how to create your own!</p><p>Enjoy!</p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Max-Pooling Feature Representations in MSRCORID]]></title>
    <link href="http://newfolder.github.io/blog/2013/03/10/max-pooling/"/>
    <updated>2013-03-10T18:13:00-04:00</updated>
    <id>http://newfolder.github.io/blog/2013/03/10/max-pooling</id>
    <content type="html"><![CDATA[<p>A few weeks ago we took a look at a paper by Coates and Ng that dealt with learning feature representations for image processing and classification.  (See: <a href="http://newfolder.github.io/blog/2013/02/27/learning-feature-representations/">this</a>). Today I want to take a second look at that paper, and especially what they mean by max-pooling over regions of the image.</p>


<!--/introduction-->




<h2>Contents</h2>


<div><ul><li><a href="#1">The MSRCORID Database</a></li><li><a href="#2">Max-Pooling</a></li><li><a href="#5">Multiple Classes</a></li><li><a href="#7">Conclusions</a></li><li><a href="#8">Bibliography</a></li></ul></div>


<h2>The MSRCORID Database<a name="1"></a></h2>


<p>If you have already read through <a href="http://newfolder.github.io/blog/2013/02/27/learning-feature-representations/">our previous post</a>, you know how to get the Microsoft Research Cambridge Object Recognition Image Database (MSRCORID), which is really a fantastic resource for image processing and classification.</p>


<p>Once you&#8217;ve downloaded, we can run the following code which was for the most-prt ripped right out of the previous blog post:</p>


<pre class="codeinput">ds = prtDataGenMsrcorid;

patchSize = [8 8];
col = [];
<span class="keyword">for</span> imgInd = 1:ds.nObservations;
    img = ds.X{imgInd};
    img = rgb2gray(img);
    img = imresize(img,.5);
    col = cat(1,col,im2col(img,patchSize,<span class="string">'distinct'</span>)');
<span class="keyword">end</span>
dsCol = prtDataSetClass(double(col));

preProc = prtPreProcZeroMeanRows + prtPreProcStdNormalizeRows(<span class="string">'varianceOffset'</span>,10) + prtPreProcZca;
preProc = preProc.train(dsCol);
dsNorm = preProc.run(dsCol);

skm = prtClusterSphericalKmeans(<span class="string">'nClusters'</span>,50);
skm = skm.train(dsNorm);
</pre>


<h2>Max-Pooling<a name="2"></a></h2>


<p>Last time, we used a simple bag-of-words model to do classification based on the feature vectors in each image.  That&#8217;s definitely an interesting way to proceed, but most image-processing techniques make use of something called &#8220;max-pooling&#8221; to aggregate feature vectors over small regions of an image.</p>


<p>The process can be accomplished in MATLAB using blockproc.m, which is in the Image-processing toolbox.  (If you don&#8217;t have image processing, it&#8217;s not too hard to write a replacement for blockproc.)</p>


<p>The goal of max-pooling is to aggregate feature vectors over local regions of an image.  For example, we can take the MAX of the cluster memberships over each 8x8 region in an image using something like:</p>


<p>featsBp = blockproc(feats,[8 8],@(x)max(max(x.data,[],1),[],2));</p>


<p>Where we&#8217;ve assumed that feats is size nx x ny x nFeats.</p>


<p>Max pooling is nice because it reduces the dependency of the feature vectors on their exact placement in an image (each element of each 8x8 block gets treated about the same), and it also maintains a lot of the information that was in each of the feature vectors, especially when the feature vectors are expected to be sparse (e.g., have a lot of zeros; see http//www.ece.duke.edu/~lcarin/Bo12.3.2010.ppt).</p>


<p>There&#8217;s a lot more to max-pooling than we have time to get into here, for example, you can max-pool, and then re-cluster, and then re-max-pool! This is actually a super clever technique to reduce the amount of spatial variation in your image, and also capture information about the relative placements of various objects.</p>


<pre class="codeinput">featVec = nan(ds.nObservations,skm.nClusters*20);
clusters = skm.run(dsNorm);

<span class="keyword">for</span> imgInd = 1:ds.nObservations;
    img = ds.X{imgInd};
    img = rgb2gray(img);
    imgSize = size(img);

    <span class="comment">% Extract the sub-patches</span>
    col = im2col(img,patchSize,<span class="string">'distinct'</span>);
    col = double(col);
    dsCol = prtDataSetClass(col');
    dsCol = run(preProc,dsCol);
    dsFeat = skm.run(dsCol);
    dsFeat.X = max(dsFeat.X,.05);

    <span class="comment">% Max Pool!</span>
    <span class="comment">%   Feats will be size 30 x 40 x nClusters</span>
    <span class="comment">%   featsBp will be size [4 x 5] x nClusters (because of the way</span>
    <span class="comment">%   blockproc handles edsges)</span>
    feats = reshape(dsFeat.X,imgSize(1)/8,imgSize(2)/8,[]);
    featsBp = blockproc(feats,[8 8],@(x)max(max(x.data,[],1),[],2));

    <span class="comment">% We'll cheat a little here, and use the whole max-pooled feature set</span>
    <span class="comment">% as our feature vector.  Instead, we might want to re-cluster, and</span>
    <span class="comment">% re-max-pool, and repeat this process a few times.  For now, we'll</span>
    <span class="comment">% keep it simple:</span>
    featVec(imgInd,:) = featsBp(:);
<span class="keyword">end</span>
</pre>


<p>Now that we&#8217;ve max-pooled, we can use our extracted features for classification - we&#8217;ll use a simple PLSDA + MAP classifier and decision algorithm here:</p>


<pre class="codeinput">dsFeat = prtDataSetClass(featVec,ds.targets);
dsFeat.classNames = ds.classNames;

yOut = kfolds(prtClassPlsda + prtDecisionMap,dsFeat,3);

close <span class="string">all</span>;
prtScoreConfusionMatrix(yOut)
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_ExampleCoatesNg_Kmeans_Mscorid_2_01.png" alt=""> <p>Almost 99% correct!  We&rsquo;ve improved performance over our previous work with bag-of-words models, and an SVM, by just (1) max-pooling, and (2) replacing the SVM with a PLSDA classifier.</p><h2>Multiple Classes<a name="5"></a></h2><p>Until now we&rsquo;ve focused on just two classes in MSRCORID.  But there are a lot of types of objects in the MSRCORID database.  In the following, we just repeat a bunch of the code from above, and run it on a data set containing images of benches, buildings, cars, chimneys, clouds and doors:</p><pre class="codeinput">ds = prtDataGenMsrcorid({<span class="string">&lsquo;benches_and_chairs&rsquo;</span>,<span class="string">&lsquo;buildings&rsquo;</span>,<span class="string">&lsquo;cars\front view&rsquo;</span>,<span class="string">&lsquo;cars\rear view&rsquo;</span>,<span class="string">&lsquo;cars\side view&rsquo;</span>,<span class="string">&lsquo;chimneys&rsquo;</span>,<span class="string">&lsquo;clouds&rsquo;</span>,<span class="string">&lsquo;doors&rsquo;</span>});</p>

<p>patchSize = [8 8];
col = [];
<span class="keyword">for</span> imgInd = 1:ds.nObservations;</p>

<pre><code>img = ds.X{imgInd};
img = rgb2gray(img);
img = imresize(img,.5);
col = cat(1,col,im2col(img,patchSize,&lt;span class="string"&gt;'distinct'&lt;/span&gt;)');
</code></pre>

<p><span class="keyword">end</span>
dsCol = prtDataSetClass(double(col));</p>

<p>preProc = prtPreProcZeroMeanRows + prtPreProcStdNormalizeRows(<span class="string">&lsquo;varianceOffset&rsquo;</span>,10) + prtPreProcZca;
preProc = preProc.train(dsCol);
dsNorm = preProc.run(dsCol);</p>

<p>skm = prtClusterSphericalKmeans(<span class="string">&lsquo;nClusters&rsquo;</span>,50);
skm = skm.train(dsNorm);</p>

<p>featVec = nan(ds.nObservations,skm.nClusters*20);
clusters = skm.run(dsNorm);</p>

<p><span class="keyword">for</span> imgInd = 1:ds.nObservations;</p>

<pre><code>img = ds.X{imgInd};
img = rgb2gray(img);
imgSize = size(img);

&lt;span class="comment"&gt;% Extract the sub-patches&lt;/span&gt;
col = im2col(img,patchSize,&lt;span class="string"&gt;'distinct'&lt;/span&gt;);
col = double(col);
dsCol = prtDataSetClass(col');
dsCol = run(preProc,dsCol);
dsFeat = skm.run(dsCol);
dsFeat.X = max(dsFeat.X,.05);

&lt;span class="comment"&gt;% Max Pool!&lt;/span&gt;
&lt;span class="comment"&gt;%   Feats will be size 30 x 40 x nClusters&lt;/span&gt;
&lt;span class="comment"&gt;%   featsBp will be size [4 x 5] x nClusters (because of the way&lt;/span&gt;
&lt;span class="comment"&gt;%   blockproc handles edsges)&lt;/span&gt;
feats = reshape(dsFeat.X,imgSize(1)/8,imgSize(2)/8,[]);
featsBp = blockproc(feats,[8 8],@(x)max(max(x.data,[],1),[],2));

&lt;span class="comment"&gt;% We'll cheat a little here, and use the whole max-pooled feature set&lt;/span&gt;
&lt;span class="comment"&gt;% as our feature vector.  Instead, we might want to re-cluster, and&lt;/span&gt;
&lt;span class="comment"&gt;% re-max-pool, and repeat this process a few times.  For now, we'll&lt;/span&gt;
&lt;span class="comment"&gt;% keep it simple:&lt;/span&gt;
featVec(imgInd,:) = featsBp(:);
</code></pre>

<p><span class="keyword">end</span></p>

<p>dsFeat = prtDataSetClass(featVec,ds.targets);
dsFeat.classNames = ds.classNames;</p>

<p>yOut = kfolds(prtClassPlsda(<span class="string">&lsquo;nComponents&rsquo;</span>,10) + prtDecisionMap,dsFeat,3);
yOut.classNames = cellfun(@(s)s(1:min([length(s),10])),yOut.classNames,<span class="string">&lsquo;uniformoutput&rsquo;</span>,false);
close <span class="string">all</span>;
prtScoreConfusionMatrix(yOut);
set(gcf,<span class="string">&lsquo;position&rsquo;</span>,[426   125   777   558]);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_ExampleCoatesNg_Kmeans_Mscorid_2_02.png" alt=""> <p>Now we&rsquo;re doing some image processing!  Overall we got about 90% correct, and that includes a lot of confusions between cars\front and cars\rear. That makes sense since the front and backs of cars look pretty similar, and there are only 23 car front examples in the whole data set.</p><h2>Conclusions<a name="7"></a></h2><p>The code in a lot of this blog entry is pretty gross &ndash; for example we have to constantly be taking data out of, and putting it back into the appropriate image sizes.</p><p>At some point in the future, we&rsquo;d like to introduce a good prtDataSet that will handle cell-arrays containing images properly.  We&rsquo;re not there yet, but when we are, we&rsquo;ll let you know on this blog!</p><p>Happy coding!</p><h2>Bibliography<a name="8"></a></h2><p>Adam Coates and Andrew Y. Ng, Learning Feature Representations with K-means, G. Montavon, G. B. Orr, K.-R. Muller (Eds.), Neural Networks: Tricks of the Trade, 2nd edn, Springer LNCS 7700, 2012</p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using prtPath]]></title>
    <link href="http://newfolder.github.io/blog/2013/03/06/using-prtpath/"/>
    <updated>2013-03-06T13:24:00-05:00</updated>
    <id>http://newfolder.github.io/blog/2013/03/06/using-prtpath</id>
    <content type="html"><![CDATA[<p>We recommend as part of installation that you edit (or create) your startup.m file to include a call to prtPath. This is adds the PRT to your MATLAB search path automatically each time you start MATLAB. There are alternatives to path management (such as the pathtool() or addpath() and savepath()) but we recommend using a startup.m file. Hopefully this post will explain one reason we recommend that.</p>




<p>One of the reasons that we like to use prtPath in a startup file is that prtPath does more than just add the PRT and all of it&#8217;s subfolders to your path.</p>


<!--/introduction-->


<h2>Contents</h2>


<div><ul><li><a href="#1">prtPath and ] directories</a></li><li><a href="#3">Why &#8220;]&#8221;?</a></li><li><a href="#4">Conclusions</a></li></ul></div>


<h2>prtPath and ] directories<a name="1"></a></h2>


<p>prtPath is actually very basic. It calls the function prtRoot and uses the MATLAB function genpath() to get a list of all of the subdirectories in the prtFolder. It then eventually calls addpath() to add that list of directories to the MATLAB path (it does not save the path for future sessions of MATLAB).</p>


<p>Before prtPath() calls addpath() though, it selectively removes some directories from the subdirectory list.</p>


<p>First it removes any directory that starts with a &#8220;.&#8221;. This was added to prevent any hidden folders (like those from source control systems) from showing up in the MATLAB path.</p>


<p>More importantly though, it removes any folders from the list that start with a &#8220;]&#8221;. This is something special that we put in to add some extra functionality to the PRT.</p>


<p>Most of our users want to stick to code that is well tested and is known to behave nicely. But as we go about our jobs and use the PRT we need to add some new functionality. We typically add things like: new classifiers, new datatypes or new pre processing techniques.</p>


<p>Some of our users want access to this newest code so it gets added to the PRT in the &#8220;]alpha&#8221; and eventually the &#8220;]beta&#8221; folder. By default prtPath will not include these folders in the path. Instead you have to tell prtPath that you are willing to accept the responsibilities of the bleeding edge. You do this by giving prtPath a list of &#8220;]&#8221; folders that you want to include. (Or rather not exclude).</p>


<p>For example:</p>


<pre class="codeinput">prtPath(<span class="string">'alpha'</span>, <span class="string">'beta'</span>);
</pre>


<p>will add both the &#8220;]alpha&#8221; and &#8220;]beta&#8221; folders (and their subfolders) to the path.</p>


<p>Currently in the PRT we have one other &#8220;]&#8221; folder, &#8220;]internal&#8221;. In the internal folder you will find some code on unit testing and documentation building. You probably wont be interested in much that&#8217;s in there so I probably wouldn&#8217;t clutter my path with it.</p>


<h2>Why &#8220;]&#8221;?<a name="3"></a></h2>


<p>We were searching for a character that is a valid path (folder) name character on all major operating systems and is at the same time a character that most people wouldn&#8217;t start a directory name with. MATLAB already uses &#8220;@&#8221; for (old style) class definitions and &#8220;+&#8221; for packages. We thought &#8220;]&#8221; fit all of these criteria.</p>


<h2>Conclusions<a name="4"></a></h2>


<p>We hope that cleared up a little of why we recommend prtPath over pathtool(), at least for the PRT. In general just call prtPath() by itself but if you want to see what might lie ahead for the PRT checkout the ]alpha and ]beta folders. In some future posts we will talk about some things in these folders that might be of interest to you. Maybe that will entice you to explore the ].</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Learning Feature Representations]]></title>
    <link href="http://newfolder.github.io/blog/2013/02/27/learning-feature-representations/"/>
    <updated>2013-02-27T14:59:00-05:00</updated>
    <id>http://newfolder.github.io/blog/2013/02/27/learning-feature-representations</id>
    <content type="html"><![CDATA[<p>Today I wanted to go through an interesting paper I recently read and show how to implement parts of that paper in the PRT.  The paper is <i>Learning Feature Representations with K-means</i> , by Adam Coates and Andrew Y. Ng (see below for full citation).</p>


<p>The meat of the Coates and Ng paper deals with how to use K-means to extract meaningful dictionaries from image data.  The latter part of the paper talks about how to do real machine learning with max-pooling for classification, but for today, I just wanted to introduce the MSRCORID (Microsoft Research Cambridge Object Recognition Image Database) data in the PRT and also show how to use the PRT to do some K-means dictionary learning.</p>




<!--/introduction-->


<h2>Contents</h2>


<div><ul><li><a href="#1">The MSRCORID Database</a></li><li><a href="#3">Extracting Patches</a></li><li><a href="#4">Normalization</a></li><li><a href="#5">K-Means</a></li><li><a href="#7">Simple Bag-Of-Words Classification</a></li><li><a href="#10">Bibliography</a></li></ul></div>


<h2>The MSRCORID Database<a name="1"></a></h2>


<p>For fun, I downloaded a new image database to play with for this data. The data is available for download from here: <a href="http://research.microsoft.com/en-us/downloads/b94de342-60dc-45d0-830b-9f6eff91b301/default.aspx">http://research.microsoft.com/en-us/downloads/b94de342-60dc-45d0-830b-9f6eff91b301/default.aspx</a></p>


<p>You can load the data automatically in the PRT, if you update to the newest version and run:</p>


<pre class="codeinput">ds = prtDataGenMsrcorid;
</pre>


<p>By default, that command will give a dataset with only images of chimneys and single flowers.  Look at the help for prtDataGenMsrcorid to see how to load data from these, and a lot more interesting classes.</p>


<p>You should note that prtDataGenMsrcorid does not output a prtDataSetClass, it produces a prtDataSetCellArray.  prtDataSetCellArray data objects are relatively new, and not fully documented, but they&#8217;re useful when you want to deal with datasets where each observation can have different sizes - e.g., images.</p>


<p>You can access elements using cell-array notation to access the .X field of the prtDataSet, for example:</p>


<pre class="codeinput">subplot(2,1,1);
imshow(ds.X{1});
title(<span class="string">'Flower'</span>);

subplot(2,1,2);
imshow(ds.X{end});
title(<span class="string">'Chimney'</span>);
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_ExampleCoatesNg_Kmeans_Mscorid_01.png" alt=""> <h2>Extracting Patches<a name="3"></a></h2><p>To generate a dictionary requires segmenting the initial images provided to us into sub-regions.  We can acheive this by using the MATLAB function im2col which with will convert every 8x8 sub-image to a 64x1 element vector.</p><pre class="codeinput">patchSize = [8 8];
col = [];
<span class="keyword">for</span> imgInd = 1:ds.nObservations;</p>

<pre><code>img = ds.X{imgInd};
img = rgb2gray(img);
img = imresize(img,.5);
col = cat(1,col,im2col(img,patchSize,&lt;span class="string"&gt;'distinct'&lt;/span&gt;)');
</code></pre>

<p><span class="keyword">end</span>
dsCol = prtDataSetClass(double(col));
</pre><h2>Normalization<a name="4"></a></h2><p>[Coates, 2012], makes it very clear that proper data normalization on a per-patch basis is fundamental to getting meaningful K-means centroids. The three main steps in the normalization are mean-normalization, energy normalization, and ZCA centering.  These are all implemented in the PRT as prtPreProcZeroMeanRows, prtPreProcStdNormalizeRows, and prtPreProcZca.</p><p>As always, we can buld an algorithm out of these independent components, then train and run the algorithm on the dsCol data we created earlier:</p><pre class="codeinput">preProc = prtPreProcZeroMeanRows + prtPreProcStdNormalizeRows(<span class="string">&lsquo;varianceOffset&rsquo;</span>,10) + prtPreProcZca;
preProc = preProc.train(dsCol);
dsNorm = preProc.run(dsCol);
</pre><h2>K-Means<a name="5"></a></h2><p>[Coates, 2012], makes a compelling case that K-means clustering is capable of learning dictionaries that can be easily used for classification.  The K-means algorithm in Coates paper is particularly intruiging, and its very fast compared to standard K-means using euclidean distances.  We&rsquo;ve implemented the K-means algorithm as described in [Coates, 2012] as prtClusterSphericalKmeans, which is much faster than using the regular K-means.</p><pre class="codeinput">skm = prtClusterSphericalKmeans(<span class="string">&lsquo;nClusters&rsquo;</span>,50);
skm = skm.train(dsNorm);
</pre><p>We can visualize the resulting cluster centers from the K-means processing by looking a the skm.clusterCenters, and plotting the first 50.  We&rsquo;ll sort these by how often data vectors were assigned to each cluster, so the top-left has the most elements, and the bottom-right has the least.</p><pre class="codeinput"></p>

<p>yOutK = skm.run(dsNorm);
[val,ind] = max(yOutK.X,[],2);
boolMat = zeros(size(yOutK.X));
indices = sub2ind(size(boolMat),(1:size(boolMat,1))&lsquo;,ind(:));
boolMat(indices) = 1;</p>

<p>clusterCounts = sum(boolMat);
[v,sortInds] = sort(clusterCounts,<span class="string">&lsquo;descend&rsquo;</span>);</p>

<p>c = skm.clusterCenters';
<span class="keyword">for</span> i = 1:50</p>

<pre><code>subplot(5,10,i);
imagesc(reshape(c(sortInds(i),:),patchSize));
title(v(i));
tickOff;
</code></pre>

<p><span class="keyword">end</span>
colormap <span class="string">gray</span>
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_ExampleCoatesNg_Kmeans_Mscorid_02.png" alt=""> <h2>Simple Bag-Of-Words Classification<a name="7"></a></h2><p>We can use some of the approaches from [Coates, 2012] to do some simple classification, also.  For example, we can use our new K-means clustering algorithm to generate features for each observation.  We can do this for every patch we extract from each image, but we&rsquo;d like to make decisions on an image-by-image basis, so we need to aggregate over the resulting feature vectors somehow.</p><p>A clever way to do this is to use max-pooling and deep-learning as specified in [Coates, 2012], but for now we&rsquo;ll just take the mean of the resulting feature vectors (in a manner similar to bag-of-words classification <a href="http://en.wikipedia.org/wiki/Bag-of-words_model"><a href="http://en.wikipedia.org/wiki/Bag-of-words_model">http://en.wikipedia.org/wiki/Bag-of-words_model</a></a> )</p><pre class="codeinput">featVec = nan(ds.nObservations,skm.nClusters);</p>

<p><span class="keyword">for</span> imgInd = 1:ds.nObservations;</p>

<pre><code>img = ds.X{imgInd};
img = rgb2gray(img);
col = im2col(img,patchSize,&lt;span class="string"&gt;'distinct'&lt;/span&gt;);
col = double(col);
dsCol = prtDataSetClass(col');
dsCol = run(preProc,dsCol);
dsFeat = skm.run(dsCol);
feats = max(dsFeat.X,.05);
featVec(imgInd,:) = mean(feats);
</code></pre>

<p><span class="keyword">end</span>
</pre><p>Now we can classify our feature vectors using another classification algorithm &ndash; e.g., here we use a SVM, with ZMUV pre-processing, and max-a-posteriori classification.</p><pre class="codeinput">dsFeat = prtDataSetClass(featVec,ds.targets);
dsFeat.classNames = ds.classNames;</p>

<p>yOut = kfolds(prtPreProcZmuv + prtClassLibSvm + prtDecisionMap,dsFeat,3);</p>

<p>close <span class="string">all</span>;
prtScoreConfusionMatrix(yOut)
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_ExampleCoatesNg_Kmeans_Mscorid_03.png" alt=""> <p>Hey!  That&rsquo;s not too bad for a few lines of code.  At some point in the future we&rsquo;ll take on the rest of the [Coates, 2012] paper, but in the meantime, let us know if you implement the max-pooling or other processes outlined therein.</p><p>Happy coding!</p><p>Note: we created prtPreProcZca, prtClusterSphericalKmeans, and prtDataGenMsrcorid for this blog entry; they&rsquo;re all in the PRT, but are recent (as of 2/27/2013) so download a new version to get access to all these.</p><h2>Bibliography<a name="10"></a></h2><p>Adam Coates and Andrew Y. Ng, Learning Feature Representations with K-means, G. Montavon, G. B. Orr, K.-R. Muller (Eds.), Neural Networks: Tricks of the Trade, 2nd edn, Springer LNCS 7700, 2012</p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[IEEE GRSS Hyperspectral Data]]></title>
    <link href="http://newfolder.github.io/blog/2013/02/16/ieee-grss-hyperspectral-data/"/>
    <updated>2013-02-16T20:06:00-05:00</updated>
    <id>http://newfolder.github.io/blog/2013/02/16/ieee-grss-hyperspectral-data</id>
    <content type="html"><![CDATA[<p>
Hi everyone.  Outside of software development, we also do some work in geoscience and remote sensing.  As a result, we were very excited to see an announcement from the IEEE GRSS that they were making some new data sets available - in particular, a hyperspectral data set, and a LIDAR data set (if you&#8217;re not familiar with these technologies, see here: <a href="http://en.wikipedia.org/wiki/Hyperspectral_imaging">http://en.wikipedia.org/wiki/Hyperspectral_imaging</a> and <a href="http://en.wikipedia.org/wiki/LIDAR">http://en.wikipedia.org/wiki/LIDAR</a>).</p>


<!--/introduction-->


<h2>Contents</h2>


<div><ul><li><a href="#1">Getting Started</a></li><li><a href="#3">A Note About UnLabeled Points</a></li><li><a href="#5">Visualization of the Hyperspectral and LIDAR data</a></li><li><a href="#6">Hyperspectral Data</a></li><li><a href="#7">PC Projections</a></li><li><a href="#9">15-Class Classification</a></li><li><a href="#11">Evaluation</a></li><li><a href="#12">Conclusions</a></li></ul></div>


<h2>Getting Started<a name="1"></a></h2>


<p>We&#8217;ve made some M-files that will load in the data for you (see the .ZIP file at the end of this post), but you&#8217;ll need to go to the GRSS website to download the data from here first: <a href="http://hyperspectral.ee.uh.edu/?page_id=459">http://hyperspectral.ee.uh.edu/?page_id=459</a>).</p>


<p>To load the data, first download the .ZIP file at the end of this post, then run the following.  But change the line below to point to the right directory, so for example, I have a file C:\Users\pete\Documents\data\2013IEEE_GRSS_DF_Contest\2013_IEEE_GRSS_DF_Contest_CASI.tif</p>


<pre class="codeinput">imgSize = [349 1905];
grssDir = <span class="string">'C:\Users\pete\Documents\data\2013IEEE_GRSS_DF_Contest\'</span>;
[dsCasi,dsLidar] = prtExampleReadGrss2013(grssDir);
</pre>


<p>Note, if you don&#8217;t want to use the PRT for anything, but would like to automatically read in the ROI file provided by GRSS, download the .ZIP at the end of this file, and just use:</p>


<pre class="codeinput">
regions = grssRoiRead(roiFile);
</pre>


<p>Each of these data sets has 664845 observations (from the 349x1905 dimensional image) the hyperspectral data has 144 dimensions, and the LIDAR data only has one dimension. The fine folks at GRSS were kind enough to provide labels for about 2832 pixels from the data sets.  These are from 15 different classes: grass_healthy, grass_stressed, grass_synthetic, tree, soil, water, residential, commercial, road, highway, railway, parking_lot1, parking_lot2, tennis_court, running_track.</p>


<h2>A Note About UnLabeled Points<a name="3"></a></h2>


<p>This data set contains a lot of unlabeled data.  Previously, to use the PRT with unlabeled data required ad-hoc fiddling with targets and classes.  But as of Jan 7, 2013, the PRT now handles unlabeled data inherently.  The PRT uses NaNs to represent unlabeled data points - e.g.,</p>


<pre class="codeinput">numNan = length(find(isnan(dsCasi.targets))); <span class="comment">%there are 662013 unlabeled points</span>
disp(numNan)
</pre>


<pre class="codeoutput">      662013

</pre>


<p>You can get a data set using only the labeled data using new mthods included specifically for unlabeled data:</p>


<pre class="codeinput">dsUnLabeled = dsCasi.removeLabeled;
dsLabeled = dsCasi.retainLabeled;
</pre>


<h2>Visualization of the Hyperspectral and LIDAR data<a name="5"></a></h2>


<p>We can visualize the data in the form of the spatial image by re-sizing it to be the correct dimensionality.  For example, the next lines reshape the total intensity (SUM) of the hyperspectral data, and reshape the LIDAR data into the right size:</p>


<pre class="codeinput">x = dsCasi.X;
x = reshape(x',[144 imgSize]);
imgCasi = sqrt(squeeze(sum(x.^2)));

subplot(2,1,1);
imagesc(imgCasi)
title(<span class="string">'CASI Data'</span>);

xLidar = dsLidar.X;
imgLidar = reshape(xLidar,imgSize);
subplot(2,1,2);
imagesc(imgLidar)
title(<span class="string">'LIDAR Data'</span>);
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Grss_01.png" alt=""> <h2>Hyperspectral Data<a name="6"></a></h2><p>The remainder of this blog entry will just show a few examples of how to use the PRT in combination with the CASI hyperspectral data; I want to be clear that we&rsquo;re not doing anything that&rsquo;s particularly well-motivated from a hyperspectral data perspective.  If you&rsquo;re interested, there&rsquo;s a great deal of research in the hyperspectral field &ndash; we can&rsquo;t summarize all the interesting stuff that&rsquo;s going on there, but if you&rsquo;re interested, check out some recent issues of WHISPERS: <a href="http://www.ieee-whispers.com/"><a href="http://www.ieee-whispers.com/">http://www.ieee-whispers.com/</a></a></p><p>In reality, the purpose of the data set from GRSS is to do data fusion, but for today we&rsquo;ll just be concerned with the hyperspectral data &ndash; dsCasi.</p><h2>PC Projections<a name="7"></a></h2><p>As an example, we can explore the data in principal component space. It&rsquo;s easy enought to do &ndash; we can treat the CASI part of the data just like any other prtDataSet.  Let&rsquo;s build an algorithm to do some standard pre-processing.  Each 144 dimensional hyperspectral vector is a row of the data matrix, so we can zero-mean, and normalize the standard deviation of each row with prtPreProcZeroMeanRows and prtPreProcStdNormalizeRows (which is new).</p><pre class="codeinput">preProc = prtPreProcZeroMeanRows + prtPreProcStdNormalizeRows(<span class="string">&lsquo;varianceOffset&rsquo;</span>,10) + prtPreProcPca(<span class="string">&lsquo;nComponents&rsquo;</span>,3);</p>

<p>dsLabeled = dsCasi.retainLabeled;
preProc = preProc.train(dsLabeled);
dsPreProc = preProc.run(dsLabeled);
subplot(1,1,1);
plot(dsPreProc);
legend(<span class="string">&lsquo;location&rsquo;</span>,<span class="string">&lsquo;EastOutside&rsquo;</span>)
title(<span class="string">&lsquo;CASI Hyperspectral PC&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Grss_02.png" alt=""> <p>Visualizing the data in PC space, we can see a few things.  First, all of the grass and tree samples are quite similar in PC space.  Also, synthetic grass looks nothing like the real grass classes &ndash; from a hyperspectral perspective, synthetic grass is clearly a man-made material, despite the color similarity between it and real grass.</p><h2>15-Class Classification<a name="9"></a></h2><p>Let&rsquo;s take a look at doing some classification.  Recall that GRSS labeled 15 unique classes for us; we can do standard machine learning with that data.  We will use similar pre-processing as above, and then a 15 component PLSDA classifier.  PLSDA is nice in this case, since it natively handles multi-class problems, and its quite fast.  We can probably get better results with a non-linear classifier, but for now we&rsquo;ll stick with PLSDA.</p><pre class="codeinput">dsLabeled = dsCasi.retainLabeled;</p>

<p>algo = prtPreProcZeroMeanRows + prtPreProcStdNormalizeRows(<span class="string">&lsquo;varianceOffset&rsquo;</span>,10) + prtClassPlsda(<span class="string">&lsquo;nComponents&rsquo;</span>,15) + prtDecisionMap;
yOut = algo.kfolds(dsLabeled,3);</p>

<p>close <span class="string">all</span>;
prtScoreConfusionMatrix(yOut);
pc = prtScorePercentCorrect(yOut);
title(sprintf(<span class="string">&lsquo;15 Class Classification; %.2f%% Correct&rsquo;</span>,pc*100));
xlabel(<span class="string">&lsquo;&rsquo;</span>);
rotateticklabel(gca,45);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Grss_03.png" alt=""> <p>Over 70% correct, with just this simple processing!  That&rsquo;s not too shabby.  Note, however, that the cross-validation approach we&rsquo;ve used here is a little suspect &ndash; a lot of the truth proivded to us was from neighboring pixels.  These pixels might be only about a meter apart, and two hyperspectral vectors from that close proximity, on, say, grass, will be expected to be much more correlated than two pixels from hundreds of meters apart.</p><p>One way to overcome this would be to build cross-validation folds using the spatial locations of the pixels; it would be interesting to see how cross-validation would work under that case.</p><h2>Evaluation<a name="11"></a></h2><p>To evaluate our algorithm we can visualize the performance on the entire larger hyperspectral image.  First, we run the algorithm, then get the estimated labels, and reshape to the right size for visualization.</p><pre class="codeinput">algo = algo.train(dsLabeled);
yOutFull = algo.run(dsCasi);</p>

<p>close <span class="string">all</span>;
img = reshape(yOutFull.X,imgSize);
imagesc(img);
colorbar
<span class="comment">% It&rsquo;s a little hard to judge, but this doesn&rsquo;t look completely</span>
<span class="comment">% unreasonable&hellip;.</span>
<span class="comment">%</span>
<span class="comment">% The following shows only the points labeled as one of the two types of</span>
<span class="comment">% grasses or trees</span>
imagesc(img == 1 | img == 2 | img == 4);
title(<span class="string">&lsquo;Points Classified as Grass (Healthy, and Stressed) or Tree&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Grss_04.png" alt=""> <h2>Conclusions<a name="12"></a></h2><p>If you&rsquo;re interested in hyperspectral data, LIDAR, or data fusion, you should definitely check out the GRSS data set.  We hope the PRT files we&rsquo;re providing will help you get started.</p><p>There&rsquo;s a lot more to do with this data &ndash; we haven&rsquo;t even explored the LIDAR data yet, or how to fuse information from the two.  This data is also a prime candidate for semi-supervised learning (<a href="http://en.wikipedia.org/wiki/Semi-supervised_learning"><a href="http://en.wikipedia.org/wiki/Semi-supervised_learning">http://en.wikipedia.org/wiki/Semi-supervised_learning</a></a>), active learning (<a href="http://en.wikipedia.org/wiki/Active_learning"><a href="http://en.wikipedia.org/wiki/Active_learning">http://en.wikipedia.org/wiki/Active_learning</a></a>), or multi-task learning (<a href="http://en.wikipedia.org/wiki/Multi-task_learning"><a href="http://en.wikipedia.org/wiki/Multi-task_learning">http://en.wikipedia.org/wiki/Multi-task_learning</a></a>).</p><p>Hopefully we&rsquo;ll get a chance to delve more into this data set in the near future.  Let us know if you have any luck with this data!</p></p>

<p><a href="http://newfolder.github.io/images/prtGrssBlog_2013_02_16.zip">Here&rsquo;s a link</a> to the .ZIP file with all the code you&rsquo;ll need.</p>

<p><p>
You may also need rotateticklabel.m, which is available from <a href="http://www.mathworks.com/matlabcentral/fileexchange/8722-rotate-tick-label">MATLAB Central</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Combining Actions]]></title>
    <link href="http://newfolder.github.io/blog/2013/02/11/combining-actions/"/>
    <updated>2013-02-11T13:46:00-05:00</updated>
    <id>http://newfolder.github.io/blog/2013/02/11/combining-actions</id>
    <content type="html"><![CDATA[<p>Hi!  Today I&#8217;d like to talk about how you can use the PRT to combine actions together to form algorithms.  This is an important and powerful tool in the PRT, and understanding it can solve a lot of headaches for you.</p>


<!--/introduction-->


<h2>Contents</h2>


<div><ul><li><a href="#1">An Example</a></li><li><a href="#5">The Problem</a></li><li><a href="#9">Combining Actions into Algorithms</a></li><li><a href="#13">Using Algorithms</a></li></ul></div>


<h2>An Example<a name="1"></a></h2>


<p>Let&#8217;s start with a concrete example.  Say we want to classify some very high dimensional data.  We&#8217;ll start with the following:</p>


<pre class="codeinput">nFeatures = 200;
ds = prtDataGenUnimodal;
xNoise = randn(ds.nObservations,nFeatures);
ds.X = cat(2,ds.X,xNoise); <span class="comment">%add nFeatures meaningless features</span>
</pre>


<p>If we try and classify this with a GLRT, for example, we&#8217;re going to run into trouble, since there are more features than there are observations, so we can&#8217;t generate a full-rank covariance structure.  For example, using the prtAction prtClassGlrt, we might write this:</p>


<pre class="codeinput">glrt = prtClassGlrt;
glrt = glrt.train(ds);
<span class="keyword">try</span>
   yOut = glrt.run(ds);  <span class="comment">%This causes errors</span>
<span class="keyword">catch</span> ME
    disp(<span class="string">'Error encountered:'</span>)
    disp(ME);
<span class="keyword">end</span>
</pre>


<pre class="codeoutput">Warning: Covariance matrix is not positive definite. This may cause errors.
Consider modifying "covarianceStructure". 
Warning: Covariance matrix is not positive definite. This may cause errors.
Consider modifying "covarianceStructure". 
Error encountered:
  MException
  Properties:
    identifier: 'prtRvUtilMvnLogPdf:BadCovariance'
       message: 'SIGMA must be symmetric and positive definite.'
         cause: {0x1 cell}
         stack: [9x1 struct]

</pre>


<p>We can always use dimension-reduction techniques to reduce the number of features in our data set, and then evaluate performance.  For example:</p>


<pre class="codeinput">pca = prtPreProcPca(<span class="string">'nComponents'</span>,2);
pca = pca.train(ds);
dsPca = pca.run(ds);
plot(dsPca);
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_combiningActions_01.png" alt=""> <p>Now we can evaluate our GLRT on the dsPca:</p><pre class="codeinput">glrt = prtClassGlrt;
yOutKfolds = glrt.kfolds(dsPca,10);
[pf,pd] = prtScoreRoc(yOutKfolds);
h = plot(pf,pd);
set(h,<span class="string">&lsquo;linewidth&rsquo;</span>,3);
title(<span class="string">&lsquo;Example GLRT ROC Curve (Running on PCA Features)&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_combiningActions_02.png" alt=""> <h2>The Problem<a name="5"></a></h2><p>There&rsquo;s a problem in the above, though.  Even though we cross-validated the GLRT using 3 random folds, we didn&rsquo;t do the same thing with the PCA. This is technically not fair, since the PCA part of the algorithm was trained using all the data.</p><p>Maybe we can get around this like so:</p><pre class="codeinput">pca = prtPreProcPca(<span class="string">&lsquo;nComponents&rsquo;</span>,2);
dsPca = pca.kfolds(ds,10);
</pre><p>But now, when we do:</p><pre class="codeinput">glrt = prtClassGlrt;
yOutKfolds = glrt.kfolds(dsPca,10);
[pf,pd] = prtScoreRoc(yOutKfolds);
h = plot(pf,pd);
set(h,<span class="string">&lsquo;linewidth&rsquo;</span>,3);
title(<span class="string">&lsquo;This is no good&hellip;&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_combiningActions_03.png" alt=""> <p>We have a problem!  At every fold, we learn a unique set of PCA loadings. Since PCA loadings have arbitrary sign (+/&ndash;), the outputs across all these folds will overlap!</p><pre class="codeinput">plot(dsPca)
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_combiningActions_04.png" alt=""> <p>The underlying problem is that there&rsquo;s no guarantee that the folds used for PCA and GLRT evaluation were the same.  We can get around <b>that</b> if we specified the folds, and wrote our own cross-validate specifically for this new process we&rsquo;ve made, but suddenly this is getting complicated.</p><p>And what if we had an even more complicated process, including other pre-processing streams, feature selection, classifiers and decision-makers?  Suddenly our code is going to be a mess!</p><h2>Combining Actions into Algorithms<a name="9"></a></h2><p>At the heart of the problem outlined above is that the PCA and GLRT parts of our process weren&rsquo;t considered as two parts of the same process &ndash; they were two separate variables, and the PRT and MATLAB didn&rsquo;t know that they should work together.</p><p>Since this problem is so common, the PRT provides an easy way to combine each individual part of a process (prtActions) into one big process (a prtAlgorithm).  This is easily done using the &ldquo;+&rdquo; operator:</p><pre class="codeinput">pcaGlrt = prtPreProcPca(<span class="string">&lsquo;nComponents&rsquo;</span>,2) + prtClassGlrt;
</pre><p>If you&rsquo;re not used to object oriented programming, the above might look a little weird.  But it&rsquo;s straightforward &ndash; we&rsquo;ve defined &ldquo;plus&rdquo; (&ldquo;+&rdquo;) for prtActions (e.g., prtPreProcPca) to mean &ldquo;Combine these into one object, where that object will perform each action in sequence from left to right&rdquo;.  Technically this returns a special kind of prtAction, called a prtAlgorithm.  That&rsquo;s just the date type we use to store a bunch of actions.  You can see that here:</p><pre class="codeinput">disp(pcaGlrt)
</pre><pre class="codeoutput">  prtAlgorithm</p>

<p>  Properties:</p>

<pre><code>                name: 'PRT Algorithm'
    nameAbbreviation: 'ALGO'
        isSupervised: 1
isCrossValidateValid: 1
          actionCell: {2x1 cell}
  connectivityMatrix: [4x4 logical]
      verboseStorage: 1
     showProgressBar: 1
           isTrained: 0
      dataSetSummary: []
             dataSet: []
            userData: [1x1 struct]
</code></pre>

<p></pre><p>You can visualize the structure of the algorithm using PLOT:</p><pre class="codeinput">plot(pcaGlrt)
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_combiningActions_05.png" alt=""> <p>You can combine any number of prtActions into an algorithm like this, so, although its silly, this is technically a valid command:</p><pre class="codeinput">sillyAlgo = prtPreProcZmuv + prtPreProcHistEq + prtPreProcPca + prtClassGlrt;
plot(sillyAlgo)
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_combiningActions_06.png" alt=""> <h2>Using Algorithms<a name="13"></a></h2><p>So we&rsquo;ve made a prtAlgorithm.  Now what?  Well, anything you can do to a prtAction, you can do with a prtAlgorithm.  What does that mean?  Methods like plot, kfolds, and crossValidate all work exactly the same as they do with regular prtActions.  And they make your life much simpler than what we had to do above:</p><pre class="codeinput">pcaGlrt = prtPreProcPca(<span class="string">&lsquo;nComponents&rsquo;</span>,2) + prtClassGlrt;
yOutKfolds = pcaGlrt.kfolds(ds,10);
[pf,pd] = prtScoreRoc(yOutKfolds);
h = plot(pf,pd);
set(h,<span class="string">&lsquo;linewidth&rsquo;</span>,3);
title(<span class="string">&lsquo;ROC Curve for a prtAlgorithm (PCA + GLRT)&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_combiningActions_07.png" alt=""> <p>The results in the ROC curve above were generated using 10-folds cross-validation on the combination of PCA and GLRT.  At each fold, 9/10ths of the data were used to train the PCA and GLRT, and 1/10th was used for evaluation.</p><p>prtAlgorithms are a very powerful tool for pattern recognition, and we hope this blog post helps clear up how to make and use them!</p><p>Let us know if you have any questions or comments.</p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Random Variables (Part 1)]]></title>
    <link href="http://newfolder.github.io/blog/2013/01/31/random-variables-part-1/"/>
    <updated>2013-01-31T16:24:00-05:00</updated>
    <id>http://newfolder.github.io/blog/2013/01/31/random-variables-part-1</id>
    <content type="html"><![CDATA[<p>Hey ya&#8217;ll! Probability theory and random variables come up all the time in machine learning. Classification techniques like Naive Bayes, the likelihood ratio test and maximum a posterior (MAP). A lot of times when someone says &#8220;Naive Bayes classification&#8221; they imply that they want to assume that the data is multinomial (counts from a fixed dictionary) or when they say &#8220;MAP classification&#8221; they mean they assumed Gaussian distributions for each of the classes. In reality though the choice of these distributions is flexible and assuming different distributions in the PRT is easy thanks to the RV objects. This is the first post in a series that will highlight how RV objects can be used for rapid classifier generation and showcase some of the ways that we use RVs for our research. In part 1, we are going to give an overview of some of the basic RV objects and show how they are used in some basic classification techniques.</p>




<h2>Contents</h2>


<div><ul><li><a href="#1">RV objects</a></li><li><a href="#7">Types of RV objects</a></li><li><a href="#9">Using RV Objects in Classifiers</a></li><li><a href="#12">Conclusions</a></li></ul></div>


<h2>RV objects<a name="1"></a></h2>


<p>Admittedly, the RV objects are one of the most under-documented features in the PRT. Sorry about that. I can take the blame there. Hopefully this post gets us started on fixing that.</p>


<p>Random variable objects are used to state that data is a random variable with an assumed distribution. Therefore, each prtRv*() assumes a different probability density function and implements the necessary methods to infer the parameters of the probability density function mle(), draw data with the same distribution draw(), and evaluate the likelihood of other data pdf() and logPdf().</p>




<p>
Let&#8217;s make an RV object with a specified distribution. For the sake of this example we will use the multi-variate Normal distribution &#8220;MVN&#8221;.
</p>




<pre class="codeinput">
rv = prtRvMvn(<span class="string">'mu'</span>,[1 2],<span class="string">'sigma'</span>,[1 0.5; 0.5 1])
</pre>


<pre class="codeoutput">rv = 
  prtRvMvn

  Properties:
                    name: 'Multi-Variate Normal'
        nameAbbreviation: 'RVMVN'
            isSupervised: 0
    isCrossValidateValid: 1
     covarianceStructure: 'full'
                      mu: [1 2]
                   sigma: [2x2 double]
             plotOptions: [1x1 prtOptions.prtOptionsRvPlot]
          verboseStorage: 1
         showProgressBar: 1
               isTrained: 0
          dataSetSummary: []
                 dataSet: []
                userData: [1x1 struct]
</pre>


<p>Let&#8217;s draw some data from this RV object and put it into a prtDataSetClass().</p>


<pre class="codeinput">x = rv.draw(1000);
ds = prtDataSetClass(x);
</pre>


<p>Using this RV we can evaluate the log of the probability density function of the data that we drew.</p>


<pre class="codeinput">y = rv.logPdf(x);
</pre>


<p>RV objects also have some plot methods and plotLogPdf() is probably the most useful. Let&#8217;s plot the log of the probability density function with the data that we drew fromt the pdf.</p>


<pre class="codeinput">rv.plotLogPdf()
hold <span class="string">on</span>
plot(ds);
hold <span class="string">off</span>
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/morton_blog_20130127_01.png" alt=""> <p>Although RV objects can be used by specifying the parameters of the densities their true power is flexibly modeling data. For example, let&rsquo;s make another RV MVN object without specifying parameters and use it to estimate the parameters of the data we drew. Here we will estimate the parameters using maximum likelihood estimation mle()</p><pre class="codeinput">rv2 = prtRvMvn;
rv2 = rv2.mle(ds); <span class="comment">% or rv2 = rv2.mle(x);</span>
estimatedMean = rv2.mu
estimateCovariance = rv2.sigma
</pre><pre class="codeoutput">estimatedMean =
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.9647    1.9896
estimateCovariance =
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.9479    0.4587
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.4587    0.9858
</pre><p>RV objects are actually sub-classes of prtActions() just like classifiers and regressors. This means that they have the train() and run() methods and can be cross-validated. By default, all RV objects implement train by calling the mle() method and implement run by using the logPdf() method. Therefore, some of the things we did above can be done as follows.</p><pre class="codeinput">rv2 = rv2.train(ds);
y = rv2.run(ds);
</pre><h2>Types of RV objects<a name="7"></a></h2><p>A list of available RVs that ship with the PRT can be displayed</p><pre class="codeinput">dirContents = what(fullfile(prtRoot,<span class="string">&lsquo;rv&rsquo;</span>));
availableRvs = dirContents.m
</pre><pre class="codeoutput">availableRvs =
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRv.m&rsquo;
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRvDiscrete.m&rsquo;
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRvGmm.m&rsquo;
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRvHmm.m&rsquo;
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRvIndependent.m&rsquo;
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRvKde.m&rsquo;
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRvMemebershipModel.m&rsquo;
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRvMixture.m&rsquo;
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRvMultinomial.m&rsquo;
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRvMvn.m&rsquo;
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRvUniform.m&rsquo;
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRvUniformImproper.m&rsquo;
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRvVq.m&rsquo;
</pre><p>As you can, most of the standard probability densities have been implemented. In addition to standard things like prtRvMvn, prtRvDiscrete and prtRvMultinomial, there are also a few RVs that operate on other RVs like prtRvIndependent, prtRvMixture, prtRvGmm and prtRvHmm and there are a few RVs that can be used for more flexible density modeling like prtRvKde and prtRvVq. We will talk about some of these more advanced RVs in a later post.</p><h2>Using RV Objects in Classifiers<a name="9"></a></h2><p>There are two primary classifiers that make use of RV objects prtClassMap and prtClassGlrt. These classifiers have very similar performance but prtClassMap is able to handle M-ary classification problems so we will use that as our example.</p><pre class="codeinput">class = prtClassMap
</pre></p>

<pre class="codeoutput">class = 
  prtClassMap

  Properties:
                    name: 'Maximum a Posteriori'
        nameAbbreviation: 'MAP'
            isNativeMary: 1
                     rvs: [1x1 prtRvMvn]
        twoClassParadigm: 'binary'
         internalDecider: []
            isSupervised: 1
    isCrossValidateValid: 1
          verboseStorage: 1
         showProgressBar: 1
               isTrained: 0
          dataSetSummary: []
                 dataSet: []
                userData: [1x1 struct]
</pre>




<p>prtClassMap has a property rvs that lists the rvs used for each class in the incoming data set. If there is only one RV specified it is used to model all of the classes. Let&#8217;s classify prtDataGenUnimodal using a quadratic classifier that arises by using a MAP classifier with MVN assumption for each class.</p>


<pre class="codeinput">class = prtClassMap(<span class="string">'rvs'</span>,prtRvMvn);
ds = prtDataGenUnimodal;

trainedClassifier = class.train(ds);
plot(trainedClassifier);
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/morton_blog_20130127_02.png" alt=""> <p>If our data is more complex, we can modify the assumptions of the distributions in both of our classes by setting the &ldquo;rvs&rdquo; parameter to something more flexible. Let&rsquo;s classify prtDataGenBimodal using prtRvKde which uses kernel density estimation.</p><pre class="codeinput">class = prtClassMap(<span class="string">&lsquo;rvs&rsquo;</span>,prtRvKde);
ds = prtDataGenBimodal;</p>

<p>trainedClassifier = class.train(ds);
plot(trainedClassifier);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/morton_blog_20130127_03.png" alt=""> <h2>Conclusions<a name="12"></a></h2><p>As you can see, RVs are pretty powerful parts of the PRT and they can be used in other parts of the PRT to make things flexible.</p><p>In future posts we will talk about how RV objects are used to make flexible mixtures like the GMM and hidden Markov models and we will explore some things that are still in beta such as how we use prtBrv objects to perform variational Bayesian inference for models like Dirichlet process mixtures.</p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Decisions Decisions]]></title>
    <link href="http://newfolder.github.io/blog/2013/01/25/decisions-decisions/"/>
    <updated>2013-01-25T10:45:00-05:00</updated>
    <id>http://newfolder.github.io/blog/2013/01/25/decisions-decisions</id>
    <content type="html"><![CDATA[<p>You may have noticed in a lot of examples, we&#8217;ve made use of prtDecision objects, and might have wondered what exactly those are, and how they work.  Today I&#8217;d like to describe the prtDecision* actions, and when you might want to use them.</p>


<!--/introduction-->


<h2>Contents</h2>


<div><ul><li><a href="#4">Making Manual Decisions</a></li><li><a href="#8">Decision Objects</a></li><li><a href="#11">Concluding</a></li></ul></div>


<p>Let&#8217;s start out with a pretty standard prtDataSet, and we&#8217;ll make a classifier and score a ROC curve:</p>


<pre class="codeinput">ds = prtDataGenUnimodal;
classifier = prtClassFld;
yOutFld = kfolds(classifier,ds,3);
[pf,pd] = prtScoreRoc(yOutFld);
h = plot(pf,pd);
set(h,<span class="string">'linewidth'</span>,3);
title(<span class="string">'ROC Curve for FLD'</span>);
xlabel(<span class="string">'Pfa'</span>);
ylabel(<span class="string">'Pd'</span>);
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Decisions_01.png" alt=""> <p>That ROC curve looks pretty good, but it doesn&rsquo;t tell the whole story. At the end of the day, if you wanted to use your FLD algorithm in a production setting, you&rsquo;ll need to make discrete decisions to take different actions depending on whether you&rsquo;re calling something Class #1 or Class #0.  An ROC curve is suitable for comparing performance across a range of possible operating points, but what if we wanted to know exactly what PD and PFA we were going to get for a particular decision point?</p><p>To clarify matters, let&rsquo;s take a look at what the output from FLD actually looks like.</p><pre class="codeinput">h = plot(1:yOutFld.nObservations,yOutFld.X,1:yOutFld.nObservations,yOutFld.Y);
set(h,<span class="string">&lsquo;linewidth&rsquo;</span>,3);
xlabel(<span class="string">&lsquo;Observation Index&rsquo;</span>);
legend(h,{<span class="string">&lsquo;FLD Output&rsquo;</span>,<span class="string">&lsquo;Class Label&rsquo;</span>});
title(<span class="string">&lsquo;FLD Output &amp; Actual Class Label vs. Observation Index&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Decisions_02.png" alt=""> <p>The above figure shows what&rsquo;s actually going on under the hood &ndash; when a classifier object is run on a data set, the output data set (yOutFld) has it&rsquo;s X value set to the classifier output.  In this case, the yOutFld.X value is a linear weighting of the input variables, and is shown in blue.  You can see how it correlated with the actual class labels (in green).</p><h2>Making Manual Decisions<a name="4"></a></h2><p>Say we wanted to make decisions based on the output of the FLD.  We have to choose a threshold (a point along the y-axis) such that whenever a blue data point is above the threshold, we call the output &ldquo;Class 1&rdquo;, and otherwise we call it &ldquo;Class 0&rdquo;.  By visual inspection, any value between, say, 0 and 2 looks reasonable.  Let&rsquo;s try manually setting a threshold of 1:</p><pre class="codeinput">yOutManual = yOutFld;
yOutManual.X = yOutManual.X &gt; 1;
h = plot(1:yOutManual.nObservations,yOutManual.X,1:yOutManual.nObservations,yOutManual.Y);
ylim([&ndash;.1 1.1]);
set(h,<span class="string">&lsquo;linewidth&rsquo;</span>,3);
xlabel(<span class="string">&lsquo;Observation Index&rsquo;</span>);
legend(h,{<span class="string">&lsquo;Manual Decision Output&rsquo;</span>,<span class="string">&lsquo;Class Label&rsquo;</span>},4);
title(<span class="string">&lsquo;Manual Decision &amp; Actual Class Label vs. Observation Index&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Decisions_03.png" alt=""> <p>You can see that our chosen threshold does pretty well.  The vast majority of the time, the blue line corresponds to the green line.  We can confirm this by considering the percent correct, and a confusion matrix:</p><pre class="codeinput">prtScoreConfusionMatrix(yOutManual);
pc = prtScorePercentCorrect(yOutManual);
title(sprintf(<span class="string">&lsquo;Percent Correct: %.0f%%&rsquo;</span>,pc<em>100));
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Decisions_04.png" alt=""> <p>98% Correct!  That&rsquo;s not bad.  But look at what happens if we try and do the same scoring on the original output from the FLD:</p><pre class="codeinput">prtScoreConfusionMatrix(yOutFld);
pc = prtScorePercentCorrect(yOutFld);
title(sprintf(<span class="string">&lsquo;Percent Correct: %.0f%% (This is clearly wrong!)&rsquo;</span>,pc</em>100));
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Decisions_05.png" alt=""> <p>What happened?  This is a little subtle, but whenever the PRT has to score discrete classes, like with prtScorePercentCorrect and prtScoreConfusionMatrix, it requires that the X values in the dataset to be equal to your best guess as to the real underlying class.</p><p>That worked out great for yOutManual, since we set yOutManual.X to zero for class 0, and 1 for class 1.  But yOutFld has continuous values stored in it (as the earlier figure shows); you need to make discrete decisions for prtScorePercentCorrect or prtScoreConfusionMatrix to make any sense.</p><h2>Decision Objects<a name="8"></a></h2><p>Fortunately, the PRT provides a special kind of prtAction &ndash; prtDecisions to make those decisions for you automatically, so you can score algorithms very easily.</p><p>For example, prtDecisionBinaryMinPe tries to find a threshold based on the training data to minimize the probability of error (Pe). You can use the decision actions like you would use any other actions in a prtAlgorithm:</p><pre class="codeinput">algo = prtClassFld + prtDecisionBinaryMinPe;
yOutDecision = kfolds(algo,ds,3);
prtScoreConfusionMatrix(yOutDecision);
pc = prtScorePercentCorrect(yOutDecision);
title(sprintf(<span class="string">&lsquo;Percent Correct: %.0f%%&rsquo;</span>,pc<em>100));
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Decisions_06.png" alt=""> <p>Now we&rsquo;re back in the ball game!  You can use different decision objects to get performance at different points on the ROC curve, for example prtDecisionBinarySpecifiedPd let&rsquo;s you specify a Pd to operate at:</p><pre class="codeinput">close <span class="string">all</span>;
algo = prtClassFld + prtDecisionBinarySpecifiedPd(<span class="string">&lsquo;pd&rsquo;</span>,.99);
yOutDecision = kfolds(algo,ds,3);
prtScoreConfusionMatrix(yOutDecision);
pc = prtScorePercentCorrect(yOutDecision);
title(sprintf(<span class="string">&lsquo;Percent Correct: %.0f%%&rsquo;</span>,pc</em>100));
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Decisions_07.png" alt=""> <p>Note that the overall probability of error is significantly worse, but almost all of the data from Class 1 was identified as Class 1.  (This may not achieve 99% Pd in some cases since the thresholds are learned differently in each fold, so there is some statistical variation in the actual Pd achieved).</p><p>We can also use prtDecisionMap to perform multi-class decision making. The &ldquo;Map&rdquo; in prtDecisionMap stands for maximum a-posteriori.  This basically means &ldquo;decide the class corresponding to the maximum classifier output&rdquo;.</p><pre class="codeinput">ds = prtDataGenMary;
algo = prtClassKnn + prtDecisionMap;
yOutDecision = kfolds(algo,ds,3);
prtScoreConfusionMatrix(yOutDecision);
pc = prtScorePercentCorrect(yOutDecision);
title(sprintf(<span class="string">&lsquo;Percent Correct: %.0f%%&rsquo;</span>,pc*100));
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Decisions_08.png" alt=""> <h2>Concluding<a name="11"></a></h2><p>So, there you go!  prtDecision objects handle a lot of book-keeping internally, so that you don&rsquo;t generally have to worry about making sure to keep class names and indices straight.  We recommend using them instead of manually making your own decision functions to operate on output classes.</p><p>As always, please feel free to comment or e-mail us with questions or ideas.</p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[An Introduction to the PRT with MNIST]]></title>
    <link href="http://newfolder.github.io/blog/2013/01/21/introduction-to-the-prt/"/>
    <updated>2013-01-21T18:58:00-05:00</updated>
    <id>http://newfolder.github.io/blog/2013/01/21/introduction-to-the-prt</id>
    <content type="html"><![CDATA[<p>The MNIST Database (<a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>) is a very well-known machine learning dataset consisting of a few thousand instances of handwritten digits from 0-9.  MNIST is actually a subset of a larger NIST database, but the authors (see the linked page above) were kind enough to do some basic pre-processing of MNIST for us.  MNIST was for a long time very widely used in the ML literature as an example of an easy to use real data set to evaluate new ideas.</p>




<!--/introduction-->


<h2>Contents</h2>


<div><ul><li><a href="#1">Obtaining, Loading, and Visualizing MNIST Data</a></li><li><a href="#5">Classification: PLSDA</a></li><li><a href="#8">Classification: SVM</a></li><li><a href="#12">Exploring the Results</a></li></ul></div>


<h2>Obtaining, Loading, and Visualizing MNIST Data<a name="1"></a></h2>


<p>Tools to read in the MNIST database into the PRT are available in the newest PRT version.  To conserve bandwidth, the actual MNIST data isn&#8217;t included in the PRT (it would kill our subversion servers).  Instead you can download the MNIST database from the website linked above.  Once you&#8217;ve downloaded it, extract the data into:</p>


<pre class="codeinput">fullfile(prtRoot,<span class="string">'dataGen'</span>,<span class="string">'dataStorage'</span>,<span class="string">'MNIST'</span>) <span class="comment">%MATLAB command will tell you the directory</span>
</pre>


<pre class="codeoutput">
ans =

C:\Users\Pete\Documents\MATLAB\toolboxes\nfPrt\dataGen\dataStorage\MNIST

</pre>


<p>For example, on my system:</p>


<pre class="codeinput">ls(fullfile(prtRoot,<span class="string">'dataGen'</span>,<span class="string">'dataStorage'</span>,<span class="string">'MNIST'</span>))
</pre>


<pre class="codeoutput">
.                        t10k-labels.idx1-ubyte   
..                       train-images.idx3-ubyte  
t10k-images.idx3-ubyte   train-labels.idx1-ubyte  

</pre>


<p>Once the MNIST files are in the right place, execute the PRT command:</p>


<pre class="codeinput">dsTrain = prtDataGenMnist;
</pre>


<p>to extract the data.  ( Note, prtDataGenMnist makes use of a M-file function called readMNIST by Siddharth Hegde.  It&#8217;s available from: <a href="http://www.mathworks.com/matlabcentral/fileexchange/27675-read-digits-and-labels-from-mnist-database">http://www.mathworks.com/matlabcentral/fileexchange/27675-read-digits-and-labels-from-mnist-database</a> ).</p>


<p>Once loaded, we can use a number of different tools to visualize the data.  First, let&#8217;s visualize the data as images.  We know that the images are size 28x28, but since the prtDataSetClass object expects each observation to correspond to a 1xN vector, we store all the 28x28 images as 1x784 vectors.</p>


<pre class="codeinput">imageSize = [28 28];

<span class="keyword">for</span> i = 1:9;
    subplot(3,3,i);
    x = dsTrain.getX(i); <span class="comment">%1x784</span>
    y = dsTrain.getY(i);
    imagesc(reshape(x,imageSize));
    colormap <span class="string">gray</span>;
    title(sprintf(<span class="string">'MNIST; Digit = %d'</span>,y));
<span class="keyword">end</span>
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/testBlog_01.png" alt=""> <h2>Classification: PLSDA<a name="5"></a></h2><p>What kinds of classification approaches can we apply to this data set? We need to satisfy a few requirements: 1) M-Ary classification, 2) Relatively fast, 3) Relatively insensitive to a large number of dimensions (784 dimensional vectors). One particularly fast, linear approach to classification that&#8217;s relatively insensitive to the number of feature dimensions is partial-least squares discriminant analysis, implemented in the PRT as prtClassPLSDA.  With only a few lines of code we can implement and evaluate a PLSDA classifier on the MNIST data, for example:</p><pre class="codeinput">algo = prtClassPlsda(<span class="string">&lsquo;nComponents&rsquo;</span>,20) + prtDecisionMap; <span class="comment">%we include the Max-A-Posteriori classifier</span>
yOut = algo.kfolds(dsTrain,3); <span class="comment">%3 folds x-val</span>
pc = prtScorePercentCorrect(yOut);
subplot(1,1,1);
prtScoreConfusionMatrix(yOut);
title(sprintf(<span class="string">&lsquo;3-Fold X-Val PLSDA on 10,000 MNIST Database Train Samples; %.0f%% Correct&rsquo;</span>,pc*100));
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/testBlog_02.png" alt=""> <p>This basic example results in the above figure, where we see we&#8217;ve achieved about 84% correct classification, and we can analyze confusions between digits.  For example, the digits 4 and 9 are often confused, which seems intuitive since they look relatively similar.</p><p>We can also evaluate the PLSDA classifier trained on 10,000 training points and evaluated on the MNIST testing data.  To do so we first load the testing data, then train our classifier and evaluate it:</p></p>

<pre class="codeinput">dsTest = prtDataGenMnistTest;
algo = algo.train(dsTrain);
yOut = algo.run(dsTest);
pc = prtScorePercentCorrect(yOut);
subplot(1,1,1);
prtScoreConfusionMatrix(yOut);
title(sprintf(<span class="string">'PLSDA on 10,000 MNIST Database Test Samples; %.0f%% Correct'</span>,pc*100));
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/testBlog_03.png" alt=""> <p>Performance on the test set is relatively similar to performance in cross-validation as can be seen above.</p><p>Overall, our performance is hovering around a 15% error rate.  That&#8217;s roughly comparable to the 12% error reported in LeCun et al., 1988, and here we&#8217;re not using a lot of the techniques in the Le Cun paper (and this is with barely 5 lines of code!).</p><h2>Classification: SVM<a name="8"></a></h2><p>As the results on <a href="http://yann.lecun.com/exdb/mnist/"><a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a></a> illustrate, other approaches to digit classification have done much better than our simple PLSDA classifier.  We can use the PRT to apply more complicated classifiers to the same data also, and hopefully decrease our error rate.</p><p>For example, consider a simple application of an SVM classifier to the digit recognition problem.  Since the SVM is not an M-ary classification technique, we need to wrap our SVM in a One-Vs-All classifier to perform M-ary classification (Warning: the following code took about 30 minutes to run on my laptop):</p><pre class="codeinput">marySvm = prtPreProcZmuv + prtClassBinaryToMaryOneVsAll(<span class="string">&lsquo;baseClassifier&rsquo;</span>,prtClassLibSvm) + prtDecisionMap;
yOut = marySvm.kfolds(dsTrain,3);
pc = prtScorePercentCorrect(yOut);
subplot(1,1,1);
prtScoreConfusionMatrix(yOut);
title(sprintf(<span class="string">&lsquo;3-Fold X-Val SVM on 10,000 MNIST Database Train Samples; %.0f%% Correct&rsquo;</span>,pc*100));
</pre><pre class="codeoutput">Warning: Non-finite or zero standard deviation encountered.  Replacing invalid
standard deviations with 1
Warning: Non-finite or zero standard deviation encountered.  Replacing invalid
standard deviations with 1
Warning: Non-finite or zero standard deviation encountered.  Replacing invalid
standard deviations with 1
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/testBlog_04.png" alt=""> <p>As can be seen above, the SVM achieves an error rate of 5% on this data set!  That&#8217;s a significant improvement over the PLSDA classification we showed before.  Similarly to with PLSDA, we can also evaluate the algorithm on completely separate testing data:</p></p>

<pre class="codeinput">marySvm = marySvm.train(dsTrain);
yOut = marySvm.run(dsTest);
pc = prtScorePercentCorrect(yOut);
subplot(1,1,1);
prtScoreConfusionMatrix(yOut);
title(sprintf(<span class="string">'PLSDA on 10,000 MNIST Database Test Samples; %.0f%% Correct'</span>,pc*100));
</pre>


<pre class="codeoutput">Warning: Non-finite or zero standard deviation encountered.  Replacing invalid
standard deviations with 1 
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/testBlog_05.png" alt=""> <p>And we see that performance is comparable to the cross-validated results. (Note that more advanced applications of SVM classifiers can do even better than the results reported here &#8211; Le Cun et al., 1998 reported 1.4% error rates with an SVM and some additional processing).</p><h2>Exploring the Results<a name="12"></a></h2><p>If we wanted to improve classification, we could optimize over the SVM parameters, kernel, pre-processing etc.  But before we did that, it might be instructive to investigate what digits the SVM classifier is mislabeling, and see if some of them seem like reasonable mistakes to make.  The following code will pick 9 instances where the SVM output label was different from the actual data label, and plot them in a subplot.</p></p>

<pre class="codeinput">incorrect = find(yOut.getX ~= yOut.getY);
yOutTestMisLabeled = yOut.retainObservations(incorrect);
dsTestMisLabeled = dsTest.retainObservations(incorrect);
<span class="keyword">for</span> i = 1:9; <span class="comment">%dsTestMisLabeled.nObservations;</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;randWrong = ceil(rand*dsTestMisLabeled.nObservations); <span class="comment">%pick a random wrong element</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;subplot(3,3,i);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x = dsTestMisLabeled.getX(randWrong);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;img = reshape(x,imageSize);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;imagesc(img);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;colormap <span class="string">gray</span>;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
title(sprintf(<span class="string">'Actual: %d; SVM Label: %d'</span>,yOutTestMisLabeled.getY(randWrong),yOutTestMisLabeled.getX(randWrong)));
<span class="keyword">end</span>
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/testBlog_06.png" alt=""> <p>Visual inspection of these mistakes illustrate some of the causes of confusions in the SVM.  For example, highly slanted digits are often mis-labeled.  Mitigating some of these mistakes may require significantly more than simply optimizing SVM parameters!</p><p>Interested readers can refer to a large body of literature that has previously investigated this data set (<a href="http://yann.lecun.com/exdb/mnist/"><a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a></a>) for tips, tricks, and ideas for further improving performance on this data set.  One particularly exciting recent advance is based on Hinton&#8217;s deep learning networks, which enables very efficient learning on the MNIST database <a href="www.cs.toronto.edu/~hinton/science.pdf">www.cs.toronto.edu/~hinton/science.pdf</a>).</p><p>We hope this example shows how quickly you can get from data to results with the PRT.  Please let us know if you have comments or questions!</p></p>
]]></content>
  </entry>
  
</feed>
