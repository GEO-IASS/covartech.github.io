<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[PRT Blog]]></title>
  <link href="http://newfolder.github.io/atom.xml" rel="self"/>
  <link href="http://newfolder.github.io/"/>
  <updated>2013-06-24T13:22:05-04:00</updated>
  <id>http://newfolder.github.io/</id>
  <author>
    <name><![CDATA[Kenneth Morton and Peter Torrione]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Max-Pooling Feature Representations in MSRCORID]]></title>
    <link href="http://newfolder.github.io/blog/2013/03/10/max-pooling/"/>
    <updated>2013-03-10T18:13:00-04:00</updated>
    <id>http://newfolder.github.io/blog/2013/03/10/max-pooling</id>
    <content type="html"><![CDATA[<p>A few weeks ago we took a look at a paper by Coates and Ng that dealt with learning feature representations for image processing and classification.  (See: <a href="http://newfolder.github.io/blog/2013/02/27/learning-feature-representations/">this</a>). Today I want to take a second look at that paper, and especially what they mean by max-pooling over regions of the image.</p>


<!--/introduction-->




<h2>Contents</h2>


<div><ul><li><a href="#1">The MSRCORID Database</a></li><li><a href="#2">Max-Pooling</a></li><li><a href="#5">Multiple Classes</a></li><li><a href="#7">Conclusions</a></li><li><a href="#8">Bibliography</a></li></ul></div>


<h2>The MSRCORID Database<a name="1"></a></h2>


<p>If you have already read through <a href="http://newfolder.github.io/blog/2013/02/27/learning-feature-representations/">our previous post</a>, you know how to get the Microsoft Research Cambridge Object Recognition Image Database (MSRCORID), which is really a fantastic resource for image processing and classification.</p>


<p>Once you&#8217;ve downloaded, we can run the following code which was for the most-prt ripped right out of the previous blog post:</p>


<pre class="codeinput">ds = prtDataGenMsrcorid;

patchSize = [8 8];
col = [];
<span class="keyword">for</span> imgInd = 1:ds.nObservations;
    img = ds.X{imgInd};
    img = rgb2gray(img);
    img = imresize(img,.5);
    col = cat(1,col,im2col(img,patchSize,<span class="string">'distinct'</span>)');
<span class="keyword">end</span>
dsCol = prtDataSetClass(double(col));

preProc = prtPreProcZeroMeanRows + prtPreProcStdNormalizeRows(<span class="string">'varianceOffset'</span>,10) + prtPreProcZca;
preProc = preProc.train(dsCol);
dsNorm = preProc.run(dsCol);

skm = prtClusterSphericalKmeans(<span class="string">'nClusters'</span>,50);
skm = skm.train(dsNorm);
</pre>


<h2>Max-Pooling<a name="2"></a></h2>


<p>Last time, we used a simple bag-of-words model to do classification based on the feature vectors in each image.  That&#8217;s definitely an interesting way to proceed, but most image-processing techniques make use of something called &#8220;max-pooling&#8221; to aggregate feature vectors over small regions of an image.</p>


<p>The process can be accomplished in MATLAB using blockproc.m, which is in the Image-processing toolbox.  (If you don&#8217;t have image processing, it&#8217;s not too hard to write a replacement for blockproc.)</p>


<p>The goal of max-pooling is to aggregate feature vectors over local regions of an image.  For example, we can take the MAX of the cluster memberships over each 8x8 region in an image using something like:</p>


<p>featsBp = blockproc(feats,[8 8],@(x)max(max(x.data,[],1),[],2));</p>


<p>Where we&#8217;ve assumed that feats is size nx x ny x nFeats.</p>


<p>Max pooling is nice because it reduces the dependency of the feature vectors on their exact placement in an image (each element of each 8x8 block gets treated about the same), and it also maintains a lot of the information that was in each of the feature vectors, especially when the feature vectors are expected to be sparse (e.g., have a lot of zeros; see http//www.ece.duke.edu/~lcarin/Bo12.3.2010.ppt).</p>


<p>There&#8217;s a lot more to max-pooling than we have time to get into here, for example, you can max-pool, and then re-cluster, and then re-max-pool! This is actually a super clever technique to reduce the amount of spatial variation in your image, and also capture information about the relative placements of various objects.</p>


<pre class="codeinput">featVec = nan(ds.nObservations,skm.nClusters*20);
clusters = skm.run(dsNorm);

<span class="keyword">for</span> imgInd = 1:ds.nObservations;
    img = ds.X{imgInd};
    img = rgb2gray(img);
    imgSize = size(img);

    <span class="comment">% Extract the sub-patches</span>
    col = im2col(img,patchSize,<span class="string">'distinct'</span>);
    col = double(col);
    dsCol = prtDataSetClass(col');
    dsCol = run(preProc,dsCol);
    dsFeat = skm.run(dsCol);
    dsFeat.X = max(dsFeat.X,.05);

    <span class="comment">% Max Pool!</span>
    <span class="comment">%   Feats will be size 30 x 40 x nClusters</span>
    <span class="comment">%   featsBp will be size [4 x 5] x nClusters (because of the way</span>
    <span class="comment">%   blockproc handles edsges)</span>
    feats = reshape(dsFeat.X,imgSize(1)/8,imgSize(2)/8,[]);
    featsBp = blockproc(feats,[8 8],@(x)max(max(x.data,[],1),[],2));

    <span class="comment">% We'll cheat a little here, and use the whole max-pooled feature set</span>
    <span class="comment">% as our feature vector.  Instead, we might want to re-cluster, and</span>
    <span class="comment">% re-max-pool, and repeat this process a few times.  For now, we'll</span>
    <span class="comment">% keep it simple:</span>
    featVec(imgInd,:) = featsBp(:);
<span class="keyword">end</span>
</pre>


<p>Now that we&#8217;ve max-pooled, we can use our extracted features for classification - we&#8217;ll use a simple PLSDA + MAP classifier and decision algorithm here:</p>


<pre class="codeinput">dsFeat = prtDataSetClass(featVec,ds.targets);
dsFeat.classNames = ds.classNames;

yOut = kfolds(prtClassPlsda + prtDecisionMap,dsFeat,3);

close <span class="string">all</span>;
prtScoreConfusionMatrix(yOut)
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_ExampleCoatesNg_Kmeans_Mscorid_2_01.png" alt=""> <p>Almost 99% correct!  We&rsquo;ve improved performance over our previous work with bag-of-words models, and an SVM, by just (1) max-pooling, and (2) replacing the SVM with a PLSDA classifier.</p><h2>Multiple Classes<a name="5"></a></h2><p>Until now we&rsquo;ve focused on just two classes in MSRCORID.  But there are a lot of types of objects in the MSRCORID database.  In the following, we just repeat a bunch of the code from above, and run it on a data set containing images of benches, buildings, cars, chimneys, clouds and doors:</p><pre class="codeinput">ds = prtDataGenMsrcorid({<span class="string">&lsquo;benches_and_chairs&rsquo;</span>,<span class="string">&lsquo;buildings&rsquo;</span>,<span class="string">&lsquo;cars\front view&rsquo;</span>,<span class="string">&lsquo;cars\rear view&rsquo;</span>,<span class="string">&lsquo;cars\side view&rsquo;</span>,<span class="string">&lsquo;chimneys&rsquo;</span>,<span class="string">&lsquo;clouds&rsquo;</span>,<span class="string">&lsquo;doors&rsquo;</span>});</p>

<p>patchSize = [8 8];
col = [];
<span class="keyword">for</span> imgInd = 1:ds.nObservations;</p>

<pre><code>img = ds.X{imgInd};
img = rgb2gray(img);
img = imresize(img,.5);
col = cat(1,col,im2col(img,patchSize,&lt;span class="string"&gt;'distinct'&lt;/span&gt;)');
</code></pre>

<p><span class="keyword">end</span>
dsCol = prtDataSetClass(double(col));</p>

<p>preProc = prtPreProcZeroMeanRows + prtPreProcStdNormalizeRows(<span class="string">&lsquo;varianceOffset&rsquo;</span>,10) + prtPreProcZca;
preProc = preProc.train(dsCol);
dsNorm = preProc.run(dsCol);</p>

<p>skm = prtClusterSphericalKmeans(<span class="string">&lsquo;nClusters&rsquo;</span>,50);
skm = skm.train(dsNorm);</p>

<p>featVec = nan(ds.nObservations,skm.nClusters*20);
clusters = skm.run(dsNorm);</p>

<p><span class="keyword">for</span> imgInd = 1:ds.nObservations;</p>

<pre><code>img = ds.X{imgInd};
img = rgb2gray(img);
imgSize = size(img);

&lt;span class="comment"&gt;% Extract the sub-patches&lt;/span&gt;
col = im2col(img,patchSize,&lt;span class="string"&gt;'distinct'&lt;/span&gt;);
col = double(col);
dsCol = prtDataSetClass(col');
dsCol = run(preProc,dsCol);
dsFeat = skm.run(dsCol);
dsFeat.X = max(dsFeat.X,.05);

&lt;span class="comment"&gt;% Max Pool!&lt;/span&gt;
&lt;span class="comment"&gt;%   Feats will be size 30 x 40 x nClusters&lt;/span&gt;
&lt;span class="comment"&gt;%   featsBp will be size [4 x 5] x nClusters (because of the way&lt;/span&gt;
&lt;span class="comment"&gt;%   blockproc handles edsges)&lt;/span&gt;
feats = reshape(dsFeat.X,imgSize(1)/8,imgSize(2)/8,[]);
featsBp = blockproc(feats,[8 8],@(x)max(max(x.data,[],1),[],2));

&lt;span class="comment"&gt;% We'll cheat a little here, and use the whole max-pooled feature set&lt;/span&gt;
&lt;span class="comment"&gt;% as our feature vector.  Instead, we might want to re-cluster, and&lt;/span&gt;
&lt;span class="comment"&gt;% re-max-pool, and repeat this process a few times.  For now, we'll&lt;/span&gt;
&lt;span class="comment"&gt;% keep it simple:&lt;/span&gt;
featVec(imgInd,:) = featsBp(:);
</code></pre>

<p><span class="keyword">end</span></p>

<p>dsFeat = prtDataSetClass(featVec,ds.targets);
dsFeat.classNames = ds.classNames;</p>

<p>yOut = kfolds(prtClassPlsda(<span class="string">&lsquo;nComponents&rsquo;</span>,10) + prtDecisionMap,dsFeat,3);
yOut.classNames = cellfun(@(s)s(1:min([length(s),10])),yOut.classNames,<span class="string">&lsquo;uniformoutput&rsquo;</span>,false);
close <span class="string">all</span>;
prtScoreConfusionMatrix(yOut);
set(gcf,<span class="string">&lsquo;position&rsquo;</span>,[426   125   777   558]);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_ExampleCoatesNg_Kmeans_Mscorid_2_02.png" alt=""> <p>Now we&rsquo;re doing some image processing!  Overall we got about 90% correct, and that includes a lot of confusions between cars\front and cars\rear. That makes sense since the front and backs of cars look pretty similar, and there are only 23 car front examples in the whole data set.</p><h2>Conclusions<a name="7"></a></h2><p>The code in a lot of this blog entry is pretty gross &ndash; for example we have to constantly be taking data out of, and putting it back into the appropriate image sizes.</p><p>At some point in the future, we&rsquo;d like to introduce a good prtDataSet that will handle cell-arrays containing images properly.  We&rsquo;re not there yet, but when we are, we&rsquo;ll let you know on this blog!</p><p>Happy coding!</p><h2>Bibliography<a name="8"></a></h2><p>Adam Coates and Andrew Y. Ng, Learning Feature Representations with K-means, G. Montavon, G. B. Orr, K.-R. Muller (Eds.), Neural Networks: Tricks of the Trade, 2nd edn, Springer LNCS 7700, 2012</p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using prtPath]]></title>
    <link href="http://newfolder.github.io/blog/2013/03/06/using-prtpath/"/>
    <updated>2013-03-06T13:24:00-05:00</updated>
    <id>http://newfolder.github.io/blog/2013/03/06/using-prtpath</id>
    <content type="html"><![CDATA[<p>We recommend as part of installation that you edit (or create) your startup.m file to include a call to prtPath. This is adds the PRT to your MATLAB search path automatically each time you start MATLAB. There are alternatives to path management (such as the pathtool() or addpath() and savepath()) but we recommend using a startup.m file. Hopefully this post will explain one reason we recommend that.</p>




<p>One of the reasons that we like to use prtPath in a startup file is that prtPath does more than just add the PRT and all of it&#8217;s subfolders to your path.</p>


<!--/introduction-->


<h2>Contents</h2>


<div><ul><li><a href="#1">prtPath and ] directories</a></li><li><a href="#3">Why &#8220;]&#8221;?</a></li><li><a href="#4">Conclusions</a></li></ul></div>


<h2>prtPath and ] directories<a name="1"></a></h2>


<p>prtPath is actually very basic. It calls the function prtRoot and uses the MATLAB function genpath() to get a list of all of the subdirectories in the prtFolder. It then eventually calls addpath() to add that list of directories to the MATLAB path (it does not save the path for future sessions of MATLAB).</p>


<p>Before prtPath() calls addpath() though, it selectively removes some directories from the subdirectory list.</p>


<p>First it removes any directory that starts with a &#8220;.&#8221;. This was added to prevent any hidden folders (like those from source control systems) from showing up in the MATLAB path.</p>


<p>More importantly though, it removes any folders from the list that start with a &#8220;]&#8221;. This is something special that we put in to add some extra functionality to the PRT.</p>


<p>Most of our users want to stick to code that is well tested and is known to behave nicely. But as we go about our jobs and use the PRT we need to add some new functionality. We typically add things like: new classifiers, new datatypes or new pre processing techniques.</p>


<p>Some of our users want access to this newest code so it gets added to the PRT in the &#8220;]alpha&#8221; and eventually the &#8220;]beta&#8221; folder. By default prtPath will not include these folders in the path. Instead you have to tell prtPath that you are willing to accept the responsibilities of the bleeding edge. You do this by giving prtPath a list of &#8220;]&#8221; folders that you want to include. (Or rather not exclude).</p>


<p>For example:</p>


<pre class="codeinput">prtPath(<span class="string">'alpha'</span>, <span class="string">'beta'</span>);
</pre>


<p>will add both the &#8220;]alpha&#8221; and &#8220;]beta&#8221; folders (and their subfolders) to the path.</p>


<p>Currently in the PRT we have one other &#8220;]&#8221; folder, &#8220;]internal&#8221;. In the internal folder you will find some code on unit testing and documentation building. You probably wont be interested in much that&#8217;s in there so I probably wouldn&#8217;t clutter my path with it.</p>


<h2>Why &#8220;]&#8221;?<a name="3"></a></h2>


<p>We were searching for a character that is a valid path (folder) name character on all major operating systems and is at the same time a character that most people wouldn&#8217;t start a directory name with. MATLAB already uses &#8220;@&#8221; for (old style) class definitions and &#8220;+&#8221; for packages. We thought &#8220;]&#8221; fit all of these criteria.</p>


<h2>Conclusions<a name="4"></a></h2>


<p>We hope that cleared up a little of why we recommend prtPath over pathtool(), at least for the PRT. In general just call prtPath() by itself but if you want to see what might lie ahead for the PRT checkout the ]alpha and ]beta folders. In some future posts we will talk about some things in these folders that might be of interest to you. Maybe that will entice you to explore the ].</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Learning Feature Representations]]></title>
    <link href="http://newfolder.github.io/blog/2013/02/27/learning-feature-representations/"/>
    <updated>2013-02-27T14:59:00-05:00</updated>
    <id>http://newfolder.github.io/blog/2013/02/27/learning-feature-representations</id>
    <content type="html"><![CDATA[<p>Today I wanted to go through an interesting paper I recently read and show how to implement parts of that paper in the PRT.  The paper is <i>Learning Feature Representations with K-means</i> , by Adam Coates and Andrew Y. Ng (see below for full citation).</p>


<p>The meat of the Coates and Ng paper deals with how to use K-means to extract meaningful dictionaries from image data.  The latter part of the paper talks about how to do real machine learning with max-pooling for classification, but for today, I just wanted to introduce the MSRCORID (Microsoft Research Cambridge Object Recognition Image Database) data in the PRT and also show how to use the PRT to do some K-means dictionary learning.</p>




<!--/introduction-->


<h2>Contents</h2>


<div><ul><li><a href="#1">The MSRCORID Database</a></li><li><a href="#3">Extracting Patches</a></li><li><a href="#4">Normalization</a></li><li><a href="#5">K-Means</a></li><li><a href="#7">Simple Bag-Of-Words Classification</a></li><li><a href="#10">Bibliography</a></li></ul></div>


<h2>The MSRCORID Database<a name="1"></a></h2>


<p>For fun, I downloaded a new image database to play with for this data. The data is available for download from here: <a href="http://research.microsoft.com/en-us/downloads/b94de342-60dc-45d0-830b-9f6eff91b301/default.aspx">http://research.microsoft.com/en-us/downloads/b94de342-60dc-45d0-830b-9f6eff91b301/default.aspx</a></p>


<p>You can load the data automatically in the PRT, if you update to the newest version and run:</p>


<pre class="codeinput">ds = prtDataGenMsrcorid;
</pre>


<p>By default, that command will give a dataset with only images of chimneys and single flowers.  Look at the help for prtDataGenMsrcorid to see how to load data from these, and a lot more interesting classes.</p>


<p>You should note that prtDataGenMsrcorid does not output a prtDataSetClass, it produces a prtDataSetCellArray.  prtDataSetCellArray data objects are relatively new, and not fully documented, but they&#8217;re useful when you want to deal with datasets where each observation can have different sizes - e.g., images.</p>


<p>You can access elements using cell-array notation to access the .X field of the prtDataSet, for example:</p>


<pre class="codeinput">subplot(2,1,1);
imshow(ds.X{1});
title(<span class="string">'Flower'</span>);

subplot(2,1,2);
imshow(ds.X{end});
title(<span class="string">'Chimney'</span>);
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_ExampleCoatesNg_Kmeans_Mscorid_01.png" alt=""> <h2>Extracting Patches<a name="3"></a></h2><p>To generate a dictionary requires segmenting the initial images provided to us into sub-regions.  We can acheive this by using the MATLAB function im2col which with will convert every 8x8 sub-image to a 64x1 element vector.</p><pre class="codeinput">patchSize = [8 8];
col = [];
<span class="keyword">for</span> imgInd = 1:ds.nObservations;</p>

<pre><code>img = ds.X{imgInd};
img = rgb2gray(img);
img = imresize(img,.5);
col = cat(1,col,im2col(img,patchSize,&lt;span class="string"&gt;'distinct'&lt;/span&gt;)');
</code></pre>

<p><span class="keyword">end</span>
dsCol = prtDataSetClass(double(col));
</pre><h2>Normalization<a name="4"></a></h2><p>[Coates, 2012], makes it very clear that proper data normalization on a per-patch basis is fundamental to getting meaningful K-means centroids. The three main steps in the normalization are mean-normalization, energy normalization, and ZCA centering.  These are all implemented in the PRT as prtPreProcZeroMeanRows, prtPreProcStdNormalizeRows, and prtPreProcZca.</p><p>As always, we can buld an algorithm out of these independent components, then train and run the algorithm on the dsCol data we created earlier:</p><pre class="codeinput">preProc = prtPreProcZeroMeanRows + prtPreProcStdNormalizeRows(<span class="string">&lsquo;varianceOffset&rsquo;</span>,10) + prtPreProcZca;
preProc = preProc.train(dsCol);
dsNorm = preProc.run(dsCol);
</pre><h2>K-Means<a name="5"></a></h2><p>[Coates, 2012], makes a compelling case that K-means clustering is capable of learning dictionaries that can be easily used for classification.  The K-means algorithm in Coates paper is particularly intruiging, and its very fast compared to standard K-means using euclidean distances.  We&rsquo;ve implemented the K-means algorithm as described in [Coates, 2012] as prtClusterSphericalKmeans, which is much faster than using the regular K-means.</p><pre class="codeinput">skm = prtClusterSphericalKmeans(<span class="string">&lsquo;nClusters&rsquo;</span>,50);
skm = skm.train(dsNorm);
</pre><p>We can visualize the resulting cluster centers from the K-means processing by looking a the skm.clusterCenters, and plotting the first 50.  We&rsquo;ll sort these by how often data vectors were assigned to each cluster, so the top-left has the most elements, and the bottom-right has the least.</p><pre class="codeinput"></p>

<p>yOutK = skm.run(dsNorm);
[val,ind] = max(yOutK.X,[],2);
boolMat = zeros(size(yOutK.X));
indices = sub2ind(size(boolMat),(1:size(boolMat,1))&lsquo;,ind(:));
boolMat(indices) = 1;</p>

<p>clusterCounts = sum(boolMat);
[v,sortInds] = sort(clusterCounts,<span class="string">&lsquo;descend&rsquo;</span>);</p>

<p>c = skm.clusterCenters';
<span class="keyword">for</span> i = 1:50</p>

<pre><code>subplot(5,10,i);
imagesc(reshape(c(sortInds(i),:),patchSize));
title(v(i));
tickOff;
</code></pre>

<p><span class="keyword">end</span>
colormap <span class="string">gray</span>
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_ExampleCoatesNg_Kmeans_Mscorid_02.png" alt=""> <h2>Simple Bag-Of-Words Classification<a name="7"></a></h2><p>We can use some of the approaches from [Coates, 2012] to do some simple classification, also.  For example, we can use our new K-means clustering algorithm to generate features for each observation.  We can do this for every patch we extract from each image, but we&rsquo;d like to make decisions on an image-by-image basis, so we need to aggregate over the resulting feature vectors somehow.</p><p>A clever way to do this is to use max-pooling and deep-learning as specified in [Coates, 2012], but for now we&rsquo;ll just take the mean of the resulting feature vectors (in a manner similar to bag-of-words classification <a href="http://en.wikipedia.org/wiki/Bag-of-words_model"><a href="http://en.wikipedia.org/wiki/Bag-of-words_model">http://en.wikipedia.org/wiki/Bag-of-words_model</a></a> )</p><pre class="codeinput">featVec = nan(ds.nObservations,skm.nClusters);</p>

<p><span class="keyword">for</span> imgInd = 1:ds.nObservations;</p>

<pre><code>img = ds.X{imgInd};
img = rgb2gray(img);
col = im2col(img,patchSize,&lt;span class="string"&gt;'distinct'&lt;/span&gt;);
col = double(col);
dsCol = prtDataSetClass(col');
dsCol = run(preProc,dsCol);
dsFeat = skm.run(dsCol);
feats = max(dsFeat.X,.05);
featVec(imgInd,:) = mean(feats);
</code></pre>

<p><span class="keyword">end</span>
</pre><p>Now we can classify our feature vectors using another classification algorithm &ndash; e.g., here we use a SVM, with ZMUV pre-processing, and max-a-posteriori classification.</p><pre class="codeinput">dsFeat = prtDataSetClass(featVec,ds.targets);
dsFeat.classNames = ds.classNames;</p>

<p>yOut = kfolds(prtPreProcZmuv + prtClassLibSvm + prtDecisionMap,dsFeat,3);</p>

<p>close <span class="string">all</span>;
prtScoreConfusionMatrix(yOut)
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_ExampleCoatesNg_Kmeans_Mscorid_03.png" alt=""> <p>Hey!  That&rsquo;s not too bad for a few lines of code.  At some point in the future we&rsquo;ll take on the rest of the [Coates, 2012] paper, but in the meantime, let us know if you implement the max-pooling or other processes outlined therein.</p><p>Happy coding!</p><p>Note: we created prtPreProcZca, prtClusterSphericalKmeans, and prtDataGenMsrcorid for this blog entry; they&rsquo;re all in the PRT, but are recent (as of 2/27/2013) so download a new version to get access to all these.</p><h2>Bibliography<a name="10"></a></h2><p>Adam Coates and Andrew Y. Ng, Learning Feature Representations with K-means, G. Montavon, G. B. Orr, K.-R. Muller (Eds.), Neural Networks: Tricks of the Trade, 2nd edn, Springer LNCS 7700, 2012</p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[IEEE GRSS Hyperspectral Data]]></title>
    <link href="http://newfolder.github.io/blog/2013/02/16/ieee-grss-hyperspectral-data/"/>
    <updated>2013-02-16T20:06:00-05:00</updated>
    <id>http://newfolder.github.io/blog/2013/02/16/ieee-grss-hyperspectral-data</id>
    <content type="html"><![CDATA[<p>
Hi everyone.  Outside of software development, we also do some work in geoscience and remote sensing.  As a result, we were very excited to see an announcement from the IEEE GRSS that they were making some new data sets available - in particular, a hyperspectral data set, and a LIDAR data set (if you&#8217;re not familiar with these technologies, see here: <a href="http://en.wikipedia.org/wiki/Hyperspectral_imaging">http://en.wikipedia.org/wiki/Hyperspectral_imaging</a> and <a href="http://en.wikipedia.org/wiki/LIDAR">http://en.wikipedia.org/wiki/LIDAR</a>).</p>


<!--/introduction-->


<h2>Contents</h2>


<div><ul><li><a href="#1">Getting Started</a></li><li><a href="#3">A Note About UnLabeled Points</a></li><li><a href="#5">Visualization of the Hyperspectral and LIDAR data</a></li><li><a href="#6">Hyperspectral Data</a></li><li><a href="#7">PC Projections</a></li><li><a href="#9">15-Class Classification</a></li><li><a href="#11">Evaluation</a></li><li><a href="#12">Conclusions</a></li></ul></div>


<h2>Getting Started<a name="1"></a></h2>


<p>We&#8217;ve made some M-files that will load in the data for you (see the .ZIP file at the end of this post), but you&#8217;ll need to go to the GRSS website to download the data from here first: <a href="http://hyperspectral.ee.uh.edu/?page_id=459">http://hyperspectral.ee.uh.edu/?page_id=459</a>).</p>


<p>To load the data, first download the .ZIP file at the end of this post, then run the following.  But change the line below to point to the right directory, so for example, I have a file C:\Users\pete\Documents\data\2013IEEE_GRSS_DF_Contest\2013_IEEE_GRSS_DF_Contest_CASI.tif</p>


<pre class="codeinput">imgSize = [349 1905];
grssDir = <span class="string">'C:\Users\pete\Documents\data\2013IEEE_GRSS_DF_Contest\'</span>;
[dsCasi,dsLidar] = prtExampleReadGrss2013(grssDir);
</pre>


<p>Note, if you don&#8217;t want to use the PRT for anything, but would like to automatically read in the ROI file provided by GRSS, download the .ZIP at the end of this file, and just use:</p>


<pre class="codeinput">
regions = grssRoiRead(roiFile);
</pre>


<p>Each of these data sets has 664845 observations (from the 349x1905 dimensional image) the hyperspectral data has 144 dimensions, and the LIDAR data only has one dimension. The fine folks at GRSS were kind enough to provide labels for about 2832 pixels from the data sets.  These are from 15 different classes: grass_healthy, grass_stressed, grass_synthetic, tree, soil, water, residential, commercial, road, highway, railway, parking_lot1, parking_lot2, tennis_court, running_track.</p>


<h2>A Note About UnLabeled Points<a name="3"></a></h2>


<p>This data set contains a lot of unlabeled data.  Previously, to use the PRT with unlabeled data required ad-hoc fiddling with targets and classes.  But as of Jan 7, 2013, the PRT now handles unlabeled data inherently.  The PRT uses NaNs to represent unlabeled data points - e.g.,</p>


<pre class="codeinput">numNan = length(find(isnan(dsCasi.targets))); <span class="comment">%there are 662013 unlabeled points</span>
disp(numNan)
</pre>


<pre class="codeoutput">      662013

</pre>


<p>You can get a data set using only the labeled data using new mthods included specifically for unlabeled data:</p>


<pre class="codeinput">dsUnLabeled = dsCasi.removeLabeled;
dsLabeled = dsCasi.retainLabeled;
</pre>


<h2>Visualization of the Hyperspectral and LIDAR data<a name="5"></a></h2>


<p>We can visualize the data in the form of the spatial image by re-sizing it to be the correct dimensionality.  For example, the next lines reshape the total intensity (SUM) of the hyperspectral data, and reshape the LIDAR data into the right size:</p>


<pre class="codeinput">x = dsCasi.X;
x = reshape(x',[144 imgSize]);
imgCasi = sqrt(squeeze(sum(x.^2)));

subplot(2,1,1);
imagesc(imgCasi)
title(<span class="string">'CASI Data'</span>);

xLidar = dsLidar.X;
imgLidar = reshape(xLidar,imgSize);
subplot(2,1,2);
imagesc(imgLidar)
title(<span class="string">'LIDAR Data'</span>);
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Grss_01.png" alt=""> <h2>Hyperspectral Data<a name="6"></a></h2><p>The remainder of this blog entry will just show a few examples of how to use the PRT in combination with the CASI hyperspectral data; I want to be clear that we&rsquo;re not doing anything that&rsquo;s particularly well-motivated from a hyperspectral data perspective.  If you&rsquo;re interested, there&rsquo;s a great deal of research in the hyperspectral field &ndash; we can&rsquo;t summarize all the interesting stuff that&rsquo;s going on there, but if you&rsquo;re interested, check out some recent issues of WHISPERS: <a href="http://www.ieee-whispers.com/"><a href="http://www.ieee-whispers.com/">http://www.ieee-whispers.com/</a></a></p><p>In reality, the purpose of the data set from GRSS is to do data fusion, but for today we&rsquo;ll just be concerned with the hyperspectral data &ndash; dsCasi.</p><h2>PC Projections<a name="7"></a></h2><p>As an example, we can explore the data in principal component space. It&rsquo;s easy enought to do &ndash; we can treat the CASI part of the data just like any other prtDataSet.  Let&rsquo;s build an algorithm to do some standard pre-processing.  Each 144 dimensional hyperspectral vector is a row of the data matrix, so we can zero-mean, and normalize the standard deviation of each row with prtPreProcZeroMeanRows and prtPreProcStdNormalizeRows (which is new).</p><pre class="codeinput">preProc = prtPreProcZeroMeanRows + prtPreProcStdNormalizeRows(<span class="string">&lsquo;varianceOffset&rsquo;</span>,10) + prtPreProcPca(<span class="string">&lsquo;nComponents&rsquo;</span>,3);</p>

<p>dsLabeled = dsCasi.retainLabeled;
preProc = preProc.train(dsLabeled);
dsPreProc = preProc.run(dsLabeled);
subplot(1,1,1);
plot(dsPreProc);
legend(<span class="string">&lsquo;location&rsquo;</span>,<span class="string">&lsquo;EastOutside&rsquo;</span>)
title(<span class="string">&lsquo;CASI Hyperspectral PC&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Grss_02.png" alt=""> <p>Visualizing the data in PC space, we can see a few things.  First, all of the grass and tree samples are quite similar in PC space.  Also, synthetic grass looks nothing like the real grass classes &ndash; from a hyperspectral perspective, synthetic grass is clearly a man-made material, despite the color similarity between it and real grass.</p><h2>15-Class Classification<a name="9"></a></h2><p>Let&rsquo;s take a look at doing some classification.  Recall that GRSS labeled 15 unique classes for us; we can do standard machine learning with that data.  We will use similar pre-processing as above, and then a 15 component PLSDA classifier.  PLSDA is nice in this case, since it natively handles multi-class problems, and its quite fast.  We can probably get better results with a non-linear classifier, but for now we&rsquo;ll stick with PLSDA.</p><pre class="codeinput">dsLabeled = dsCasi.retainLabeled;</p>

<p>algo = prtPreProcZeroMeanRows + prtPreProcStdNormalizeRows(<span class="string">&lsquo;varianceOffset&rsquo;</span>,10) + prtClassPlsda(<span class="string">&lsquo;nComponents&rsquo;</span>,15) + prtDecisionMap;
yOut = algo.kfolds(dsLabeled,3);</p>

<p>close <span class="string">all</span>;
prtScoreConfusionMatrix(yOut);
pc = prtScorePercentCorrect(yOut);
title(sprintf(<span class="string">&lsquo;15 Class Classification; %.2f%% Correct&rsquo;</span>,pc*100));
xlabel(<span class="string">&lsquo;&rsquo;</span>);
rotateticklabel(gca,45);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Grss_03.png" alt=""> <p>Over 70% correct, with just this simple processing!  That&rsquo;s not too shabby.  Note, however, that the cross-validation approach we&rsquo;ve used here is a little suspect &ndash; a lot of the truth proivded to us was from neighboring pixels.  These pixels might be only about a meter apart, and two hyperspectral vectors from that close proximity, on, say, grass, will be expected to be much more correlated than two pixels from hundreds of meters apart.</p><p>One way to overcome this would be to build cross-validation folds using the spatial locations of the pixels; it would be interesting to see how cross-validation would work under that case.</p><h2>Evaluation<a name="11"></a></h2><p>To evaluate our algorithm we can visualize the performance on the entire larger hyperspectral image.  First, we run the algorithm, then get the estimated labels, and reshape to the right size for visualization.</p><pre class="codeinput">algo = algo.train(dsLabeled);
yOutFull = algo.run(dsCasi);</p>

<p>close <span class="string">all</span>;
img = reshape(yOutFull.X,imgSize);
imagesc(img);
colorbar
<span class="comment">% It&rsquo;s a little hard to judge, but this doesn&rsquo;t look completely</span>
<span class="comment">% unreasonable&hellip;.</span>
<span class="comment">%</span>
<span class="comment">% The following shows only the points labeled as one of the two types of</span>
<span class="comment">% grasses or trees</span>
imagesc(img == 1 | img == 2 | img == 4);
title(<span class="string">&lsquo;Points Classified as Grass (Healthy, and Stressed) or Tree&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Grss_04.png" alt=""> <h2>Conclusions<a name="12"></a></h2><p>If you&rsquo;re interested in hyperspectral data, LIDAR, or data fusion, you should definitely check out the GRSS data set.  We hope the PRT files we&rsquo;re providing will help you get started.</p><p>There&rsquo;s a lot more to do with this data &ndash; we haven&rsquo;t even explored the LIDAR data yet, or how to fuse information from the two.  This data is also a prime candidate for semi-supervised learning (<a href="http://en.wikipedia.org/wiki/Semi-supervised_learning"><a href="http://en.wikipedia.org/wiki/Semi-supervised_learning">http://en.wikipedia.org/wiki/Semi-supervised_learning</a></a>), active learning (<a href="http://en.wikipedia.org/wiki/Active_learning"><a href="http://en.wikipedia.org/wiki/Active_learning">http://en.wikipedia.org/wiki/Active_learning</a></a>), or multi-task learning (<a href="http://en.wikipedia.org/wiki/Multi-task_learning"><a href="http://en.wikipedia.org/wiki/Multi-task_learning">http://en.wikipedia.org/wiki/Multi-task_learning</a></a>).</p><p>Hopefully we&rsquo;ll get a chance to delve more into this data set in the near future.  Let us know if you have any luck with this data!</p></p>

<p><a href="http://newfolder.github.io/images/prtGrssBlog_2013_02_16.zip">Here&rsquo;s a link</a> to the .ZIP file with all the code you&rsquo;ll need.</p>

<p><p>
You may also need rotateticklabel.m, which is available from <a href="http://www.mathworks.com/matlabcentral/fileexchange/8722-rotate-tick-label">MATLAB Central</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Combining Actions]]></title>
    <link href="http://newfolder.github.io/blog/2013/02/11/combining-actions/"/>
    <updated>2013-02-11T13:46:00-05:00</updated>
    <id>http://newfolder.github.io/blog/2013/02/11/combining-actions</id>
    <content type="html"><![CDATA[<p>Hi!  Today I&#8217;d like to talk about how you can use the PRT to combine actions together to form algorithms.  This is an important and powerful tool in the PRT, and understanding it can solve a lot of headaches for you.</p>


<!--/introduction-->


<h2>Contents</h2>


<div><ul><li><a href="#1">An Example</a></li><li><a href="#5">The Problem</a></li><li><a href="#9">Combining Actions into Algorithms</a></li><li><a href="#13">Using Algorithms</a></li></ul></div>


<h2>An Example<a name="1"></a></h2>


<p>Let&#8217;s start with a concrete example.  Say we want to classify some very high dimensional data.  We&#8217;ll start with the following:</p>


<pre class="codeinput">nFeatures = 200;
ds = prtDataGenUnimodal;
xNoise = randn(ds.nObservations,nFeatures);
ds.X = cat(2,ds.X,xNoise); <span class="comment">%add nFeatures meaningless features</span>
</pre>


<p>If we try and classify this with a GLRT, for example, we&#8217;re going to run into trouble, since there are more features than there are observations, so we can&#8217;t generate a full-rank covariance structure.  For example, using the prtAction prtClassGlrt, we might write this:</p>


<pre class="codeinput">glrt = prtClassGlrt;
glrt = glrt.train(ds);
<span class="keyword">try</span>
   yOut = glrt.run(ds);  <span class="comment">%This causes errors</span>
<span class="keyword">catch</span> ME
    disp(<span class="string">'Error encountered:'</span>)
    disp(ME);
<span class="keyword">end</span>
</pre>


<pre class="codeoutput">Warning: Covariance matrix is not positive definite. This may cause errors.
Consider modifying "covarianceStructure". 
Warning: Covariance matrix is not positive definite. This may cause errors.
Consider modifying "covarianceStructure". 
Error encountered:
  MException
  Properties:
    identifier: 'prtRvUtilMvnLogPdf:BadCovariance'
       message: 'SIGMA must be symmetric and positive definite.'
         cause: {0x1 cell}
         stack: [9x1 struct]

</pre>


<p>We can always use dimension-reduction techniques to reduce the number of features in our data set, and then evaluate performance.  For example:</p>


<pre class="codeinput">pca = prtPreProcPca(<span class="string">'nComponents'</span>,2);
pca = pca.train(ds);
dsPca = pca.run(ds);
plot(dsPca);
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_combiningActions_01.png" alt=""> <p>Now we can evaluate our GLRT on the dsPca:</p><pre class="codeinput">glrt = prtClassGlrt;
yOutKfolds = glrt.kfolds(dsPca,10);
[pf,pd] = prtScoreRoc(yOutKfolds);
h = plot(pf,pd);
set(h,<span class="string">&lsquo;linewidth&rsquo;</span>,3);
title(<span class="string">&lsquo;Example GLRT ROC Curve (Running on PCA Features)&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_combiningActions_02.png" alt=""> <h2>The Problem<a name="5"></a></h2><p>There&rsquo;s a problem in the above, though.  Even though we cross-validated the GLRT using 3 random folds, we didn&rsquo;t do the same thing with the PCA. This is technically not fair, since the PCA part of the algorithm was trained using all the data.</p><p>Maybe we can get around this like so:</p><pre class="codeinput">pca = prtPreProcPca(<span class="string">&lsquo;nComponents&rsquo;</span>,2);
dsPca = pca.kfolds(ds,10);
</pre><p>But now, when we do:</p><pre class="codeinput">glrt = prtClassGlrt;
yOutKfolds = glrt.kfolds(dsPca,10);
[pf,pd] = prtScoreRoc(yOutKfolds);
h = plot(pf,pd);
set(h,<span class="string">&lsquo;linewidth&rsquo;</span>,3);
title(<span class="string">&lsquo;This is no good&hellip;&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_combiningActions_03.png" alt=""> <p>We have a problem!  At every fold, we learn a unique set of PCA loadings. Since PCA loadings have arbitrary sign (+/&ndash;), the outputs across all these folds will overlap!</p><pre class="codeinput">plot(dsPca)
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_combiningActions_04.png" alt=""> <p>The underlying problem is that there&rsquo;s no guarantee that the folds used for PCA and GLRT evaluation were the same.  We can get around <b>that</b> if we specified the folds, and wrote our own cross-validate specifically for this new process we&rsquo;ve made, but suddenly this is getting complicated.</p><p>And what if we had an even more complicated process, including other pre-processing streams, feature selection, classifiers and decision-makers?  Suddenly our code is going to be a mess!</p><h2>Combining Actions into Algorithms<a name="9"></a></h2><p>At the heart of the problem outlined above is that the PCA and GLRT parts of our process weren&rsquo;t considered as two parts of the same process &ndash; they were two separate variables, and the PRT and MATLAB didn&rsquo;t know that they should work together.</p><p>Since this problem is so common, the PRT provides an easy way to combine each individual part of a process (prtActions) into one big process (a prtAlgorithm).  This is easily done using the &ldquo;+&rdquo; operator:</p><pre class="codeinput">pcaGlrt = prtPreProcPca(<span class="string">&lsquo;nComponents&rsquo;</span>,2) + prtClassGlrt;
</pre><p>If you&rsquo;re not used to object oriented programming, the above might look a little weird.  But it&rsquo;s straightforward &ndash; we&rsquo;ve defined &ldquo;plus&rdquo; (&ldquo;+&rdquo;) for prtActions (e.g., prtPreProcPca) to mean &ldquo;Combine these into one object, where that object will perform each action in sequence from left to right&rdquo;.  Technically this returns a special kind of prtAction, called a prtAlgorithm.  That&rsquo;s just the date type we use to store a bunch of actions.  You can see that here:</p><pre class="codeinput">disp(pcaGlrt)
</pre><pre class="codeoutput">  prtAlgorithm</p>

<p>  Properties:</p>

<pre><code>                name: 'PRT Algorithm'
    nameAbbreviation: 'ALGO'
        isSupervised: 1
isCrossValidateValid: 1
          actionCell: {2x1 cell}
  connectivityMatrix: [4x4 logical]
      verboseStorage: 1
     showProgressBar: 1
           isTrained: 0
      dataSetSummary: []
             dataSet: []
            userData: [1x1 struct]
</code></pre>

<p></pre><p>You can visualize the structure of the algorithm using PLOT:</p><pre class="codeinput">plot(pcaGlrt)
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_combiningActions_05.png" alt=""> <p>You can combine any number of prtActions into an algorithm like this, so, although its silly, this is technically a valid command:</p><pre class="codeinput">sillyAlgo = prtPreProcZmuv + prtPreProcHistEq + prtPreProcPca + prtClassGlrt;
plot(sillyAlgo)
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_combiningActions_06.png" alt=""> <h2>Using Algorithms<a name="13"></a></h2><p>So we&rsquo;ve made a prtAlgorithm.  Now what?  Well, anything you can do to a prtAction, you can do with a prtAlgorithm.  What does that mean?  Methods like plot, kfolds, and crossValidate all work exactly the same as they do with regular prtActions.  And they make your life much simpler than what we had to do above:</p><pre class="codeinput">pcaGlrt = prtPreProcPca(<span class="string">&lsquo;nComponents&rsquo;</span>,2) + prtClassGlrt;
yOutKfolds = pcaGlrt.kfolds(ds,10);
[pf,pd] = prtScoreRoc(yOutKfolds);
h = plot(pf,pd);
set(h,<span class="string">&lsquo;linewidth&rsquo;</span>,3);
title(<span class="string">&lsquo;ROC Curve for a prtAlgorithm (PCA + GLRT)&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_combiningActions_07.png" alt=""> <p>The results in the ROC curve above were generated using 10-folds cross-validation on the combination of PCA and GLRT.  At each fold, 9/10ths of the data were used to train the PCA and GLRT, and 1/10th was used for evaluation.</p><p>prtAlgorithms are a very powerful tool for pattern recognition, and we hope this blog post helps clear up how to make and use them!</p><p>Let us know if you have any questions or comments.</p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Random Variables (Part 1)]]></title>
    <link href="http://newfolder.github.io/blog/2013/01/31/random-variables-part-1/"/>
    <updated>2013-01-31T16:24:00-05:00</updated>
    <id>http://newfolder.github.io/blog/2013/01/31/random-variables-part-1</id>
    <content type="html"><![CDATA[<p>Hey ya&#8217;ll! Probability theory and random variables come up all the time in machine learning. Classification techniques like Naive Bayes, the likelihood ratio test and maximum a posterior (MAP). A lot of times when someone says &#8220;Naive Bayes classification&#8221; they imply that they want to assume that the data is multinomial (counts from a fixed dictionary) or when they say &#8220;MAP classification&#8221; they mean they assumed Gaussian distributions for each of the classes. In reality though the choice of these distributions is flexible and assuming different distributions in the PRT is easy thanks to the RV objects. This is the first post in a series that will highlight how RV objects can be used for rapid classifier generation and showcase some of the ways that we use RVs for our research. In part 1, we are going to give an overview of some of the basic RV objects and show how they are used in some basic classification techniques.</p>




<h2>Contents</h2>


<div><ul><li><a href="#1">RV objects</a></li><li><a href="#7">Types of RV objects</a></li><li><a href="#9">Using RV Objects in Classifiers</a></li><li><a href="#12">Conclusions</a></li></ul></div>


<h2>RV objects<a name="1"></a></h2>


<p>Admittedly, the RV objects are one of the most under-documented features in the PRT. Sorry about that. I can take the blame there. Hopefully this post gets us started on fixing that.</p>


<p>Random variable objects are used to state that data is a random variable with an assumed distribution. Therefore, each prtRv*() assumes a different probability density function and implements the necessary methods to infer the parameters of the probability density function mle(), draw data with the same distribution draw(), and evaluate the likelihood of other data pdf() and logPdf().</p>




<p>
Let&#8217;s make an RV object with a specified distribution. For the sake of this example we will use the multi-variate Normal distribution &#8220;MVN&#8221;.
</p>




<pre class="codeinput">
rv = prtRvMvn(<span class="string">'mu'</span>,[1 2],<span class="string">'sigma'</span>,[1 0.5; 0.5 1])
</pre>


<pre class="codeoutput">rv = 
  prtRvMvn

  Properties:
                    name: 'Multi-Variate Normal'
        nameAbbreviation: 'RVMVN'
            isSupervised: 0
    isCrossValidateValid: 1
     covarianceStructure: 'full'
                      mu: [1 2]
                   sigma: [2x2 double]
             plotOptions: [1x1 prtOptions.prtOptionsRvPlot]
          verboseStorage: 1
         showProgressBar: 1
               isTrained: 0
          dataSetSummary: []
                 dataSet: []
                userData: [1x1 struct]
</pre>


<p>Let&#8217;s draw some data from this RV object and put it into a prtDataSetClass().</p>


<pre class="codeinput">x = rv.draw(1000);
ds = prtDataSetClass(x);
</pre>


<p>Using this RV we can evaluate the log of the probability density function of the data that we drew.</p>


<pre class="codeinput">y = rv.logPdf(x);
</pre>


<p>RV objects also have some plot methods and plotLogPdf() is probably the most useful. Let&#8217;s plot the log of the probability density function with the data that we drew fromt the pdf.</p>


<pre class="codeinput">rv.plotLogPdf()
hold <span class="string">on</span>
plot(ds);
hold <span class="string">off</span>
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/morton_blog_20130127_01.png" alt=""> <p>Although RV objects can be used by specifying the parameters of the densities their true power is flexibly modeling data. For example, let&rsquo;s make another RV MVN object without specifying parameters and use it to estimate the parameters of the data we drew. Here we will estimate the parameters using maximum likelihood estimation mle()</p><pre class="codeinput">rv2 = prtRvMvn;
rv2 = rv2.mle(ds); <span class="comment">% or rv2 = rv2.mle(x);</span>
estimatedMean = rv2.mu
estimateCovariance = rv2.sigma
</pre><pre class="codeoutput">estimatedMean =
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.9647    1.9896
estimateCovariance =
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.9479    0.4587
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.4587    0.9858
</pre><p>RV objects are actually sub-classes of prtActions() just like classifiers and regressors. This means that they have the train() and run() methods and can be cross-validated. By default, all RV objects implement train by calling the mle() method and implement run by using the logPdf() method. Therefore, some of the things we did above can be done as follows.</p><pre class="codeinput">rv2 = rv2.train(ds);
y = rv2.run(ds);
</pre><h2>Types of RV objects<a name="7"></a></h2><p>A list of available RVs that ship with the PRT can be displayed</p><pre class="codeinput">dirContents = what(fullfile(prtRoot,<span class="string">&lsquo;rv&rsquo;</span>));
availableRvs = dirContents.m
</pre><pre class="codeoutput">availableRvs =
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRv.m&rsquo;
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRvDiscrete.m&rsquo;
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRvGmm.m&rsquo;
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRvHmm.m&rsquo;
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRvIndependent.m&rsquo;
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRvKde.m&rsquo;
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRvMemebershipModel.m&rsquo;
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRvMixture.m&rsquo;
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRvMultinomial.m&rsquo;
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRvMvn.m&rsquo;
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRvUniform.m&rsquo;
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRvUniformImproper.m&rsquo;
&nbsp;&nbsp;&nbsp;&nbsp;&lsquo;prtRvVq.m&rsquo;
</pre><p>As you can, most of the standard probability densities have been implemented. In addition to standard things like prtRvMvn, prtRvDiscrete and prtRvMultinomial, there are also a few RVs that operate on other RVs like prtRvIndependent, prtRvMixture, prtRvGmm and prtRvHmm and there are a few RVs that can be used for more flexible density modeling like prtRvKde and prtRvVq. We will talk about some of these more advanced RVs in a later post.</p><h2>Using RV Objects in Classifiers<a name="9"></a></h2><p>There are two primary classifiers that make use of RV objects prtClassMap and prtClassGlrt. These classifiers have very similar performance but prtClassMap is able to handle M-ary classification problems so we will use that as our example.</p><pre class="codeinput">class = prtClassMap
</pre></p>

<pre class="codeoutput">class = 
  prtClassMap

  Properties:
                    name: 'Maximum a Posteriori'
        nameAbbreviation: 'MAP'
            isNativeMary: 1
                     rvs: [1x1 prtRvMvn]
        twoClassParadigm: 'binary'
         internalDecider: []
            isSupervised: 1
    isCrossValidateValid: 1
          verboseStorage: 1
         showProgressBar: 1
               isTrained: 0
          dataSetSummary: []
                 dataSet: []
                userData: [1x1 struct]
</pre>




<p>prtClassMap has a property rvs that lists the rvs used for each class in the incoming data set. If there is only one RV specified it is used to model all of the classes. Let&#8217;s classify prtDataGenUnimodal using a quadratic classifier that arises by using a MAP classifier with MVN assumption for each class.</p>


<pre class="codeinput">class = prtClassMap(<span class="string">'rvs'</span>,prtRvMvn);
ds = prtDataGenUnimodal;

trainedClassifier = class.train(ds);
plot(trainedClassifier);
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/morton_blog_20130127_02.png" alt=""> <p>If our data is more complex, we can modify the assumptions of the distributions in both of our classes by setting the &ldquo;rvs&rdquo; parameter to something more flexible. Let&rsquo;s classify prtDataGenBimodal using prtRvKde which uses kernel density estimation.</p><pre class="codeinput">class = prtClassMap(<span class="string">&lsquo;rvs&rsquo;</span>,prtRvKde);
ds = prtDataGenBimodal;</p>

<p>trainedClassifier = class.train(ds);
plot(trainedClassifier);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/morton_blog_20130127_03.png" alt=""> <h2>Conclusions<a name="12"></a></h2><p>As you can see, RVs are pretty powerful parts of the PRT and they can be used in other parts of the PRT to make things flexible.</p><p>In future posts we will talk about how RV objects are used to make flexible mixtures like the GMM and hidden Markov models and we will explore some things that are still in beta such as how we use prtBrv objects to perform variational Bayesian inference for models like Dirichlet process mixtures.</p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Decisions Decisions]]></title>
    <link href="http://newfolder.github.io/blog/2013/01/25/decisions-decisions/"/>
    <updated>2013-01-25T10:45:00-05:00</updated>
    <id>http://newfolder.github.io/blog/2013/01/25/decisions-decisions</id>
    <content type="html"><![CDATA[<p>You may have noticed in a lot of examples, we&#8217;ve made use of prtDecision objects, and might have wondered what exactly those are, and how they work.  Today I&#8217;d like to describe the prtDecision* actions, and when you might want to use them.</p>


<!--/introduction-->


<h2>Contents</h2>


<div><ul><li><a href="#4">Making Manual Decisions</a></li><li><a href="#8">Decision Objects</a></li><li><a href="#11">Concluding</a></li></ul></div>


<p>Let&#8217;s start out with a pretty standard prtDataSet, and we&#8217;ll make a classifier and score a ROC curve:</p>


<pre class="codeinput">ds = prtDataGenUnimodal;
classifier = prtClassFld;
yOutFld = kfolds(classifier,ds,3);
[pf,pd] = prtScoreRoc(yOutFld);
h = plot(pf,pd);
set(h,<span class="string">'linewidth'</span>,3);
title(<span class="string">'ROC Curve for FLD'</span>);
xlabel(<span class="string">'Pfa'</span>);
ylabel(<span class="string">'Pd'</span>);
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Decisions_01.png" alt=""> <p>That ROC curve looks pretty good, but it doesn&rsquo;t tell the whole story. At the end of the day, if you wanted to use your FLD algorithm in a production setting, you&rsquo;ll need to make discrete decisions to take different actions depending on whether you&rsquo;re calling something Class #1 or Class #0.  An ROC curve is suitable for comparing performance across a range of possible operating points, but what if we wanted to know exactly what PD and PFA we were going to get for a particular decision point?</p><p>To clarify matters, let&rsquo;s take a look at what the output from FLD actually looks like.</p><pre class="codeinput">h = plot(1:yOutFld.nObservations,yOutFld.X,1:yOutFld.nObservations,yOutFld.Y);
set(h,<span class="string">&lsquo;linewidth&rsquo;</span>,3);
xlabel(<span class="string">&lsquo;Observation Index&rsquo;</span>);
legend(h,{<span class="string">&lsquo;FLD Output&rsquo;</span>,<span class="string">&lsquo;Class Label&rsquo;</span>});
title(<span class="string">&lsquo;FLD Output &amp; Actual Class Label vs. Observation Index&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Decisions_02.png" alt=""> <p>The above figure shows what&rsquo;s actually going on under the hood &ndash; when a classifier object is run on a data set, the output data set (yOutFld) has it&rsquo;s X value set to the classifier output.  In this case, the yOutFld.X value is a linear weighting of the input variables, and is shown in blue.  You can see how it correlated with the actual class labels (in green).</p><h2>Making Manual Decisions<a name="4"></a></h2><p>Say we wanted to make decisions based on the output of the FLD.  We have to choose a threshold (a point along the y-axis) such that whenever a blue data point is above the threshold, we call the output &ldquo;Class 1&rdquo;, and otherwise we call it &ldquo;Class 0&rdquo;.  By visual inspection, any value between, say, 0 and 2 looks reasonable.  Let&rsquo;s try manually setting a threshold of 1:</p><pre class="codeinput">yOutManual = yOutFld;
yOutManual.X = yOutManual.X &gt; 1;
h = plot(1:yOutManual.nObservations,yOutManual.X,1:yOutManual.nObservations,yOutManual.Y);
ylim([&ndash;.1 1.1]);
set(h,<span class="string">&lsquo;linewidth&rsquo;</span>,3);
xlabel(<span class="string">&lsquo;Observation Index&rsquo;</span>);
legend(h,{<span class="string">&lsquo;Manual Decision Output&rsquo;</span>,<span class="string">&lsquo;Class Label&rsquo;</span>},4);
title(<span class="string">&lsquo;Manual Decision &amp; Actual Class Label vs. Observation Index&rsquo;</span>);
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Decisions_03.png" alt=""> <p>You can see that our chosen threshold does pretty well.  The vast majority of the time, the blue line corresponds to the green line.  We can confirm this by considering the percent correct, and a confusion matrix:</p><pre class="codeinput">prtScoreConfusionMatrix(yOutManual);
pc = prtScorePercentCorrect(yOutManual);
title(sprintf(<span class="string">&lsquo;Percent Correct: %.0f%%&rsquo;</span>,pc<em>100));
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Decisions_04.png" alt=""> <p>98% Correct!  That&rsquo;s not bad.  But look at what happens if we try and do the same scoring on the original output from the FLD:</p><pre class="codeinput">prtScoreConfusionMatrix(yOutFld);
pc = prtScorePercentCorrect(yOutFld);
title(sprintf(<span class="string">&lsquo;Percent Correct: %.0f%% (This is clearly wrong!)&rsquo;</span>,pc</em>100));
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Decisions_05.png" alt=""> <p>What happened?  This is a little subtle, but whenever the PRT has to score discrete classes, like with prtScorePercentCorrect and prtScoreConfusionMatrix, it requires that the X values in the dataset to be equal to your best guess as to the real underlying class.</p><p>That worked out great for yOutManual, since we set yOutManual.X to zero for class 0, and 1 for class 1.  But yOutFld has continuous values stored in it (as the earlier figure shows); you need to make discrete decisions for prtScorePercentCorrect or prtScoreConfusionMatrix to make any sense.</p><h2>Decision Objects<a name="8"></a></h2><p>Fortunately, the PRT provides a special kind of prtAction &ndash; prtDecisions to make those decisions for you automatically, so you can score algorithms very easily.</p><p>For example, prtDecisionBinaryMinPe tries to find a threshold based on the training data to minimize the probability of error (Pe). You can use the decision actions like you would use any other actions in a prtAlgorithm:</p><pre class="codeinput">algo = prtClassFld + prtDecisionBinaryMinPe;
yOutDecision = kfolds(algo,ds,3);
prtScoreConfusionMatrix(yOutDecision);
pc = prtScorePercentCorrect(yOutDecision);
title(sprintf(<span class="string">&lsquo;Percent Correct: %.0f%%&rsquo;</span>,pc<em>100));
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Decisions_06.png" alt=""> <p>Now we&rsquo;re back in the ball game!  You can use different decision objects to get performance at different points on the ROC curve, for example prtDecisionBinarySpecifiedPd let&rsquo;s you specify a Pd to operate at:</p><pre class="codeinput">close <span class="string">all</span>;
algo = prtClassFld + prtDecisionBinarySpecifiedPd(<span class="string">&lsquo;pd&rsquo;</span>,.99);
yOutDecision = kfolds(algo,ds,3);
prtScoreConfusionMatrix(yOutDecision);
pc = prtScorePercentCorrect(yOutDecision);
title(sprintf(<span class="string">&lsquo;Percent Correct: %.0f%%&rsquo;</span>,pc</em>100));
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Decisions_07.png" alt=""> <p>Note that the overall probability of error is significantly worse, but almost all of the data from Class 1 was identified as Class 1.  (This may not achieve 99% Pd in some cases since the thresholds are learned differently in each fold, so there is some statistical variation in the actual Pd achieved).</p><p>We can also use prtDecisionMap to perform multi-class decision making. The &ldquo;Map&rdquo; in prtDecisionMap stands for maximum a-posteriori.  This basically means &ldquo;decide the class corresponding to the maximum classifier output&rdquo;.</p><pre class="codeinput">ds = prtDataGenMary;
algo = prtClassKnn + prtDecisionMap;
yOutDecision = kfolds(algo,ds,3);
prtScoreConfusionMatrix(yOutDecision);
pc = prtScorePercentCorrect(yOutDecision);
title(sprintf(<span class="string">&lsquo;Percent Correct: %.0f%%&rsquo;</span>,pc*100));
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/torrione_blog_Decisions_08.png" alt=""> <h2>Concluding<a name="11"></a></h2><p>So, there you go!  prtDecision objects handle a lot of book-keeping internally, so that you don&rsquo;t generally have to worry about making sure to keep class names and indices straight.  We recommend using them instead of manually making your own decision functions to operate on output classes.</p><p>As always, please feel free to comment or e-mail us with questions or ideas.</p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[An Introduction to the PRT with MNIST]]></title>
    <link href="http://newfolder.github.io/blog/2013/01/21/introduction-to-the-prt/"/>
    <updated>2013-01-21T18:58:00-05:00</updated>
    <id>http://newfolder.github.io/blog/2013/01/21/introduction-to-the-prt</id>
    <content type="html"><![CDATA[<p>The MNIST Database (<a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>) is a very well-known machine learning dataset consisting of a few thousand instances of handwritten digits from 0-9.  MNIST is actually a subset of a larger NIST database, but the authors (see the linked page above) were kind enough to do some basic pre-processing of MNIST for us.  MNIST was for a long time very widely used in the ML literature as an example of an easy to use real data set to evaluate new ideas.</p>




<!--/introduction-->


<h2>Contents</h2>


<div><ul><li><a href="#1">Obtaining, Loading, and Visualizing MNIST Data</a></li><li><a href="#5">Classification: PLSDA</a></li><li><a href="#8">Classification: SVM</a></li><li><a href="#12">Exploring the Results</a></li></ul></div>


<h2>Obtaining, Loading, and Visualizing MNIST Data<a name="1"></a></h2>


<p>Tools to read in the MNIST database into the PRT are available in the newest PRT version.  To conserve bandwidth, the actual MNIST data isn&#8217;t included in the PRT (it would kill our subversion servers).  Instead you can download the MNIST database from the website linked above.  Once you&#8217;ve downloaded it, extract the data into:</p>


<pre class="codeinput">fullfile(prtRoot,<span class="string">'dataGen'</span>,<span class="string">'dataStorage'</span>,<span class="string">'MNIST'</span>) <span class="comment">%MATLAB command will tell you the directory</span>
</pre>


<pre class="codeoutput">
ans =

C:\Users\Pete\Documents\MATLAB\toolboxes\nfPrt\dataGen\dataStorage\MNIST

</pre>


<p>For example, on my system:</p>


<pre class="codeinput">ls(fullfile(prtRoot,<span class="string">'dataGen'</span>,<span class="string">'dataStorage'</span>,<span class="string">'MNIST'</span>))
</pre>


<pre class="codeoutput">
.                        t10k-labels.idx1-ubyte   
..                       train-images.idx3-ubyte  
t10k-images.idx3-ubyte   train-labels.idx1-ubyte  

</pre>


<p>Once the MNIST files are in the right place, execute the PRT command:</p>


<pre class="codeinput">dsTrain = prtDataGenMnist;
</pre>


<p>to extract the data.  ( Note, prtDataGenMnist makes use of a M-file function called readMNIST by Siddharth Hegde.  It&#8217;s available from: <a href="http://www.mathworks.com/matlabcentral/fileexchange/27675-read-digits-and-labels-from-mnist-database">http://www.mathworks.com/matlabcentral/fileexchange/27675-read-digits-and-labels-from-mnist-database</a> ).</p>


<p>Once loaded, we can use a number of different tools to visualize the data.  First, let&#8217;s visualize the data as images.  We know that the images are size 28x28, but since the prtDataSetClass object expects each observation to correspond to a 1xN vector, we store all the 28x28 images as 1x784 vectors.</p>


<pre class="codeinput">imageSize = [28 28];

<span class="keyword">for</span> i = 1:9;
    subplot(3,3,i);
    x = dsTrain.getX(i); <span class="comment">%1x784</span>
    y = dsTrain.getY(i);
    imagesc(reshape(x,imageSize));
    colormap <span class="string">gray</span>;
    title(sprintf(<span class="string">'MNIST; Digit = %d'</span>,y));
<span class="keyword">end</span>
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/testBlog_01.png" alt=""> <h2>Classification: PLSDA<a name="5"></a></h2><p>What kinds of classification approaches can we apply to this data set? We need to satisfy a few requirements: 1) M-Ary classification, 2) Relatively fast, 3) Relatively insensitive to a large number of dimensions (784 dimensional vectors). One particularly fast, linear approach to classification that&#8217;s relatively insensitive to the number of feature dimensions is partial-least squares discriminant analysis, implemented in the PRT as prtClassPLSDA.  With only a few lines of code we can implement and evaluate a PLSDA classifier on the MNIST data, for example:</p><pre class="codeinput">algo = prtClassPlsda(<span class="string">&lsquo;nComponents&rsquo;</span>,20) + prtDecisionMap; <span class="comment">%we include the Max-A-Posteriori classifier</span>
yOut = algo.kfolds(dsTrain,3); <span class="comment">%3 folds x-val</span>
pc = prtScorePercentCorrect(yOut);
subplot(1,1,1);
prtScoreConfusionMatrix(yOut);
title(sprintf(<span class="string">&lsquo;3-Fold X-Val PLSDA on 10,000 MNIST Database Train Samples; %.0f%% Correct&rsquo;</span>,pc*100));
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/testBlog_02.png" alt=""> <p>This basic example results in the above figure, where we see we&#8217;ve achieved about 84% correct classification, and we can analyze confusions between digits.  For example, the digits 4 and 9 are often confused, which seems intuitive since they look relatively similar.</p><p>We can also evaluate the PLSDA classifier trained on 10,000 training points and evaluated on the MNIST testing data.  To do so we first load the testing data, then train our classifier and evaluate it:</p></p>

<pre class="codeinput">dsTest = prtDataGenMnistTest;
algo = algo.train(dsTrain);
yOut = algo.run(dsTest);
pc = prtScorePercentCorrect(yOut);
subplot(1,1,1);
prtScoreConfusionMatrix(yOut);
title(sprintf(<span class="string">'PLSDA on 10,000 MNIST Database Test Samples; %.0f%% Correct'</span>,pc*100));
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/testBlog_03.png" alt=""> <p>Performance on the test set is relatively similar to performance in cross-validation as can be seen above.</p><p>Overall, our performance is hovering around a 15% error rate.  That&#8217;s roughly comparable to the 12% error reported in LeCun et al., 1988, and here we&#8217;re not using a lot of the techniques in the Le Cun paper (and this is with barely 5 lines of code!).</p><h2>Classification: SVM<a name="8"></a></h2><p>As the results on <a href="http://yann.lecun.com/exdb/mnist/"><a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a></a> illustrate, other approaches to digit classification have done much better than our simple PLSDA classifier.  We can use the PRT to apply more complicated classifiers to the same data also, and hopefully decrease our error rate.</p><p>For example, consider a simple application of an SVM classifier to the digit recognition problem.  Since the SVM is not an M-ary classification technique, we need to wrap our SVM in a One-Vs-All classifier to perform M-ary classification (Warning: the following code took about 30 minutes to run on my laptop):</p><pre class="codeinput">marySvm = prtPreProcZmuv + prtClassBinaryToMaryOneVsAll(<span class="string">&lsquo;baseClassifier&rsquo;</span>,prtClassLibSvm) + prtDecisionMap;
yOut = marySvm.kfolds(dsTrain,3);
pc = prtScorePercentCorrect(yOut);
subplot(1,1,1);
prtScoreConfusionMatrix(yOut);
title(sprintf(<span class="string">&lsquo;3-Fold X-Val SVM on 10,000 MNIST Database Train Samples; %.0f%% Correct&rsquo;</span>,pc*100));
</pre><pre class="codeoutput">Warning: Non-finite or zero standard deviation encountered.  Replacing invalid
standard deviations with 1
Warning: Non-finite or zero standard deviation encountered.  Replacing invalid
standard deviations with 1
Warning: Non-finite or zero standard deviation encountered.  Replacing invalid
standard deviations with 1
</pre><img vspace="5" hspace="5" src="http://newfolder.github.io/images/testBlog_04.png" alt=""> <p>As can be seen above, the SVM achieves an error rate of 5% on this data set!  That&#8217;s a significant improvement over the PLSDA classification we showed before.  Similarly to with PLSDA, we can also evaluate the algorithm on completely separate testing data:</p></p>

<pre class="codeinput">marySvm = marySvm.train(dsTrain);
yOut = marySvm.run(dsTest);
pc = prtScorePercentCorrect(yOut);
subplot(1,1,1);
prtScoreConfusionMatrix(yOut);
title(sprintf(<span class="string">'PLSDA on 10,000 MNIST Database Test Samples; %.0f%% Correct'</span>,pc*100));
</pre>


<pre class="codeoutput">Warning: Non-finite or zero standard deviation encountered.  Replacing invalid
standard deviations with 1 
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/testBlog_05.png" alt=""> <p>And we see that performance is comparable to the cross-validated results. (Note that more advanced applications of SVM classifiers can do even better than the results reported here &#8211; Le Cun et al., 1998 reported 1.4% error rates with an SVM and some additional processing).</p><h2>Exploring the Results<a name="12"></a></h2><p>If we wanted to improve classification, we could optimize over the SVM parameters, kernel, pre-processing etc.  But before we did that, it might be instructive to investigate what digits the SVM classifier is mislabeling, and see if some of them seem like reasonable mistakes to make.  The following code will pick 9 instances where the SVM output label was different from the actual data label, and plot them in a subplot.</p></p>

<pre class="codeinput">incorrect = find(yOut.getX ~= yOut.getY);
yOutTestMisLabeled = yOut.retainObservations(incorrect);
dsTestMisLabeled = dsTest.retainObservations(incorrect);
<span class="keyword">for</span> i = 1:9; <span class="comment">%dsTestMisLabeled.nObservations;</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;randWrong = ceil(rand*dsTestMisLabeled.nObservations); <span class="comment">%pick a random wrong element</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;subplot(3,3,i);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x = dsTestMisLabeled.getX(randWrong);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;img = reshape(x,imageSize);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;imagesc(img);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;colormap <span class="string">gray</span>;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
title(sprintf(<span class="string">'Actual: %d; SVM Label: %d'</span>,yOutTestMisLabeled.getY(randWrong),yOutTestMisLabeled.getX(randWrong)));
<span class="keyword">end</span>
</pre>


<p><img vspace="5" hspace="5" src="http://newfolder.github.io/images/testBlog_06.png" alt=""> <p>Visual inspection of these mistakes illustrate some of the causes of confusions in the SVM.  For example, highly slanted digits are often mis-labeled.  Mitigating some of these mistakes may require significantly more than simply optimizing SVM parameters!</p><p>Interested readers can refer to a large body of literature that has previously investigated this data set (<a href="http://yann.lecun.com/exdb/mnist/"><a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a></a>) for tips, tricks, and ideas for further improving performance on this data set.  One particularly exciting recent advance is based on Hinton&#8217;s deep learning networks, which enables very efficient learning on the MNIST database <a href="www.cs.toronto.edu/~hinton/science.pdf">www.cs.toronto.edu/~hinton/science.pdf</a>).</p><p>We hope this example shows how quickly you can get from data to results with the PRT.  Please let us know if you have comments or questions!</p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hi]]></title>
    <link href="http://newfolder.github.io/blog/2013/01/21/hi/"/>
    <updated>2013-01-21T16:02:00-05:00</updated>
    <id>http://newfolder.github.io/blog/2013/01/21/hi</id>
    <content type="html"><![CDATA[<p>Hi,</p>

<p>Thanks for checking out the PRT. The PRT is intended to help people who do machine learning handle the most common classification tasks within MATLAB.  This includes things like cross-validation, data visualization, classifier development and feature selection. The PRT includes object types for data set storage, pre-processing, classification, clustering, distance calculations, outlier removal, kernel learning, data generation, scoring and classifier evaluation, feature selection, random variable definitions, and visualization. We think it serves as a great platform for both machine learning research and applied algorithm development (that&rsquo;s why we built it!).</p>

<p>The PRT is free and released under the MIT license which means that it is free for academic and commercial use. The PRT is hosted on <a href="https://github.com/newfolder/PRT">GitHub</a> so feel free to fork or clone the repository or just download the latest <a href="https://github.com/newfolder/PRT/zipball/master">zip</a>.</p>

<p>Check out the <a href="http://newfolder.github.io/prtdoc/prtDocInstallation.html">installation instructions</a> to get you up and running. Once installed, open up MATLAB&reg;, and follow through with our <a href="http://newfolder.github.io/prtdoc/prtDocGettingStarted.html">Getting Started</a> guide, which will walk you through some examples of using the PRT in practice and show you what the PRT can do.</p>

<p>Even though there are a lot of M-files in the PRT, we’ve built the PRT to be easy to use and to make it easy to find what you need from within MATLAB.  For example, to find a list of all classifiers in the PRT, simply type “prtClass” and then <tab> at the MATLAB command prompt.  Other object types behave the same way, e.g., prtPreProc <tab>. If you have any feed back or questions don&rsquo;t hesitate to ask on on <a href="https://github.com/newfolder/PRT">GitHub</a> or through email at <a href="mailto:prt@newfolderconsulting.com"><a href="&#109;&#x61;&#105;&#108;&#x74;&#111;&#x3a;&#112;&#114;&#x74;&#x40;&#x6e;&#x65;&#119;&#102;&#x6f;&#108;&#x64;&#x65;&#x72;&#99;&#111;&#110;&#x73;&#x75;&#x6c;&#x74;&#105;&#x6e;&#103;&#46;&#99;&#x6f;&#109;">&#112;&#114;&#x74;&#x40;&#110;&#x65;&#x77;&#x66;&#x6f;&#108;&#100;&#x65;&#x72;&#99;&#x6f;&#110;&#x73;&#x75;&#x6c;&#x74;&#105;&#110;&#x67;&#46;&#x63;&#111;&#109;</a></a>.</p>

<p>We are going to start blogging every once in a while about the PRT and things it can do. We hope that it starts a conversation about the types of things people are doing with the PRT and what things people might want the PRT to do in the future.</p>

<p>Talk to you soon.  Happy coding!</p>

<p>-Kenny and Pete</p>
]]></content>
  </entry>
  
</feed>
